
==OpenDaylight Clustering==
The OpenDaylight controller supports a cluster-based high availability model. There are several instances of the OpenDaylight controller, which logically act as one logical controller.

==Prerequisites==
* Read documentation [https://wiki.opendaylight.org/view/OpenDaylight_Controller:Clustering:HowTo here] before starting the cluster.
* Start a cluster of Opendaylight controllers with minimum 2 controllers:
** In the first controller machine do &nbsp;  '''./run.sh -Dsupernodes=Controller1_IP:Controller2_IP -start''' &nbsp;  with administrator priviledges
*** This will start Controller1 in a cluster as supernode
** In the second controller machine do &nbsp;  '''./run.sh -Dsupernodes=Controller1_IP:Controller2_IP -start''' &nbsp;   with administrator priviledges
*** This will start Controller2 in a cluster as supernode
* Start mininet (i.e. '''sudo mn --controller=remote,ip=Controller1_IP --topo tree,2''')
** This generates 3 nodes: s1 (*:01), s2 (*:02), s3 (*:03)
** At this point only the Controller1 get provisioned in OVS bridges (s1, s2, s3)
** s1 has 2 ports: 1,2 that connects to s2 and s3
** s2 has 3 ports: 1 to host1 (10.0.0.1), 2 to host2 (10.0.0.2) and 3 to s1
** s3 has 3 ports: 1 to host3 (10.0.0.3), 2 to host4 (10.0.0.4) and 3 to s1
* Now, puase the mininet (i.e. ''mininet> CTRL + Z'')
* In order to get Controller2 provisioned in OVS bridges, do the following commands with administrator priviledges:
** ''$ ovs-vsctl set-controller s1 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''
** ''$ ovs-vsctl set-controller s2 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''
** ''$ ovs-vsctl set-controller s3 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''

==Notes==
* All the following test cases are written in Gherkin style ([http://robotframework.googlecode.com/hg/doc/userguide/RobotFrameworkUserGuide.html?r=2.8.3#different-test-case-styles behavior-driven]), and when possible the REST requests are provided.
* Note that ODL controller needs a couple of minutes to get all of its modules loaded. Therefore, any test that comes after restarting the controller, has to be done after a couple of minutes waiting time.

==Test Content==

=== Cluster Manager ===

===== Two controllers running ===== 
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: When C1 is up and running
:: And C2 is up and running
:: Then the system is working with C1 and C2

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* See the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* See the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check it that shows the same topology

:: (The following is an alternative test)
* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using mininet run a '''pingpair''' or simply run the following commands: '''h1 ping h2''' then  '''h1 ping h3''' then  '''h1 ping h4''' , ... 
* List the active hosts on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four hosts are listed
* List the active hosts on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four hosts are listed

===== Controller1 fails =====
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: When C1 goes down
:: Then C2 takes over
:: And the system is working with C2

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. '''./run.sh -stop''')
* See the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that the request fails
* See the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology

:: (The following is an alternative test)
* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using mininet run a '''pingpair''' or simply run the following commands: '''h1 ping h2''' then  '''h1 ping h3''' then  '''h1 ping h4''' , ... 
* List the active hosts on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that the request fails
* List the active hosts on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four host are still listed and Controller2 takes over

===== Controller2 fails =====
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: When C2 goes down
:: Then C1 takes over
:: And the system is working with C1

* Similar to the case when Controller1 fails.

===== Controller1 recovers after failure =====
:: Given C1 goses down
:: And C2 takes over
:: When C1 recovers
:: Then the system is working with C1 and C2

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e.  '''./run.sh -stop''')
* See the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that the request fails
* See the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* Restart Controller1 (i.e.  '''./run.sh -start''')
* See the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* See the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check it that shows the same topology
:: (The following is an alternative test)
* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using mininet run a '''pingpair''' or simply run the following commands: '''h1 ping h2''' then  '''h1 ping h3''' then  '''h1 ping h4''' , ... 
* Stop Controller1 (i.e.  '''./run.sh -stop''')
* List the active hosts on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that the request fails
* List the active hosts on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four hosts are listed and Controller2 takes over
* Restart Controller1 (i.e.  '''./run.sh -start''')
* List the active hosts on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four hosts are listed
* List the active hosts on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that all four hosts are listed

===== Controller1 and Controller2 fail =====
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: When C1 goes down
:: And C2 goes down
:: Then the system does not work any more

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e.  '''./run.sh -stop''')
* See the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that the request fails
* Stop Controller2 (i.e.  '''./run.sh -stop''')
* See the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that the request fails
:: (The following is an alternative test)
* List the active hosts on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that the request fails
* List the active hosts on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active) and check that the request fails

--------------------------------------------------------------------------------

=== Forwarding Rules Manager in a Cluster ===

===== The installed flow can be seen in a cluster of two controllers =====
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: When a flow is installed in a bridge
:: Then C1 see the flow
:: And C2 see the flow

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using Controller1 install flow1 on a bridge (i.e. PUT http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1 + [[CrossProject:Integration_Group:REST_Body#Flow1_Body|flow1 body]]) and check the response is success
* See flow1 on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present
* See flow1 on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and check the same flow is present

===== The installed flow remains in the bridge after the controller failure =====
:: Given C1 a controller in cluster of two controllers
:: And C2 a controller in cluster of two controllers
:: And both controllers get provisioned on all OVS bridges
:: And a flow is installed in a bridge
:: And C1 see the flow
:: And C2 see the flow
:: And C1 goes down
:: When C1 recovers
:: Then C1 see the flow

* Follow the instruction in the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using Controller1 install flow1 on a bridge (i.e. PUT http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1 + [[CrossProject:Integration_Group:REST_Body#Flow1_Body|flow1 body]]) and check the response is success
* See flow1 on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present
* See flow1 on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and check the same flow is present
* Stop Controller1 (i.e. '''sudo ./run.sh -stop''') 
* See flow1 on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check that the request fails
* Restart Controller1 (i.e. '''sudo ./run.sh -start''')
* See flow1 on Controller1 after failure (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check the same flow is still present
