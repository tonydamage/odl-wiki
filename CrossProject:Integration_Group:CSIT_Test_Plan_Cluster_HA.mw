
==OpenDaylight Clustering==
The OpenDaylight controller support a cluster-based high availability model. There are several instances of the OpenDaylight controller, which logically act as one logical controller. 

==Prerequisites==
* Make sure you meet the requirements documented [https://wiki.opendaylight.org/view/OpenDaylight_Controller:Clustering:HowTo here] before starting the cluster.
* Start a cluster of Opendaylight controllers with minimum 2 controllers:
** In the first controller machine do &nbsp;  '''./run.sh -Dsupernodes=Controller1_IP:Controller2_IP -start''' &nbsp;  with administrator priviledges
*** This will start Controller1 in a cluster as supernode
** In the second controller machine do &nbsp;  '''./run.sh -Dsupernodes=Controller1_IP:Controller2_IP -start''' &nbsp;   with administrator priviledges
*** This will start Controller2 in a cluster as supernode
* Start mininet (i.e. mininet> '''sudo mn --controller=remote,ip=Controller1_IP --topo tree,2''')
** This generates 3 nodes: s1 (*:01), s2 (*:02), s3 (*:03)
** At this point only the Controller1 get provisioned in OVS bridges (s1, s2, s3)
** s1 has 2 ports: 1,2 that connects to s2 and s3
** s2 has 3 ports: 1 to host1 (10.0.0.1), 2 to host2 (10.0.0.2) and 3 to s1
** s3 has 3 ports: 1 to host3 (10.0.0.3), 2 to host4 (10.0.0.4) and 3 to s1
* Now, puase the mininet (i.e. ''mininet> CTRL + Z'')
* In order to get Controller2 provisioned in OVS bridges do the following commands with administrator priviledges:
** ''$ ovs-vsctl set-controller s1 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''
** ''$ ovs-vsctl set-controller s2 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''
** ''$ ovs-vsctl set-controller s3 tcp:Controller1_IP:6633 tcp:Controller2_IP:6633''


* All the following test cases are written in Gherkin style ([http://robotframework.googlecode.com/hg/doc/userguide/RobotFrameworkUserGuide.html?r=2.8.3#different-test-case-styles behavior-driven]), and when possible the REST requests are provided.

==Test Content==

=== Cluster Manager ===

==== Two controllers running ====
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: When C1 is up and running
::: And c2 is up and running
::: Then the system is working with C1 and C2

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Check the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* Check the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check it that shows the same topology


* Controller1 fails
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: When C1 goes down
::: Then C2 takes over
::: And the system is working with C2

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. '''./run.sh -stop''')
* Check the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it does not show the topology
* Check the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology

* Controller2 fails
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: When C2 goes down
::: Then C1 takes over
::: And the system is working with C1

* Similar to the case when Controller1 fails.

* Controller1 recovers after failure
::: Given C1 goses down
::: And C2 takes over
::: When C1 recovers
::: Then the system is working with C1 and C2

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e.  '''./run.sh -stop''')
* Check the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it does not show the topology
* Check the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* Restart Controller 1 (i.e.  '''./run.sh -start''')
* Check the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it shows the topology
* Check the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check it that shows the topology


* Controller1 and Controller2 fail
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: When C1 goes down
::: And C2 goes down
::: Then the system does not work any more

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e.  '''./run.sh -stop''')
* Check the topology using Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/topology/default) and check that it does not show the topology
* Stop Controller2 (i.e.  '''./run.sh -stop''')
* Check the topology using Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/topology/default) and check that it does not show the topology

--------------------------------------------------------------------------------

=== Forwarding Rules Manager in a Cluster ===

* The installed flow can be seen in a cluster of two controllers
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: When a flow is installed in a bridge
::: Then C1 see the flow
::: And C2 see the flow

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
* Using Controller1 install flow1 on a bridge (i.e. PUT http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1 + [[CrossProject:Integration_Group:REST_Body#Flow1_Body|flow1 body]]) 
* See flow1 on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present
* See flow1 on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present


* The installed flow remains in the bridge after the controller failure
::: Given C1 a controller in cluster of two controllers
::: And C2 a controller in cluster of two controllers
::: And both controllers get provisioned on all OVS bridges
::: And a flow is installed in a bridge
::: And C1 see the flow
::: And C2 see the flow
::: And C1 goes down
::: When C1 recovers
::: Then C1 see the flow

* Follow the instruction on the '''Prerequisites''' section to have a cluster of two controllers, create a network using mininet, and then make sure that both controllers get provisioned on all OVS bridges.
** Using Controller1 install flow1 on a bridge (i.e. PUT http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1 + [[CrossProject:Integration_Group:REST_Body#Flow1_Body|flow1 body]]) 
* See flow1 on Controller1 (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present
* See flow1 on Controller2 (i.e. GET http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is present
* Restart Controller1 i.e.:
** Stop Controller1 using '''sudo ./run.sh -stop'''
** Start Controller1 in the background using '''sudo ./run.sh -start'''
* See flow1 on Controller1 after failure (i.e. GET http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and check flow is still present
