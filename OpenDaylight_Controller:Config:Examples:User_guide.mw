This user guide contains examples of controller configuration using yuma toolkit.

=Configuring thread pools with yangcli-pro=
==Requirements:==
yangcli-pro version 13.04-9.2 or later

==Connecting to plaintext TCP socket and ssh==
Currently TCP and SSH is exposed by the controller. The network interface and port are configured in configuration/config.ini . Current configuration of netconf is:

<source>
# Netconf startup configuration
netconf.tcp.address=0.0.0.0
netconf.tcp.port=8383

netconf.ssh.address=0.0.0.0
netconf.ssh.port=1830
</source>

To connect the yangcli-pro client, use following syntax:
<source>
yangcli-pro --user=a --password=a --transport=tcp --ncport=8383 --server=localhost
</source>
Authentication in this case is ignored. 

Alternatively, if you wish to use ssh use syntax : 

<source>
yangcli-pro --user=netconf --password=netconf --transport=ssh --ncport=1830 --server=localhost
</source>

For better debugging please include following arguments:

<source>
 --log=/tmp/yuma.log --log-level=debug4
</source>
Note that when log file is set, output will not appear on stdout.

==Configuring threadfactory==
'''threadfactory''' is a service interface that can be plugged into threadpools, defined in config-threadpool-api 
(see yang file [https://git.opendaylight.org/gerrit/gitweb?p=controller.git;a=blob;f=opendaylight/config/threadpool-config-api/src/main/yang/threadpool.yang;h=8f3064822be319dfee6fd7c7061c8bee14db268f;hb=refs/heads/master here]). 

The implementation we are going to use is called '''threadfactory-naming'''. This implementation will set a name for each thread created using configurable prefix and auto incremented index. Yang file can be found [https://git.opendaylight.org/gerrit/gitweb?p=controller.git;a=blob;f=opendaylight/config/threadpool-config-impl/src/main/yang/threadpool-impl.yang;h=a2366f285a0c0b8682b1093f18fb5ee184c9cde3;hb=refs/heads/master here].

* Launch yangcli-pro and connect to the server. 
* enter '''get-config source=running''' to see current configuration. Example output:
<source>
rpc-reply {
  data {
    modules {
      module  binding-broker-singleton {
        type binding-impl:binding-broker-impl-singleton
        name binding-broker-singleton
      }
    }
    services {
      service  md-sal-binding:binding-broker-osgi-registry {
        type md-sal-binding:binding-broker-osgi-registry
        instance  ref_binding-broker-singleton {
          name ref_binding-broker-singleton
          provider /modules/module[type='binding-broker-impl-singleton'][name='binding-broker-singleton']
        }
      }
    }
  }
}
</source>
* enter '''merge /modules/module'''
* you will be asked to enter string value for leaf <name>. This is name of the config module. Enter '''threadfactory-bgp'''
* next set identityref for leaf <type>. You can press Tab to see a list of available module names. Enter '''threadfactory-naming'''
* you will be then asked to choose case statement. Example output:
<source>
  1: case netty-threadgroup-fixed:
       leaf thread-count
  2: case netty-hashed-wheel-timer:
       leaf tick-duration
       leaf ticks-per-wheel
       container thread-factory
  3: case async-eventbus:
       container threadpool
  4: case threadfactory-naming:
       leaf name-prefix
  5: case threadpool-fixed:
       leaf max-thread-count
       container threadFactory
  6: case threadpool-flexible:
       leaf max-thread-count
       leaf minThreadCount
       leaf keepAliveMillis
       container threadFactory
  7: case threadpool-scheduled:
       leaf max-thread-count
       container threadFactory
  8: case logback:
       list file-appenders
       list rolling-appenders
       list console-appenders
       list loggers
</source>
In this case, we choose 4.
* Next fill string value for leaf <name-prefix>. Enter '''bgp'''
You should get an OK response from the server.
* Optionally issue '''get-config source=candidate''' to verify the change.
* Issue '''commit'''
* Issue '''get-config source=running'''. Example output:
<source>
rpc-reply {
  data {
    modules {
      module  binding-broker-singleton {
        type binding-impl:binding-broker-impl-singleton
        name binding-broker-singleton
      }
      module  threadfactory-bgp {
        type th-java:threadfactory-naming
        name threadfactory-bgp
        name-prefix bgp
      }
    }
    services {
      service  th:threadfactory {
        type th:threadfactory
        instance  ref_threadfactory-bgp {
          name ref_threadfactory-bgp
          provider /modules/module[type='threadfactory-naming'][name='threadfactory-bgp']
        }
      }
      service  md-sal-binding:binding-broker-osgi-registry {
        type md-sal-binding:binding-broker-osgi-registry
        instance  ref_binding-broker-singleton {
          name ref_binding-broker-singleton
          provider /modules/module[type='binding-broker-impl-singleton'][name='binding-broker-singleton']
        }
      }
    }
  }
}
</source>
==Configuring fixed threadpool==
Service interface '''threadpool''' is defined in config-threadpool-api. Implementation we will use is called '''threadpool-fixed''' that is defined in config-threadpool-impl (see previous chapter). This implementation creates a threadpool of fixed size. There are two mandatory attributes: size and dependency on a threadfactory.
* Issue '''get-config source=running'''. As you can see in last step of configuring threadfactory, /services/service node associated with it has instance name '''ref_threadfactory-bgp'''.
* Issue '''merge /modules/module'''
* Enter name '''bgp-threadpool'''
* Enter type '''threadpool-fixed'''
* Select appropriate case statement
* Enter value for leaf <max-thread-count>: '''100'''
* Enter '''threadfactory''' for attribute threadfactory/type. This is reference to /services/service/type, in other words, service interface.
* Enter '''ref_threadfactory-bgp'''
Server should reply with OK message.
* Issue commit
* Issue '''get-config source=running'''
Example output:
<source>
rpc-reply {
  data {
    modules {
      module  binding-broker-singleton {
        type binding-impl:binding-broker-impl-singleton
        name binding-broker-singleton
      }
      module  bgp-threadpool {
        type th-java:threadpool-fixed
        name bgp-threadpool
        threadFactory {
          type th:threadfactory
          name ref_threadfactory-bgp
        }
        max-thread-count 100
      }
      module  threadfactory-bgp {
        type th-java:threadfactory-naming
        name threadfactory-bgp
        name-prefix bgp
      }
    }
    services {
      service  th:threadpool {
        type th:threadpool
        instance  ref_bgp-threadpool {
          name ref_bgp-threadpool
          provider /modules/module[type='threadpool-fixed'][name='bgp-threadpool']
        }
      }
      service  th:threadfactory {
        type th:threadfactory
        instance  ref_threadfactory-bgp {
          name ref_threadfactory-bgp
          provider /modules/module[type='threadfactory-naming'][name='threadfactory-bgp']
        }
      }
      service  md-sal-binding:binding-broker-osgi-registry {
        type md-sal-binding:binding-broker-osgi-registry
        instance  ref_binding-broker-singleton {
          name ref_binding-broker-singleton
          provider /modules/module[type='binding-broker-impl-singleton'][name='binding-broker-singleton']
        }
      }
    }
  }
}
</source>

To see actual netconf messages, use the logging arguments described at top of this page.
To validate that a threadpool has been created, tool like VisualVM can be used.

[[File:Configure-threadpools-visualvm.png]]

= Logback configuration - Yuma =
This approach to configure logback will utilize a 3rd party cli netconf client from Yuma. We will modify existing console appender in logback and then call reset rpc on logback to clear its status list.

For initial configuration of controller and startup parameters for yuma consult threadpool example [[OpenDaylight Controller:Config:Examples:Threadpool|Threadpool configuration using Yuma]]. 

Start the controller and yuma cli client as in previous example.

There is no need to initialize configuration module wrapping logback manually, since it creates a default instance. Therefore you should see output containing logback configuration after execution of '''get-config source=running''' command in yuma:

<source>
rpc-reply {
  data {
    modules {
      module  singleton {
        type logging:logback
        name singleton
        console-appenders {
          threshold-filter ERROR
          name STDOUT
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} [%thread] %-5level %logger{36} - %msg%n'
        }
        file-appenders {
          append true
          file-name logs/audit.log
          name audit-file
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} %msg %n'
        }
        loggers {
          level WARN
          logger-name org.opendaylight.controller.logging.bridge
        }
        loggers {
          level INFO
          logger-name audit
          appenders audit-file
        }
        loggers {
          level ERROR
          logger-name ROOT
          appenders STDOUT
          appenders opendaylight.log
        }
        loggers {
          level INFO
          logger-name org.opendaylight
        }
        loggers {
          level WARN
          logger-name io.netty
        }
        rolling-appenders {
          append true
          max-file-size 10MB
          file-name logs/opendaylight.log
          name opendaylight.log
          file-name-pattern logs/opendaylight.%d.log.zip
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} [%thread] %-5level %logger{35} - %msg%n'
          clean-history-on-start false
          max-history 1
          rolling-policy-type TimeBasedRollingPolicy
        }
      }
      module  binding-broker-singleton {
        type binding-impl:binding-broker-impl-singleton
        name binding-broker-singleton
      }
    }
    services {
      service  md-sal-binding:binding-broker-osgi-registry {
        type md-sal-binding:binding-broker-osgi-registry
        instance  ref_binding-broker-singleton {
          name ref_binding-broker-singleton
          provider /modules/module[type='binding-broker-impl-singleton'][name='binding-broker-singleton']
        }
      }
    }
  }
}

</source>

=== Modifying existing console appender in logback ===
Start edit-config with merge option:
<source>
merge /modules/module
</source>

For '''Name''' of the module enter '''singleton'''.<br />
For '''Type''' enter '''logback'''.<br />
Pick corresponding case statement with name '''logback'''.<br />

We do not want to modify '''file-appenders''', '''rolling-appenders''' and '''loggers''' lists, so the answer to questions from yuma is N (for no):
<source>
Filling optional case /modules/module/configuration/logback:
Add optional list 'file-appenders'?
Enter Y for yes, N for no, or C to cancel: [default: Y]
</source>

But we want to modify '''console-appenders''', so we will answer '''Y''' to the question from Yuma:
<source>
Filling optional case /modules/module/configuration/logback:
Add optional list 'console-appenders'?
Enter Y for yes, N for no, or C to cancel: [default: Y]
</source>
This will start a new configuration process for console appender and we will fill following values:
* <encoder-pattern> %date{"yyyy-MM-dd HH:mm:ss.SSS z"} %msg %n,
* <threshold-filter> INFO,
* <name> STDOUT,

Answer N to the next question:
<source>
Add another list?
Enter Y for yes, N for no, or C to cancel: [default: N]
</source>

Notice that we changed the level for '''threashold-filter''' for '''STDOUT''' console appender from '''ERROR''' to '''INFO'''.
Now issue a '''commit''' command to commit changed configuration and the response from '''get-config source=running''' command should look like this:
<source>
rpc-reply {
  data {
    modules {
      module  singleton {
        type logging:logback
        name singleton
        console-appenders {
          threshold-filter INFO
          name STDOUT
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} [%thread] %-5level %logger{36} - %msg%n'
        }
        file-appenders {
          append true
          file-name logs/audit.log
          name audit-file
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} %msg %n'
        }
        loggers {
          level WARN
          logger-name org.opendaylight.controller.logging.bridge
        }
        loggers {
          level INFO
          logger-name audit
          appenders audit-file
        }
        loggers {
          level ERROR
          logger-name ROOT
          appenders STDOUT
          appenders opendaylight.log
        }
        loggers {
          level INFO
          logger-name org.opendaylight
        }
        loggers {
          level WARN
          logger-name io.netty
        }
        rolling-appenders {
          append true
          max-file-size 10MB
          file-name logs/opendaylight.log
          name opendaylight.log
          file-name-pattern logs/opendaylight.%d.log.zip
          encoder-pattern '%date{"yyyy-MM-dd HH:mm:ss.SSS z"} [%thread] %-5level %logger{35} - %msg%n'
          clean-history-on-start false
          max-history 1
          rolling-policy-type TimeBasedRollingPolicy
        }
      }
      module  binding-broker-singleton {
        type binding-impl:binding-broker-impl-singleton
        name binding-broker-singleton
      }
    }
    services {
      service  md-sal-binding:binding-broker-osgi-registry {
        type md-sal-binding:binding-broker-osgi-registry
        instance  ref_binding-broker-singleton {
          name ref_binding-broker-singleton
          provider /modules/module[type='binding-broker-impl-singleton'][name='binding-broker-singleton']
        }
      }
    }
  }
}
</source>

=== Invoking RPCs ===
==== Invoking Reset RPC on logback ====
Configuration module for logback exposes some information about its current state(list of logback status messages). This information can be accessed using get netconf operation or get command from yuma. Example response after issuing '''get''' command in yuma:
<source>
rpc-reply {
  data {
    modules {
      module  singleton {
        type logging:logback
        name singleton
        status {
          message 'Found resource [configuration/logback.xml] at
[file:/.../controller/opendaylight/distribution/opendaylight/target/distribution.opendaylight-
osgipackage/opendaylight/configuration/logback.xml]'
          level INFO
          date 2479534352
        }
        status {
          message 'debug attribute not set'
          level INFO
          date 2479534441
        }
        status {
          message 'Will scan for changes in
[[/.../controller/opendaylight/distribution/opendaylight/target/distribution.opendaylight-
osgipackage/opendaylight/configuration/logback.xml]] 
every 60 seconds.'
          level INFO
          date 2479534448
        }
        status {
          message 'Adding ReconfigureOnChangeFilter as a turbo filter'
          level INFO
          date 2479534448
        }
 ...
</source>

Logback also exposes an rpc called reset that wipes out the list of logback status messages and to invoke an rpc with name '''reset''' on module named '''singleton''' of type '''logback''', following command needs to be issued in yuma:
<source>
reset context-instance="/modules/module[type='logback' and name='singleton']"
</source>

After an ok response, issuing '''get''' command should produce response with empty logback status message list:
<source>
rpc-reply {
  data {
    modules {
      module  singleton {
        type logging:logback
        name singleton
      }
    }
  }
}
</source>

This response confirms successful execution of the reset rpc on logback.
==== Invoking shutdown RPC ====
This command entered in yuma will shut down the server. If all bundles do not stop correctly within 10 seconds, it will force the process to exit.
<source>
shutdown context-instance="/modules/module[type='shutdown' and name='shutdown']",input-secret="",max-wait-time="10000",reason="reason"
</source>
