<onlyinclude>
The OpenFlow renderer is a network virtualization solution based on Open vSwitch, OpenFlow, and OVSDB.  It allows constructing virtual networks that use group-based policy among devices attached to Open vSwitch interfaces.  The features include:
* Routing when required between endpoint groups, including serving as a distributed default gateway
* Optional broadcast within a bridge domain.
* Management of L2 broadcast protocols including ARP and DHCP to avoid broadcasting.
* Layer 2-4 classifiers for policy between endpoint groups, including connection tracking/reflexive ACLs.
* Service insertion/redirection
</onlyinclude>
== Network Architecture ==

=== Network Topology ===
[[File:Overlay design red tunnel.png|framed|Figure 1: An example of a supported network topology, with an underlying IP network and hypervisors with Open vSwitch.  Three endpoint groups exist with different subnets in the same layer 3 context.  A tunneled path is shown between two red virtual machines on different VM hosts.]]
The network architecture is an overlay network based on VXLAN or similar encapsulation technology, with an underlying IP network that provides connectivity between hypervisors and the controller.  The overlay network is a full-mesh set of tunnels that connect each pair of vSwitches.

The "underlay" IP network has no special requirements though it should be set up with ECMP to the top-of-rack switch for the best performance, but this is not a strict requirement for correct behavior.  Also, the underlay network should be configured with a path MTU that's large enough to accommodate the overlay tunnel headers.  For a typical overlay network with a 1500 byte MTU, a 1600 byte MTU in the underlay network should be sufficient.  If this is not configured correctly, the behavior will be correct but it will result in fragmentation which could have a severe negative effect on performance.

Physical devices such as routers on the IP network are trusted entities in the system since these devices would have the ability to forge encapsulated packets.

=== Control Network ===
The security of the system depends on keeping a logically isolated control network separate from the data network, so that guests cannot reach the control network.  Ideally, the network is kept isolated through an out-of-band control network.  This can be accomplished using a separate NIC, a special VLAN, or other mechanism.  However, the system is also designed to operate in the case where the control traffic and the data traffic are on the same network as well and isolation is enforced.

In Figure 1, the control network is shown as 172.16/16.  The VM hosts, and controllers all all have addresses on this network, and communicate using OpenFlow and OVSDB on this network.  In the example, the router is shown with an interface configured on this network as well; this works but in practice it is preferable to isolate this network by accessing it through a VPN or jump box if needed.  Note that there is no requirement that the control network be all in one subnet.

The router is also shown with an interface configured on the 10/8 network.  This network will be used for routing traffic destined for internet hosts.  Both the 172.16/16 and 10/8 networks here are isolated from the guest address spaces.

=== Overlay Network ===
Whenever traffic between two guests is in the network, it will be encapsulated using a VXLAN tunnel (though supporting additional encapsulation formats could be configured in the future).  A packet encapsulated as VXLAN contains:
* Outer ethernet header, with source and destination MAC
* Outer IP header, with source and destination IP address
* Outer UDP header
* VXLAN header, with a virtual network identifier (VNI)
* Encapsulated original packet, which includes:
** Inner ethernet header, with source and destination MAC
** (Optional) Inner IP header, with source and destination IP address



=== Forwarding ===

* on-the-wire format (vxlan, ixvlan?, geneve?, etc.)  Hardware acceleration is a major driver here.
* OVS configuration
* Mapping of interfaces to endpoint groups
* Mapping of endpoint groups to vnids/classes


==== OpenFlow Tables ====
* tables we'll use
* OVS configuration needed to run the system


==== Bridging ====
* How to distribute broadcast packets
* Convert ARP/DHCP to unicast (through packetin)
* How and when we learn devices
** Pure orchestration vs. learning
* Port security features and configuration

==== Routing ====
* Where, when, how do we perform routing actions?
* Any configuration required

== Renderer Architecture ==
Main pieces:
* Switch Manager: Manage connected switch configuration using OVSDB.  Maintain overlay tunnels.
* ARP/DHCP manager: convert ARP and DHCP into unicast
* Endpoint manager: Optionally learn endpoints based on simple rules that map interfaces to endpoint groups.  Can add additional rules in the future.  Keep endpoint registry up to date.  If disabled, then an orchestration system must program all endpoints and endpoint mappings.
* Topology manager: Maintain topology and pathing information as needed.  Compute spanning trees for broadcast, etc.
* Routing manager: Maintain routing information
* Policy manager: subscribe to renderer common infrastructure and endpoint registry and manage the state of the flow tables in OVS.
