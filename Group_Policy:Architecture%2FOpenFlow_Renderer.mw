<onlyinclude>
The OpenFlow renderer is a network virtualization solution based on Open vSwitch, OpenFlow, and OVSDB.  It allows constructing virtual networks that use group-based policy among devices attached to Open vSwitch interfaces.  The features include:
* Routing when required between endpoint groups, including serving as a distributed default gateway
* Optional broadcast within a bridge domain.
* Management of L2 broadcast protocols including ARP and DHCP to avoid broadcasting.
* Layer 2-4 classifiers for policy between endpoint groups, including connection tracking/reflexive ACLs.
* Service insertion/redirection
</onlyinclude>
== Network Architecture ==

=== Network Topology ===
[[File:Network topology.png|framed|center|An example of a supported network topology, with an underlying IP network and hypervisors with Open vSwitch.]]
The network architecture is an overlay network based on VXLAN or similar encapsulation technology, with an underlying IP network that provides connectivity between hypervisors and the controller.  The overlay network is a full-mesh set of tunnels that connect each pair of vSwitches.

The "underlay" IP network has no special requirements though it should be set up with ECMP to the top-of-rack switch for the best performance, but this is not a strict requirement for correct behavior.

Physical devices such as routers on the IP network are trusted entities in the system since these devices would have the ability to forge encapsulated packets.

=== Control Network ===
The security of the system depends on keeping a logically isolated control network separate from the data network, so that guests cannot reach the control network.  Ideally, the network would be out-of-band through a dedicated control NIC, but in practice this requirement adds a lot of cost and complexity to deploying the solution.

=== Overlay Tunnels ===
* on-the-wire format (vxlan, ixvlan?, geneve?, etc.)  Hardware acceleration is a major driver here.
* OVS configuration

* Mapping of interfaces to endpoint groups
* Mapping of endpoint groups to vnids/classes

=== OpenFlow Tables ===
* tables we'll use
* OVS configuration needed to run the system

=== Forwarding ===

==== Bridging ====
* How to distribute broadcast packets
* Convert ARP/DHCP to unicast (through packetin)
* How and when we learn devices
** Pure orchestration vs. learning
* Port security features and configuration

==== Routing ====
* Where, when, how do we perform routing actions?
* Any configuration required

== Renderer Architecture ==
Main pieces:
* Switch Manager: Manage connected switch configuration using OVSDB.  Maintain overlay tunnels.
* ARP/DHCP manager: convert ARP and DHCP into unicast
* Endpoint manager: Optionally learn endpoints based on simple rules that map interfaces to endpoint groups.  Can add additional rules in the future.  Keep endpoint registry up to date.  If disabled, then an orchestration system must program all endpoints and endpoint mappings.
* Topology manager: Maintain topology and pathing information as needed.  Compute spanning trees for broadcast, etc.
* Routing manager: Maintain routing information
* Policy manager: subscribe to renderer common infrastructure and endpoint registry and manage the state of the flow tables in OVS.
