== Discussed Work Items ==

=== OpenFlow 1.3 Support ===

There was some consensus that this should proceed in two phases. The first phase is updating the SAL's interfaces to make it possible to express OpenFlow 1.3 constructs across it. The second phase is to actually provide an implementation for OpenFlow 1.3 as a southbound plugin.

In terms of phase 1, there are really two major shifts in OpenFlow 1.3 from 1.0:
# There are now multiple tables each with their own flowmods, statistics, etc.
# You need some way to query for, request, or negotiate these tables and their properties, i.e., what tables exist, what fields can you match on in each table, and what actions are allowed in each table

In the very near-term, it would be useful for someone to read over the OpenFlow 1.3 spec and add new calls for the SAL that specify the table number in appropriate places and, at the very least, add calls that allow for a representation of TABLE_FEATURES structures (in the OpenFlow 1.3 spec in section A.3.5.5) to be passed back and forth to query a given switch's current tables and/or request new tables.

Otherwise, I think that the implementation mostly involves adding fields to various existing data types including adding the ability to match on MPLS and IPv6 headers and other changes.

There will obviously be a lot more details as we make progress, but this should be at least a starting point.

=== Porting/Implementing Tunnel Manager ===


=== Porting net-virt-platform's HostTracker ===

* Initial plan here: [[D-E Proposal:Host Tracker Plan|Host Tracker Plan]]
* Initial commit from Rob Adams here: [https://git.opendaylight.org/gerrit/#/c/438/ https://git.opendaylight.org/gerrit/#/c/438/]
* A group of people made progress starting with Rob Adams's commit and are in the process of pushing that progress back in a way that makes sense. The progress is described on a controller-dev mail thread starting here: [https://lists.opendaylight.org/pipermail/controller-dev/2013-June/000652.html https://lists.opendaylight.org/pipermail/controller-dev/2013-June/000652.html]

=== Threading Model/Performance ===

[[File:Controller-threading-now.png|thumb|right|300px|Current threading model.]]
[[File:Controller-threading-proposed.png|thumb|right|300px|Proposed threading model.]]

''Written up by Colin Dixon based on conversations with Ed Warnicke, Davide Erickson, Chris Price, Bhushan Kanekar and I'm sure others.''

Essentially, there are two models for how you can handle tasks in a system like OpenDaylight:
# You can have run-to-completion event handling where the thread that receives an event does all of the work to handle the event and then comes back to handle the next event.
# You can have staged event handling where there is a pool of threads to handle each kind of event and it typically picks up events from one place, processes them and possibly produces events of it's own and puts them in other event queues.

Right now, we mostly have the second solution. For example, the SwitchHandler takes PACKET_IN events (actually switch message events I think) and then hands them off to a queue DataPacketServices in the SAL, that then dequeues the event and calls all of the listeners which in turn can generate PACKET_OUT or flow programming events which are picked up from their respective queues and so on.

The advantage of this approach is that, in theory, the system is always responsive and a given slow module/bundle can't screw over the performance of the whole system. There are several disadvantages though:
* You have to manually tune the balance of threads at each stage and, to actually get good performance, you'll want do dynamically monitor and tune this to workload which is a pain in the ass.
* Managing the queues between stages is complicated. Often there is a single lock for the queue and this lock creates tons of contention. Having multiple queues doesn't solve the problem completely either since you have to create a thread that polls all of the queues which is inefficient, destroys processor affinity and can screw up your cache lines.
* In practice, for most of your modules, they have to get their work done and so even if they're slow, you eventually have to wait for them and starving them of threads is only going to cause the rest of your system to be slowed down to the same rate.

Instead, you can basically have one giant pool of threads and just have them servicing switch events and running them to completion. Since the same pool of threads goes through all of the modules, this is auto-tuning as all threads wind up being able to do all work in all modules/bundles. There are no queues to manage carefully. You still may have to do some locking in some places to order events, but it's a lot less than having a single queue/lock per stage.

The only place this breaks down, is that some events have to be processed on a particular controller instance, for instance the one with the attached switches. To do this, you need to have some queue that enables the transferring of the event from one instance to another. Thus this approach really becomes "have as few stages as possible" and that's one input stage that does all of the work and enqueues outuput actions to be sent to the corresponding switches.

There's also a few timer-based events that get fired and presumably we'll need a way to service northbound events, i.e., REST API calls, but ideally I'd want to have a single pool of threads handling all four sources of events:
# Timers firing
# Messages from sockets, i.e., switches
# New items on queues, i.e., the flow programming queue or packet_out queue
# REST API calls

I suspect that this is doable in the long run with a framework like Netty, but in the short term, we could have a few different pools of threads which are coarsely tuned and punt further optimization into the future.

As a start, I've pushed a patch which removes the RX queue from DataPacketServices and the patch can be found here: [https://git.opendaylight.org/gerrit/#/c/459/ https://git.opendaylight.org/gerrit/#/c/459/]

=== Consistent Data Store ===


=== Asynchronous Flow Programing ===

Currently, most, and possibly all, of the applications and network services in the OpenDaylight <tt>controller</tt> project make use of synchronous flow programming APIs when they install rules on switches. In practice this severely limits performance because each programming request blocks the calling thread for what is often around 10 milliseconds.

Switching to using the asynchronous flow programming APIs should drastically reduce this penalty, but requires that the logic in the calling applications and services be changed to handle asynchronous notifications of whether rules were successfully installed or not.

=== PACKET_IN handling/filtering ===
