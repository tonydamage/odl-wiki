[[requirements]]
== Requirements

[[helium]]
=== Helium

1.  Move InMemoryDataStore implementation to it's own bundle
2.  *Sharding :* Distribute data in the datastore into shards such that
a subset of shards can be located in any cluster member. *Distributed
transactions across shards WILL NOT be supported*. A transaction on a
single shard MUST be supported. The Sharding strategy in helium will be
fixed i.e all of a modules data will be in a single shard.
3.  *Persistence :* A shard should be backed by a persistent store so
that when a cluster member is restarted the shard can be reconstructed
from the persisted data
4.  *Replication :* A shard should be replicated to configurable number
( 2 for Helium) of replicas
5.  *Clustering Services:* An OSGi service which provides information
about an akka cluster
6.  *Monitoring :* There MUST be logging/monitoring of transactions and
data change notifications that would help an app developer tune data
access. For example we know that remote-reads could be expensive so we
should log reads that took an extra-ordinary amount of time or which is
trying to get too much data. Similarly for data change notifications we
should log if a registration results in too many notifications or the
data delivered by that notification is too much or if delivery is taking
too much time.
7.  *Remote Rpc :* A mechanism to invoke a method on a remote MD-SAL
providers
8.  *Remote Notifications :* A mechanism to notify remote listeners of a
notification to remote MD-SAL consumers

[[beyond-helium]]
=== Beyond Helium

1.  '''Data Aggregation : ''' Allow data from multiple shards to be
returned in a single read request
2.  *Querying :* All querying for data using some sort of query API
which will work along with the aggregation
3.  *Data Validator :* Support validation of data that is being inserted
into the shard

[[questions-on-requirements]]
=== Questions on requirements

1.  What should the data size be?
2.  What kind of performance is expected? How many data updates/second?

[[general-design-principles]]
== General Design Principles

1.  The Clustered/Sharded Data Store should be a drop in replacement for
the InMemory Data Store
2.  Reuse the InMemory Data Store to represent a shard because it
already takes care of maintaining a tree which contains data for all
modules and can therefore be scaled down to deal with data for a single
module
3.  Use Akka for doing operations on remote shards. Akka seems to fit
very well with the existing design of md-sal as it is already based on
the actor model.
4.  The sharding strategy should be customizable. ODL should ship with a
default sharding strategy (which will be static and pre-determined for a
given cluster size, similar to the APIC design; applications will be
responsible for determining the sharding of their data). The sharding
strategy determines the location of a shard possibly based on the
instance identifier of a data object and the collection of the cluster
members
5.  Application/Services reading/writing to the shard should be
co-located with the shard to minimize remote transactions as much as
possible. Only one application should be the logical owner of a shard.
When other applications need that data, they should either utilize the
APIs provided by the owner application or subscribe to data change
notifications.
6.  In a cluster atleast 3 replicas must be configured for each shard
(and they should be on different hosts). Only when a piece of data is
replicated to atleast one replica is the "transaction" considered
successful.

[[problems-with-sharding-strategy]]
==== Problems with Sharding Strategy

1.  Migration of shards based on changes in sharding strategy?
2.  For Helium - build a REST api to specify the cluster configuration

[[akka]]
=== Akka

We are going to rely heavily on akka to provide us the building blocks
for our clustering solution. The main components of akka that we will
use are,

1.  http://doc.akka.io/docs/akka/snapshot/java/remoting.html[Akka
Remoting]
2.  http://doc.akka.io/docs/akka/snapshot/java/cluster-usage.html[Akka
Clustering]
3.  http://doc.akka.io/docs/akka/snapshot/java/persistence.html[Akka
Persistence]

[[design]]
== Design

[[components]]
=== Components

image:High Level Design.png[General
Concepts|x600px,title="General Concepts|x600px"]

[cols=",",options="header",]
|=======================================================================
|Component Name |Description
|ClusteringConfiguration a|
The ClusteringConfiguration represents information about the cluster.
The information it provides would be roughly the following,

1.  What are the member in the cluster?
2.  Which shards live on each node?
3.  What data goes in each shard?

|ClusteringService a|
The ClusteringService would have the following responsibilities

1.  Read the cluster configuration. Where it reads the cluster
configuration from should not matter. Initially we could even read the
configuration from the file system. Overtime ofcourse we could have a
"primary" node come up with a cluster configuration and distribute it to
the other members in the cluster.
2.  Resolve the node name to actual host name/ip
3.  Maintain a registration of components that are interested in being
notified of member status changes

|DistributedDataStore a|
The DistributedDataStore would have the following responsibilities

1.  Implement the DOMDataStore so that we could replace the
InMemoryDataStore with the DistributedDataStore
2.  Create the local shard actors as per the cluster configuration
3.  Create the listener wrapper actors when a consumer registers a
listener.

|Shard a|
A Shard would be a *processor* which contains some of the data in the
system. Since a Shard is an actor you would communicate with it using
messages. The messages passed to a shard would for the most part be
similar to the operations on the DOMDataStore interface.

Since the Shard is a *Processor* as per akka-persistence it is a special
actor which when passed a *Persistent* message will log it to a journal.
This journal along with snapshots would be used as a method to recover
the state of the DataStore. The state of the Shard would be maintained
in an InMemoryDataStore object.

The MD-SAL DataStore supports three phase commit. The Shard will
therefore also provide the functions of the ThreePhaseCommitCohort.

|ShardTransaction |A ShardTransaction would be an actor which wraps an
InMemoryDataStoreTransaction. Any operation that needs to be done on a
transaction - namely ""read"", ""write"", ""delete"" and ""ready"" would
be fronted by the ShardTransaction. The ShardTransaction will also
maintain the state of any writes/deletes that happen on a transaction.
We will call this state the "transactionLog". The transactionLog would
then be used during commits to persist a transaction to a journal which
will be written onto the disk using akka's persistence module. The
journal will then be used when a controller shards up to reconstruct the
state of a shard.

|TransactionProxy |The TransactionProxy will hold a reference to a
collection of remote ShardTransaction actor and when returned to the
consumer of the DistributedDataStore could be used to invoke the
transaction operations on any remote ShardTransaction object depending
on the instance identifier of the object.

|ListenerWrapper |The ListenerWrapper is an actor that would represent a
local data change listener. It would be created as a remote actor on the
node where the Data Change registration is done.

|ListenerProxy |The ListenerProxy represents a remote data change
listener. When the local Shard issues a data change notification it is
the ListenerProxy's responsibility to send that data change notification
over to the remote ListenerWrapper actor.

|ShardCommitCohort |A ShardCommitCohort would be an actor which wraps he
InMemoryDataStoreCommitCohort. Any operation that needs to be done on a
three phase commit cohort namely , "canCommit", "preCommit", "commit"
and "abort" would be fronted by the ShardCommitCohort

|ThreePhaseCommitCohortProxy |The ThreePhaseCommitCohortProxy holds a
reference to a collection of ShardCommitCohorts. It implements the
DOMStoreThreePhaseCommitCohort interface and any operation done on the
proxy is invoked on every ShardCommitCohort in the collection.
|=======================================================================

[[packaging]]
=== Packaging

The following osgi bundles should be created,

1.  MD-SAL InMemoryDataStore Implementation (this needs to be moved out
of sal-dom-broker)
2.  MD-SAL Clustering Service API and implementation
3.  MD-SAL Distributed DataStore, Remote Rpc Provider and Remote
Notifications

[[configuration]]
=== Configuration

Cluster configuration defines the members of the cluster and what lives
within it. This configuration can be static or dynamic. To make things
simple we could go with a static configuration for Helium. The
configuration could be defined in a file or files which could be put in
the ODL distribution. When the ODL controller is started up we would
pass the configuration file to it.

When the MD-SAL Clustering Service bundle comes up it could look at
which specific configuration needs to be loaded, reads it from disk and
initializes itself.

Clustering configuration would be as follows,

[[modules.conf]]
==== modules.conf

modules.conf defines all the modules in the system, the shards for those
modules and the cluster members on which the replicas of those shards
should exist. Which replica would be primary depends on the order of the
replica list.

-------------------------------------

modules = [
    {
        name = "inventory"
        shards = [
            {
                name = "shard-1"
                replicas = [
                    "member-2"
                    "member-1"
                    "member-3"
                ]
            }

            {
                name = "shard-2"
                replicas = [
                    "member-1"
                    "member-2"
                    "member-3"
                ]
            }

        ]
    },
    {
        name = "topology"
        shards = [
            {
                name = "module-shard"
                replicas = [
                    "member-2"
                    "member-1"
                    "member-3"
                ]
            }
        ]
    }

]
-------------------------------------

[[module-sharding-strategies.conf]]
==== module-sharding-strategies.conf

The module-sharding-strategies.conf file defines each module and the
strategy that needs to be used for that module.

---------------------------------

module-sharding-strategies = [
    {
        module-name : "inventory"
        strategy : "module"
    },
    {
        module-name : "topology"
        strategy : "module"
    }

]
---------------------------------

Now since we are planning to use akka pay special attention to the
""role-name"". The role-name that one uses should correspond to the
role-name specified for this node in the akka-cluster configuration.
Right now I can see this as a potential area where mistakes could be
made as two separate configuration files need to be kept in sync (need
to think of a clean solution for this).

[[discovery]]
=== Discovery

ClusteringService will be responsible for Discovery and all related
functions. It will depend on
http://doc.akka.io/docs/akka/snapshot/java/cluster-usage.html[akka-clustering]
to identify the members of the cluster.

When the ClusteringService comes up it first checks for the state of the
cluster. It looks up all the members in the cluster and verifies that
all the roles defined in the cluster-configuration are fulfilled by the
cluster membership. Once all the members with the required roles are up
and running the Clustering Service notifies it's listeners that the
controller is now open for business.

[[sharding-and-data-access]]
=== Sharding (and data access)

The DistributedDataStore creates a ShardingManager. The ShardingManager
looks at the configuration of the cluster and automatically creates all
the local shards. The ShardingManager also provides a mechanism to
locate the shard to which a message needs to be sent.

Data is sharded at a sub-tree level. In other words, If a Tree Node
belongs to a shard, All the sub-tree nodes will also belong to the same
shard. Sharding strategy will be pluggable at a module level. Default
sharding strategy will be to allocate a shard per top level module. It
is expected that few modules such as Inventory and Topology will be the
heavy hitters on the data store and data may need to be further sharded
at a sub-module level. In those cases, Sharding strategy can specify the
path prefix to shard on. When data needs to be read or written to a
shard the ShardingManager will pass the module name of the data and the
instance identifier of the data to a ShardingStrategy which will then
locate the shard on which the data belongs.

image:ShardManagement.png[ShardManagement.png,title="ShardManagement.png"]

[[creating-a-new-transaction]]
==== Creating a new transaction

image:Create a new transaction.png[Create a new transaction.png,title="Create a new transaction.png"]

In the ""current option"" when a consumer tries to create a new
transaction on the DistributedDataStore we have create a transaction on
some remote Shard(s). Why do we need to create a transaction on multiple
Shards? Because if the transaction is created using the current
DOMDataStore API at the outset we are not told which "module" we want to
do the transaction on.

There are a few ways in which we could fix this,

1.  When creating the transaction pass the instance identifier of the
object on which you want to do the transaction. I think this is a simple
option because it introduces a more restrictive API that forces the
consumer to decide the Shard on which she would like to operate.
2.  We do not create remote transactions up-front. When a CRUD operation
is done on the TransactionProxy then the TransactionProxy could first
create a transaction on the remote Shard and then only do that
operation. Once the transaction is created though we allow it to live
till it is committed. This is also workable and the overall behavior may
not be much worse than the current option.

[[readwrite-on-a-transaction]]
==== Read/Write on a transaction

image:Read on a transaction.png[Read on a transaction.png,title="Read on a transaction.png"]

image:Write on a transaction.png[Write on a transaction.png,title="Write on a transaction.png"]

[[readying-a-transaction-for-commit]]
==== Readying a transaction for commit

image:Readying a transaction for commit.png[Readying a transaction for commit.png,title="Readying a transaction for commit.png"]

[[committing-a-transaction]]
==== Committing a transaction

We plan to use 3-phase commit semantics for committing transactions. The
3-phase commit protocol works as shown in the following diagram. This
would imply that we do guarantee distributed transactions but in-reality
we may not be able to. If 3-phase commit is not important or possible
then we should probably not even have it on the DOMStore interfaces.

If 3-phase commit is not to be supported we can simply have a commit on
the transaction.

image:Three-phase commit diagram.png[Three-phase commit diagram.png,title="Three-phase commit diagram.png"]

The coordinator in our case would be the ThreePhaseCommitCohortProxy
(shown as ThreePhaseCommitProxy in the following diagram) and the actual
cohorts will be the ShardCommitCohort's.

image:Committing a transaction.png[Committing a transaction.png,title="Committing a transaction.png"]

[[replication]]
==== Replication

To be implemented using the RAFT consensus algorithm
https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf

[[aggregation]]
==== Aggregation

If data from a single module is put into multiple shards it is possible
that a read may require data from multiple shards to be retrieved
aggregated and returned to the consumer.

[[querying]]
==== Querying

With the current DOM-Store the only type read supported is to read a
piece of data based on it's identifier. Querying for data based on
attributes of the data is not supported. This capability needs to be
added to the DOM-Store interfaces and optimized (indexed) for fast
access.

Following types of queries will be supported.

* Query based on instance identifier
* Query based on object class (Extend YANG with the notion of an Object
Class?)
* Query based on attribute filters
* Scoped query based on Object Class
* Scoped query on attribute filters

Data Store will implement B-Tree Indexes to support efficient queries
based on attribute filters. YANG data modelers are in the best position
to define the expected query patterns and therefore define the indexes
which need to be created. YANG language extensions will be provided to
simplify the definition of query indexes.

[[data-change-notifications]]
==== Data Change Notifications

[[registration]]
===== Registration

Data change notifications can be thought of as continuous queries where
the query is specified once and instead of returning the matching data
immediately system sends notifications when matching data appear in the
data tree. As such, data change subscriptions can be customized just
like the data queries.

Following types of subscriptions will be supported.

* Query based on instance identifier
* Query based on object class (Extend YANG with the notion of an Object
Class?)
* Query based on attribute filters
* Scoped query based on Object Class
* Scoped query on attribute filters

Question: Do we need to support the notion of a custom filter where a
piece of java code can be supplied by the consumer which will be invoked
for every matching node during notification and only those nodes which
pass the filter will be returned

image:Register a DataChangeListener.png[Register a DataChangeListener.png,title="Register a DataChangeListener.png"]

[[notification]]
===== Notification

image:DataChange notification.png[DataChange notification.png,title="DataChange notification.png"]

[[shard-primary-replica-election]]
==== Shard Primary Replica Election

As per the RAFT consensus algorithm
https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf

[[data-validation]]
==== Data Validation

As data is added into the data store there might be a need for
application developers to write a validator to verify that the data
being written is correct. While Validators and DataCommitHandlers are
both invoked during commit there is a difference between the two. The
DataCommitHandler is a broker concept whereas the Validator would be a
DataStore concept. Validators would be associated with a single shard
and remote registrations of Validators would not be allowed.

[[recovery]]
=== Recovery

To enable recovery we could use Akka's persistence module. One caveat to
using Akka's persistence module is that it is experimental. Only a POC
will determine if it is usable for our purposes. If it is not we may
need to roll our own but in general the principles would be the same.

Here is what we need to do to enable proper recovery,

1.  Write a journal where each successful transaction on a given shard
is logged
2.  From time to time write the state of the datastore as a snapshot.
The idea of using a snapshot is to enable faster recovery.
3.  When the controller is restarted first re-construct the state of the
local shard from the saved snapshot. Then play the transactions in the
transaction journal onto the datastore. When both are complete the Shard
is ready for business.

The above can be provided by Akka.

In addition we will also need to ensure that replica being recovered is
in sync with the primary replica. One way to ensure this would be for
the primary replica to send all "undelivered" messages from it's
transaction log to the current replica.

[[availability]]
=== Availability

High availability will be enabled by,

1.  Replicating shard data to a configurable number of replicas
2.  Detecting failure of nodes and switching the primary replica
3.  Shard priority order will be fixed for one of the secondary replicas
to become leader.

[[replication-1]]
===== Replication

After a successful local commit on the primary replica a replication
message would be sent to all the secondary replicas. The secondary
replicas would write this message into the journal and then commit the
message as a transaction on the InMemoryDataStore. A Transaction is not
considered to be complete for external purposes until the data is
written to the replication journal on at least one replica.

[[node-failure-detection]]
===== Node Failure detection

The ClusteringService would be monitoring the state of all the members
of a cluster. When it detects node failure it will notifiy it's
listeners of the failure of that node. One of it's listeners would be
the DistributedDataStore which on receipt of the failure will then send
a message to all it's TransactionProxy's informing it of the failure of
that node and that failure would be then propagated forward by the
TransactionProxy's to the ThreePhaseCommitCohortProxy. If the node
failure affects the transaction - that is if the failure is on a node
where one of the transactions shard resides then the transaction will be
marked as failed and any further action on it will throw an exception.
The same applies on the ThreePhaseCommitCohortProxy.

[[monitoring]]
=== Monitoring

An Akka cluster can be monitored using a variety of commercial
monitoring software like AppDynamics or NewRelic. TypeSafe used to have
a product called TypeSafe Console which has been discontinued.

Scenario

`    - Single node cluster` +
`    - Primary replica for all shards local` +
`    - No secondary replicas`

- Replication : off

Scenario

`    - Two node cluster` +
`    - Both nodes running`

- Replication : on

Scenario

`    - Two node cluster` +
`    - Node 1 running` +
`    - Node 2 running -> down`

- Node 1 : primary - Cluster operations : suspended

Scenario

`    - Two node cluster` +
`    - Node 1 running` +
`    - Node 2 down -> running` +
`    `

- Node 1 : primary - Node 1 replicates all data to Node 2 - Cluster
operations : resumed

Scenario

`    - Two node cluster` +
`    - Node 1 running -> down` +
`    - Node 2 running`

- Node 2 : primary - Cluster operations : suspended

Scenario

`    - Two node cluster` +
`    - Node 1 down -> running` +
`    - Node 2 running`

- Node 2 : primary - Cluster operations : resumed

Scenario

`    - Two node cluster` +
`    - Node 2 comes up first` +
`    - Node 1 comes up second`

- Node 2 : primary

Scenario

`    - Three node cluster` +
`    - Node 1 comes up first ` +
`    - Node 2 comes up second` +
`    - Node 3 comes up third`

- Node 1 : primary

Scenario

`    - Three node cluster` +
`    - Node 1 running` +
`    - Node 2 running -> down` +
`    - Node 3 running`

- Node 1 : primary - Node 1 : starts storing messages destined for Node
2 - Node 3 : saves replicated messages for Node 2 - Node 3 : fully
replicated

Scenario

`    - Three node cluster` +
`    - Node 1 running` +
`    - Node 2 down -> running` +
`    - Node 3 running`

- Node 1 : primary - Node 1 : replicates stored messages to Node 2 -
Node 3 : discards the stored messages for Node 2 - Node 3 : fully
replicated

Scenario

`    - Three node cluster` +
`    - Node 1 running -> down` +
`    - Node 2 down -> running` +
`    - Node 3 running`

- Node 3 : primary - Node 3 : replicates stored messages to Node 2

Scenario

`   - Three Node cluster` +
`   - Node 1 down -> running` +
`   - Node 2 running` +
`   - Node 3 running` +
`   `

- Node 3 : primary - Node 1 : discards any stored messages for Node 2

Scenario

`   - Three Node cluster` +
`   - Node 1 running -> down` +
`   - Node 2 running -> down` +
`   - Node 3 running` +
`   `

Node 3 : primary Cluster operations : suspended

Scenario

`   - Three Node cluster` +
`   - Node 1 running -> down` +
`   - Node 2 running -> down (with unreplicated messages)` +
`   - Node 3 running -> down (with unreplicated messages)` +
`   - Node 1 down -> running`

Node 1 : primary Cluster operations : suspended

Scenario

`   - Three Node Cluster` +
`   - Node 1 running` +
`   - Node 2 down -> running (is more uptodate)` +
`   - Node 3 down` +
`   `

Node 2 : primary

[[performance-measurementtuning]]
=== Performance Measurement/Tuning

[[concurrency]]
==== Concurrency

Akka has the concept of a
http://doc.akka.io/docs/akka/snapshot/java/dispatchers.html[dispatcher]
which is essentially a means for Akka to process messages for an actor.
What this really boils down to is what kind of thread model we want to
use with our actors. Akka offers a few configurable choices.

[[serialization]]
==== Serialization

Serializing objects over the wire is going to likely be an expensive
operation. We need to figure out which type of serialization works best
for us.

[[remote-rpc]]
== Remote RPC

When there is a cluster of controllers, there can be cases where one
member of the cluster is asked to execute RPC call on a device which is
controlled by another member. Remote RPC broker would route such
requests to another member which controls the device. In general, any
RPC request for which provider is NOT found locally, is routed to
another member in the cluster who has a provider for it.

There are 2 components:

1.  *Route Registry* : Maintains list of registered RPCs per member of
the cluster
2.  *Remote RPC Broker* : Routes RPC calls to the cluster member where
RPC is registered and handles response. It also acts as a listener for
incoming RPC requests from another cluster member.

 +
 +
=== Route Registry === This is a container for registered RPCs per
cluster member (controller)

`var registry: mutable.Map[Address, Bucket] = mutable.Map.empty[Address, Bucket]` +
` final case class Bucket(` +
`  version: Long,` +
`  rpcs: List[String]` +
`)`

*Address*: Cluster member address. Its really Akka address of remote
actor system where Route Registry actor is managed +
*Bucket*: Its a container for list of registered RPCs with a version.
Version is a timestamp. +
 +
The registry is replicated across all members of the cluster.
Replication is done using *Gossip protocol* and follows *eventual
consistency* model. +
 +
Registry is front-ended with an Actor that talks to corresponding Actor
on another member. This Actor can handle 3 kinds of messages:

1.  GossipTick - Sent by local scheduler that triggers the actor to send
registry `Status` to a randomly selected remote member.
2.  Status - This message contains member `Address`es and `version`s of
their corresponding `Bucket`s that `sender` has.
3.  Delta - This message contains the delta between sender member and
local member's registry. Local registry can be updated based on this
delta.

 +
 +
A member can update ONLY ITS OWN bucket as and when RPCs are
registered/unregistered on it.

[TODO] Add call flow

[[remote-rpc-broker]]
=== Remote RPC Broker

The main functions of this component are to:

1.  Route rpc requests to "right" members and collect responses
2.  Act as a listener for rpc requests coming in from remote members,
deliver it to MD-SAL Broker and respond back with result.

 +
 +
The component can be broken down to 2

[[open-questionsrandom-thoughts]]
== Open Questions/Random Thoughts

[[why-cant-we-use-an-existing-distributed-data-store-instead-of-rolling-our-own]]
=== Why can't we use an existing Distributed Data Store instead of
rolling our own?

1.  Most distributed DBs do not support transactions. Not even
transactions on a single shard. We do intend to support transactions on
a given shard.
2.  Not sure if the current existing DB's could even perform well - they
certainly cannot perform as well as our in-memory data store
3.  External DBs generally do not do data change notifications
4.  If we used an external DB that would make deployment a little more
complicated - we would have to setup ODL and also the external DB - some
people like the current deployment simplicity of ODL
5.  One of the principles that we want to follow is to discourage data
reads and promote data delivery (via change notifications) in this model
the advantage of fast reads that a high performing external DB like say
Mongo would become irrelevant

[[notes-regarding-sharding-design]]
=== Notes regarding Sharding design

The design of sharding should be done carefully based on the queries
applications make and noting down that *it will be painful(migration
involved)* if we want to change the sharding logic later after release.

[[clustering-scenarios]]
== Clustering Scenarios

The scenarios below determine what the clustering implementation will do
give a certain cluster state. The following assumptions have been made,

1.  All the local shard replicas of the first node to come up become
primary replicas
2.  A primary replica will stay the primary replica unless it is deemed
as down by the cluster
3.  A write is considered successful only if it is successfully written
to the journal of the primary and the journal of any one secondary
replica
4.  Cluster operations are suspended if zero secondary replicas can be
written to
5.  All replicas of a shard negotiate with each other as to which
replica should be the primary

[[scenario-1]]
=== Scenario 1

`    - Single node cluster` +
`    - Primary replica for all shards local` +
`    - No secondary replicas`

* Replication : off

[[scenario-2]]
=== Scenario 2

`    - Two node cluster` +
`    - Both nodes running`

* Replication : on

[[scenario-3-follows-scenario-2]]
=== Scenario 3 [follows Scenario 2]

`    - Two node cluster` +
`    - Node 1 running` +
`    - Node 2 running -> down`

* Node 1 : primary
* Cluster operations : suspended

[[scenario-4-follows-scenario-3]]
=== Scenario 4 [follows Scenario 3]

`    - Two node cluster` +
`    - Node 1 running` +
`    - Node 2 down -> running` +
`    `

* Node 1 : primary
* Node 1 replicates all data to Node 2
* Cluster operations : resumed

[[scenario-5-follows-scenario-2]]
=== Scenario 5 [follows Scenario 2]

`    - Two node cluster` +
`    - Node 1 running -> down` +
`    - Node 2 running`

* Node 2 : primary
* Cluster operations : suspended

[[scenario-6-follows-scenario-5]]
=== Scenario 6 [follows Scenario 5]

`    - Two node cluster` +
`    - Node 1 down -> running` +
`    - Node 2 running`

* Node 2 : primary
* Cluster operations : resumed

[[scenario-7]]
=== Scenario 7

`    - Two node cluster` +
`    - Node 2 comes up first` +
`    - Node 1 comes up second`

* Node 2 : primary

[[scenario-8]]
=== Scenario 8

`    - Three node cluster` +
`    - Node 1 comes up first ` +
`    - Node 2 comes up second` +
`    - Node 3 comes up third`

* Node 1 : primary

[[scenario-9-follows-scenario-8]]
=== Scenario 9 [follows Scenario 8]

`    - Three node cluster` +
`    - Node 1 running` +
`    - Node 2 running -> down` +
`    - Node 3 running`

* Node 1 : primary
* Node 1 : starts storing messages destined for Node 2
* Node 3 : saves replicated messages for Node 2
* Node 3 : fully replicated

[[scenario-10-follows-scenario-9]]
=== Scenario 10 [follows Scenario 9]

`    - Three node cluster` +
`    - Node 1 running` +
`    - Node 2 down -> running` +
`    - Node 3 running`

* Node 1 : primary
* Node 1 : replicates stored messages to Node 2
* Node 3 : discards the stored messages for Node 2
* Node 3 : fully replicated

[[scenario-11-follows-scenario-9]]
=== Scenario 11 [follows Scenario 9]

`    - Three node cluster` +
`    - Node 1 running -> down` +
`    - Node 2 down -> running` +
`    - Node 3 running`

* Node 3 : primary
* Node 3 : replicates stored messages to Node 2

[[scenario-12-follows-scenario-11]]
=== Scenario 12 [follows Scenario 11]

`   - Three Node cluster` +
`   - Node 1 down -> running` +
`   - Node 2 running` +
`   - Node 3 running` +
`   `

* Node 3 : primary
* Node 1 : discards any stored messages for Node 2

[[scenario-13-follows-scenario-8]]
=== Scenario 13 [follows Scenario 8]

`   - Three Node cluster` +
`   - Node 1 running -> down` +
`   - Node 2 running -> down` +
`   - Node 3 running` +
`   `

* Node 3 : primary
* Cluster operations : suspended

[[scenario-14-follows-scenario-8]]
=== Scenario 14 [follows Scenario 8]

`   - Three Node cluster` +
`   - Node 1 running -> down` +
`   - Send Persistent messages to a shard` +
`   - Node 2 running -> down (with unreplicated messages)` +
`   - Node 3 running -> down (with unreplicated messages)` +
`   - Node 1 down -> running`

* Node 1 : primary
* Cluster operations : suspended

[[scenario-15-follows-scenario-14]]
=== Scenario 15 [follows Scenario 14]

`   - Three Node Cluster` +
`   - Node 1 running` +
`   - Node 2 down -> running (is more uptodate)` +
`   - Node 3 down` +
`   `

* Node 2 : primary

[[proof-of-concept]]
== Proof of Concept

[[goals]]
=== Goals

* Figure out if Akka can be leveraged for clustering [Done]
* Validate design concepts [Done]
* Make design choices [Done]
* Estimate performance characteristics [Done]

[[focus-areas]]
=== Focus Areas

* Data Distribution / Sharding
** Determine location of Shard [Done]
** Akka Clustering [Done]
** Akka Remoting [Done]
** Akka Sharding
** Aggregation (Scatter Gather)
* Persistence / Recovery
** Akka Persistence [Done]
* Replication / High Availability [Done]
* Querying / Indexing
* Serviceability (Monitoring and Diagnosis)
** Akka atmos
* Data Change Notification (Query like Filters)
* Serialization over the wire
** Google Protocol Buffers
** EXI
** BSON
* Data Validators (nothing to do with DataCommitHandlers)
* Fault Tolerance
** Akka Supervision [Done]
* Remote Rpc
** Remote Rpc Registry and update using Gossip [Done]
* Expose an actor using OSGi [Done]

[[references]]
== References

https://wiki.opendaylight.org/view/OpenDaylight_Controller:MD-SAL:Architecture:DOM_DataStore[DOM
Data Store]

[[trello]]
== Trello

* https://trello.com/b/7oW0V2Yl/opendaylight-clustering

[[status-update]]
== Status Update

[cols=",",options="header",]
|=======================================================================
|Date |Description
|07/01/2014 a|
[[completed]]
=== Completed

* Move InMemoryDataStore into it's own bundle
* Implemented a Distributed DataStore which wraps the in-memory data
store using akka actors. Only a single Shard is used.

[[in-progress]]
=== In Progress

* Code reviews and merging of commits
* Further testing (mininet + cbench)
* Serialization of Normalized Node and conversion of all messages to
protocol buffers serialization (target Jul 8th)
* Modify the Distributed DataStore to use multiple shards instead of the
single shard (target Jul 8th)
* Implementation of Remote Rpc provider (target Jul 9th)
* Build/Borrow an implementation of RAFT for Replication (target Jul
25th)

[[help-needed]]
=== Help Needed

* Build a monitoring solution with Dashboard

[[trying-out-the-distributed-data-store]]
=== Trying out the Distributed Data Store

* Build an openflowplugin distribution
* Pull changes up to this gerrit
https://git.opendaylight.org/gerrit/#/c/8427/
* Go to opendaylight/md-sal/sal-distributed-datastore _mvn clean
install_
* _cp target/sal-distributed-datastore*.jar_ to the openflow plugin
distribution (should also work on most integration distributions)
* In the distribution folder edit _config/initial/01-md-sal.xml_ and
follow instructions to use distributed datastore instead of in-memory
datastore
* run the distribution using the command _./run.sh -Dakka.loglevel=debug
-Dshard.persistent=false -Xmx4G -Xms2G -XX:NewRatio=5 -XX:+UseG1GC
-XX:MaxPermSize=256m_

|=======================================================================

