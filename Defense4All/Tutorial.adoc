Back to Defense4All:User_Guide[Defense4All User Guide Page]

[[introduction]]
= Introduction

A well-known DoS attack mitigation and threats detection strategy is to
divert suspected traffic from its normal network path to dedicated
attack mitigation infrastructures, for cleaning and threat detection.
These infrastructures are also known as “Security Centers” or “Scrubbing
Centers”, mainly built of L3-L7 DoS attack mitigations devices. These
“Security Centers” can be deployed in dedicated remote sites within the
network in “out of path” manner (i.e. not “inline” with the native
traffic flow), so diversion of traffic toward these centers is
essential. During the traffic cleaning process, the attack mitigation
infrastructure identifies and drops malicious IP packets and forwards
legitimate IP packets back to their original targeted network
destinations. The “Scrubbing Centers” can be located within the
enterprise cooperate network, datacenter, cloud and also as part of
carrier’s infrastructure. In general protection against DDoS attacks in
Out-Off-Path (OOP) systems comprises three main elements:

1.  Collection of traffic statistics and learning of statistics behavior
of protected objects during peace-time. From this collected statistics,
the normal traffic baselines of the protected objects are built.
2.  Detection of DoS attack patterns as traffic anomalies deviating from
normal baselines.
3.  Diversion of suspicious traffic from its normal path to mitigation
(scrubbing) centers for traffic cleansing, selective source blockage,
etc. “Clean” traffic out of scrubbing centers is re-injected back to
packet’s original destination.

Defense4All is a security SDN application for detecting and driving
mitigation of DoS/DDoS attacks in different SDN topologies. It realizes
anti-DoS in Out Of Path mode for OpenDaylight SDN environment.
Administrators can configure Defense4All to protect certain
networks/servers (henceforth, protected networks or PNs).

Defense4All exploits SDN capabilities to count specified traffic, and
installs traffic counting flows for each protocol of each configured PN
in every network location through which traffic of the subject PN flows.
Defense4All then monitors traffic of all configured PNs, summarizing
readings, rates, and averages from all relevant network locations. In
case it detects a deviation from normal learned traffic behavior in some
protocol (TCP, UDP, ICMP, or rest of the traffic) of some PN,
Defense4All declares an attack against that protocol in the subject PN.

To mitigate a detected attack Defense4All performs several steps: 1) It
selects one or more mitigation devices to mitigate the attack. 2) It
configures SDN to divert attacked traffic through the mitigation
devices. 3) It continues monitoring attacked traffic both from SDN and
(optionally) from mitigation devices. When Defense4All receives no
indications about the attack it cancels attacked traffic diversions and
returns to peace-time monitoring.

Administrators need to notify Defense4All about relevant SDN controller,
switches/routers, and mitigation devices of choice. Defense4All users
can retrieve all present and past information about PN learned traffic
normal, attacks and mitigations, health and status of mitigation devices
and links, SDN configurations, and other operational information. Past
information is stored in persistent flight recorder storage.

In this version Defense4All runs as a single instance (non-clustered),
but it integrates three main fault tolerance features - 1) it runs as a
Linux service that is automatically restarted should it fail, 2) its
state is entirely persisted in stable storage, and upon restart
Defense4All obtains the latest state, and 3) it carries a health tracker
with restart and (several degrees of) reset capabilities to overcome
certain logical and aging bugs.

Defense4All is designed for pluggability, allowing integration of
different types of mitigation devices [Defense4All contains a reference
implementation of a driver to Radware’s mitigation device – DefensePro.
To attach some other mitigation device one needs to integrate driver to
that device into Defense4All.], different SDN and non-SDN based attack
detectors, different mitigation drivers, and different abstraction
levels of network controller functionality for collecting traffic
statistics and for traffic diversion. Finally, Defense4All itself
resides in a framework, and can be replaced by other SDN applications
[SDN and mitigation device drivers should be repackaged into the
framework rather than Defense4All].

The diagram below describes the possible state of any given PN. Radware
DefensePro, abbreviated as "DP" is an example of an incorporated AMS.
image:pn_possible_states.jpg[PN possible
states,title="fig:PN possible states"]

Back to Defense4All:User_Guide[Defense4All User Guide Page]

[[deployment-alternatives]]
= Deployment alternatives

Defense4All supports “short diversion”, in which the AMS (Attack
mitigation system) is connected to the edge router, so one hop traffic
redirection is achieved. See also PE1 and DefensePro 2 in Figure 3.
“Long diversion”, in which AMS scrubbing centers are located in
arbitrary remote locations in the network, may be added in future
Defense4All versions. See also PE2 and DefensePro 1 in Figure below.
Defense4All supports both automatic and manual diversion modes. The
manual mode includes user-based confirmation before diversion.
image:redirection_alternatives.jpg[Defense4All Deployment traffic
redirection
alternatives,title="fig:Defense4All Deployment traffic redirection alternatives"]

[[defense4all-in-odl-environment]]
= Defense4All in ODL Environment

Defense4All is an SDN application for detecting and mitigating DDoS
attacks. The application communicates with OpenDaylight Controller via
the ODC north-bound REST API. Through this API Defense4All performs two
main tasks:

1.  Monitoring behavior of protected traffic - the application sets flow
entries in selected network locations to read traffic statistics for
each of the PNs (aggregating statistics collected for a given PN from
multiple locations).
2.  Diverting attacked traffic to selected AMSs – the application set
flow entries in selected network locations to divert traffic to selected
AMSs. When an attack is over the application removes these flow entries,
thus returning to normal operation and traffic monitoring.

Defense4All can optionally communicate with the defined AMSs – e.g., to
dynamically configure them, monitor them or collect and act upon attack
statistics from the AMSs. The API to AMS is not standardized, and in any
case beyond the scope of the OpenDaylight work. Defense4All contains a
reference implementation pluggable driver to communicate with Radware’s
DefensePro AMS. The application presents its north-bound REST and CLI
APIs to allow its manager to:

1.  Control and configure the application (runtime parameters, ODC
connectivity, AMSs in domain, PNs, etc.).
2.  Obtain reporting data – operational or security, current or
historical, unified from Defense4All and other sources (like ODC, AMSs).

image:D4A_in_odl.jpg[Defense4All logical positioning in OpenDaylight
environment,title="fig:Defense4All logical positioning in OpenDaylight environment"]
Defense4All comprises an SDN applications Framework and the Defense4All
application itself – packaged as a single entity. Application
integration into the Framework is pluggable, so any other SDN
application can benefit from the common Framework services. The main
advantages of this architecture are:

1.  Faster application development and changes - the Framework contains
common code for multiple applications, complex elements (e.g.,
clustering or repository services) are implemented once for the benefit
of any application.
2.  Faster, flexible deployment in different environments, form-factors,
satisfying different NFRs – the Framework masks from SDN applications
factors such as required data survivability, scale and elasticity,
availability, security.
3.  Enhanced robustness - complex Framework code is implemented and
tested once, cleaner separation of concerns leads to more stable code,
Framework can increase robustness proactively with no additional code in
application logic (e.g., periodic application recycle).
4.  Unified management of common aspects – common look and feel.

[[framework-view]]
= Framework View

image:framework_view.jpg[Defense4All Structure from Framework point of
view,title="fig:Defense4All Structure from Framework point of view"] The
Framework contains the following elements:

*FrameworkMain* – the Framework root point contains references to all
Framework modules and global repositories, as well as the roots of
deployed SDN applications (in current version the Framework can
accommodate only one). This is also the point to start, stop or reset
the Framework (along with its hosted application) WebServer – Jetty web
server running the Jersey RESTful Web Services framework, with Jackson
parser for JSON encoded parameters. The REST Web Server runs a servlet
for Framework and another servlet for each deployed application
(currently only one). All REST and CLI APIs are supported through this
REST Web Server.

*FrameworkRestService* – A set of classes constituting the Framework
Servlet that responds to Framework REST requests (e.g., get latest
Flight Recorder records, perform factory reset, etc.). The
FrameworkRestService invokes control and configuration methods against
the FrameworkMgmtPoint, and for reporting it retrieves information
directly from the relevant repositories. For flight recordings it
invokes methods against the FlightRecorder.

*FrameworkMgmtPoint* – Is the point to drive control and configuration
commands (e.g., start, stop, reset, set address of the hosting machine,
etc.). FrameworkMgmtPoint invokes in turn methods against other relevant
modules in the right order. It forwards lifecycle requests (start, stop,
reset) directly to FrameworkMain to drive them in the right order.
*Defense4All Application* – Is the AppRoot object that should be
implemented/extended by any SDN application – in our case Defense4All.
SDN Applications do not have “main”, and their lifecycle (start, stop,
reset) is managed by the Framework operating against the Application
root object, which then drives all lifecycle operations in the
application. This module also contains reference back to the Framework,
allowing the application to use Framework services (e.g., create a Repo,
log a flight record) and common utilities. *Common classes and
utilities* – is a library of convenience classes and utilities, from
which any Framework or SDN Application module can benefit. Examples
include wrapped threading services (for asynchronous, periodic or
background execution), short hash of a string, confirmation by user,
etc.

*Repository services* – One of the key elements in the Framework
philosophy is decoupling compute state from compute logic. All durable
state should be stored in a set of repositories that can be then
replicated, cached, distributed under the covers –with no awareness of
the compute logic (Framework or Application). Repository services
comprise the RepoFactory and Repo or its annotations friendly equivalent
– the EntityManager. The RepoFactory is responsible to establish
connectivity with the underlying Repository plugged in service,
instantiate new requested repositories and return references to existing
ones. The chosen underlying Repository service is Hector Client over
Cassandra NoSQL DB. Repo presents an abstraction of a single DB table.
It allows reading the whole table, only table keys (tables are indexed
by only the single primary key), records or single cells, as well as
writing records or single cells with controlled eagerness. A sub-record
(with only a portion of cells) may be written. In such a case the
appearing cells override existing ones in the repository. Other cells in
the repository remain unchanged. In contrast to relational DB, in which
all columns must be specified up-front (in schema design), Repo
leverages the underlying Cassandra support to contain rows (records) in
the same table with different sets of columns, some of which may not
being even defined up-front. Furthermore, cells with new columns can be
added or removed on the fly. RepoFactory and Repo (as well as its Entity
Manager annotation friendly equivalent) constitute a convenience library
targeted to Framework and SDN Applications goals – on top of the Hector
client library communicating with Cassandra Repository cluster. Scaling
Cassandra cluster, distributing data shards across Cassandra cluster
members, configuring read/write eagerness and consistency – are for the
most part encapsulated in this layer.

*Logging and Flight Recorder services* – The logging service simply uses
Log4J library to log error, warning, trace or informational messages.
These logs are mainly for Defense4All developers. Administrators can
obtain additional details about failures from errors log. FlightRecorder
records all flight records recorded by any Defense4All module, including
information received from external network elements, such as ODC, AMSs,
etc. It then allows a user/administrator to obtain that information
through REST/CLI. Flight records can be filtered by categories (zero or
more can be specified) and by time ranges. FlightRecorder stores all
flight records in its own Repo (with another repo holding time ranges
for efficient time ranges retrieval from the records repo). Because all
flight records are stored in Cassandra, the number of flight records
Defense4All can keep is limited only by the size of the underlying
persistent storage capacity of all Cassandra servers, and so even on a
single Cassandra instance months of historical information can be kept.

*HealthTracker* – Is the point to hold the aggregated runtime health of
Defense4All, and to act in response to severe deteriorations. Any
module, upon sensing an unexpected/faulty behavior in it or in any other
module can record a “health issue” in the HealthTracker, providing
health issue significance. This is instead of directly triggering
Defense4All termination. The idea is that numerous health issues in a
short period of time with high aggregated significance are likely to
indicate a significant wide-spread Defense4All problem, but
sporadic/intermittent operational “hiccups” can be neglected – even if
Defense4All remains less than 100% operational (the administrator can
always reset restart it to fully recover). As such, every non-permanent
health issue has a gradually diminished affect over time. If Defense4Al
health deteriorates below a predefined threshold HealthTracker triggers
responsive actions – depending on the nature of health issues. A restart
can heal transient problems, and so the HealthTracker triggers
Defense4All termination (running as a Linux service Defense4All will be
automatically restarted). To recover from more permanent problems
HealthTracker may additional trigger a Defense4All reset. If it does not
help then the next time the HealthTracker will attempt a more severe
reset. As a last resort the administrator can be suggested to perform
factory reset.

*ClusterMgr* – Currently not implemented. This module is responsible for
managing a Defense4All cluster (separate from Cassandra or ODC clusters,
modeled as separate tier clusters). A clustered Defense4All carries
improved high availability and scalability. Any module in Defense4All
Framework or Application can register with ClusterMgr for clustered
operation, specifying whether its functionality should be carried out by
a single or by multiple/all active instances (running on different
Defense4All cluster members). When cluster membership changes,
ClusterMgr notifies each instance in each module about its role in the
clustered operation of that module. In case of a single active instance
that instance is told so, while all other instances are told they are
standby. In case of multiple active instances, each active instance is
notified about the number of active instances, and its logical
enumeration in that range. All state is stored in a globally accessible
and shared repository, so any instance of a module is stateless, and can
perform any role after every membership change. For example, following
membership change N an instance can be enumerated as 2 out of 7, and
thus perform relevant portion of the work. Then at membership change N+1
the same instance can be enumerated 5 out of 6, and perform the work
portion allocated for 5 and not for 2. We skip the peer messaging
services which the ClusterMgr can provide for a more coordinated
cross-instance operation.

The Defense4All Application is highly pluggable - it can accommodate
different attack detection mechanisms, different attack mitigation
drivers, and drivers (called reps – short for representative) to
different versions of ODC and different AMSs. Defense4All Application
comprises “core” modules and “pluggable” modules implementing
well-defined Defense4All Application APIs.

[[application-view]]
= Application View

image:d4a_application_view.jpg[Defense4All Defense4All Application
Structure,title="Defense4All Defense4All Application Structure"]

The Defense4All Application modules are described below:

*DFAppRoot* – This is the root module of the Defense4All Application. As
mentioned before, the Defense4All Application does not have “main”, and
its lifecycle (start, stop, reset) is managed by the Framework operating
against this module, which in turn drives all lifecycle operations in
the Defense4All Application. DFAppRoot also contains references to all
Defense4All Application modules (core and pluggable), global
repositories, and reference back to the Framework, allowing the
Defense4All Application modules to use Framework services (e.g., create
a Repo, log a flight record) and common utilities.

*DFRestService* – A set of classes constituting the Defense4All
Application Servlet that responds to Defense4All Application REST
requests. The DFRestService invokes control and configuration methods
against the DFMgmtPoint, and for reporting it retrieves information
directly from the relevant repositories. For flight recordings it
invokes methods against the FlightRecorder.

*DFMgmtPoint* – The point to drive control and configuration commands
(e.g., addams, addpn). DFMgmtPoint invokes in turn methods against other
relevant modules in the right order.

*ODL Reps* – Is a pluggable module-set for different versions of ODC.
Comprises two functions in two sub-modules – stats collection for and
traffic diversion of relevant traffic. These two sub-modules adhere to
StatsCollectionRep DvsnRep APIs. ODL Reps is detailed in Figure 6 and
description that follows it. SDNStatsCollector – Is responsible to set
“counters” for every PN at specified network locations (physical or
logical). A counter is a set of OpenFlow flow entries in ODC enabled
network switches/routers. The SDNStatsCollector periodically collects
statistics from those counters and feeds them to the
SDNBasedDetectionMgr (see next). The module uses the
SDNStatsCollectionRep to both set the counters and read latest
statistics from those counters. A stat report consists of read time,
counter specification, PN label, and a list of trafficData information,
where each trafficData element contains latest bytes and packets values
for flow entries configured for in the counter location. Protocol can be
\{tcp,udp,icmp,other ip}, port is any layer 4 port, and direction can be
\{inbound, outbound}.

*SDNBasedDetectionMgr* – Is a container for pluggable SDN based
detectors. It feeds stat reports, received from SDNStatsCollector, to
plugged-in SDN based detectors. It also feeds to all SDN based detectors
notifications from AttackDecisionPoint (see ahead) about ended attacks
(so as to allow reset of detection mechanisms).

*RateBasedDetector sub-module* – This detector learns for each PN its
normal traffic behavior over time, and notifies AttackDecisionPoint (see
next) when it detects traffic anomalies. For each protocol \{tcp, udp,
icmp, other ip} of each PN the RateBasedDetector maintains latest rates
and exponential moving averages (baselines) of bytes and packets, as
well as last reading time. The detector maintains those values both for
each counter as well as aggregation of all counters for each PN. The
organization at two levels of calculations (counter and PN aggregate)
allows for better scalability (e.g., working with clustered ODC, where
each instance is responsible to obtain statistics from a portion of
network switches, and bypassing the ODC single instance image API). Such
organization also enables a more precise stats collection (avoiding the
difficulty to collect all stats during a very small time interval).
Stats are processed at the counter level, and periodically aggregated at
the PN level. Continuous detections of traffic anomalies cause the
RateBasedDetector to notify AttackDecisionPoint about attack detection.
Then absence of anomalies for some period of time causes the detector to
stop notifying AttackDecisionPoint about attack detection. The detector
specifies a detection duration – time within which the detection is
valid. After that time the detection expires, but can be “prolonged”
with another notification about the same attack.

*AttackDecisionPoint* – This module is responsible to maintain attack
lifecycles. It can receive attack detections from multiple detectors.
Defense4All supports the RateBasedDetector, external detectors (in
future versions), and AMS based detector reference implementation (over
Radware’s DefensePro). In current version AttackDecisionPoint fully
honors each detection (max detector confidence, max detection
confidence). It declares a new attack for every detection of a new
attacked traffic (PN, protocol, port), and add more detections for
existing (already declared attacks). The module checks periodically the
statuses of all attacks. As long as there is at least one unexpired
detection (each detection has an expiration time) attack is kept
declared. If all detections are expired for a given attack
AttackDecisionPoint declared attack end. The module notifies the
MitigationMgr (see next) to start mitigating any new declared attack. It
notifies the MitigationMgr to stop mitigating ended attacks, and also
notifies the detectionMgr to reset stats calculations for traffic on
which an attack has just ended.

*MitigationMgr* - Is a container for pluggable mitigation drivers. The
MitigationMgr maintains the lifecycle of all mitigations, resulted from
mitigation notifications from AttackDecisionPoint. It holds a
pre-ordered list of the MitigationDriver sub-modules, and attempts to
satisfy each mitigation in that order. If MitigationDriveri indicates to
MitigationMgr that it does not mitigate a mitigation (because of per PN
preferences, unavailability of AMS resources, network problems, etc.)
MitigationMgr will attempt mitigation by MitigationDriveri+1. If none of
the plugged-in MitigationDrivers handle mitigation it remains in status
‘not-mitigated’.

*MitigationDriverLocal* – This mitigation driver is responsible to drive
attack mitigations using AMSs in its sphere of management. When
requested to mitigate an attack this mitigator performs the following
sequence of steps:

1.  It consults with the plugged in DvsnRep (see ahead) about
topologically feasible options of diversion to each of the managed AMSs
from each of the relevant network locations. In this version diversion
is always done from the location where the stats counters are installed.
2.  The MitigationDriverLocal then selects an AMS out of all feasible
options (in the first release the selection is trivial – first in list.
3.  It optionally configures all the AMSs (each diversion source may
have a different AMS associated with it) prior to instructing to divert
traffic to each. This is done through the plugged in AMSRep.
4.  The MitigationDriverLocal instructs the DvsnRep to divert traffic
from each source NetNode (in this version NetNode is modeled over an SDN
Switch) to the AMS associated with that NetNode. Diversion can be either
for inbound traffic only or both for inbound and outbound traffic.
5.  The mitigation driver notifies the AMSBasedDetector to optionally
start monitoring attack status in all the AMSs, and feed attack
detections to AttackDecisionPoint.
6.  In future versions the MitigationDriverLocal should monitor health
of all AMSs and relevant portions of network topologies, re-selecting
AMSs should some fail, or should network topologies changes require
that.

When mitigation should be ended the MitigationDriverLocal notifies
AMSBasedDetector to stop monitoring attack status for the ended attack,
notifies DvsnRep to stop traffic diversions to all AMSs – for this
mitigation, and finally notifies the AMSRep to optionally clean all
mitigation related configuration set in each relevant AMS.

*AMSBasedDetector* – This optional module (can be packaged as part of
the AMSRep) is responsible for monitoring/querying attack mitigation by
AMSs. Registering as a detector this module can then notify
AttackDecisionPoint about attack continuations and endings. It monitors
only specified AMSs and only for specified (attacked) traffic.

*AMSRep* - Is a pluggable module for different AMSs. The module adheres
to AMSRep APIs. It can support configuration of all introduced AMSs
(permanently or before/after attack mitigations). It can also
receive/query security information (attack statuses), as well as
operational information (health, load). AMSRep module is entirely
optional – AMSs can be configured and monitored externally. In many
cases attacks can continue be monitored solely via SDN counters.
Defense4All contains a reference implementation AMSRep that communicates
with Radware’s DefensePro AMSs.

[[odl-reps-view]]
= ODL Reps View

image:d4a_odl_reps_view.jpg[Defense4All Defense4All ODL Reps
Structure,title="Defense4All Defense4All ODL Reps Structure"]

The figure depicts the Defense4All Application ODL Reps module-set
structure. Different versions of OFC may be represented by different
versions of ODL Reps module-set. As mentioned before, ODLReps comprises
two functions – stats collection for and traffic diversion of relevant
traffic. Both or either of the functions may be utilized in a given
deployment. As such they have a common point to communicate with the ODC
and hold all general information – the ODC (see below).

ODL Reps supports two types of SDN Switches: sdn-hybrid, which support
both SDN and legacy routing, and sdn-native, which supports SDN only
routing. Counting traffic in sdn-hybrid switch can be simply
accomplished by programming a flow entry with desired traffic selection
criteria and the action is “send to normal”, i.e., continue with legacy
routing. Counting traffic in sdn-native switch requires an explicit
routing action (i.e., which output port to send the traffic to).
Defense4All avoids learning all routing tables by requiring an
sdn-native switch which is more or less a bump-in the wire with respect
to traffic routing, i.e., traffic entering port 1 normally exits port 2
and traffic entering port 3 normally exits port 4 and vice versa. Such a
switch allows for easy programming of flow entries just to count traffic
or to divert traffic to/from attached AMS. So when Defense4All programs
a traffic counting flow entry with selection criteria that includes port
1, its action will be output to port 2, and similarly with 3 to 4. In
future versions this restriction should be lifted.

The sub-modules are described below:

*StatsCollectionRep* - The module adheres to StatsCollectionRep APIs.
Its main tasks are:

* Offer counter placement NetNodes in the network. The NetNodes offered
are all NetNodes defined for a PN. This essentially maps to all SDN
Switches through which traffic of the given PN flows.
* Add a peacetime counter in selected NetNodes to collect statistics for
a given PN. StatsCollectionRep creates a single counter for a PN in each
NetNodes. (Overall, a NetNodes can have multiple counters for different
PNs; and a PN may have multiple counters - in NetNodes as specified for
the given PN). StatsCollectionRep translates installation of a counter
in NetNodes to programming 4 flow entries (for tcp, udp, icmp, and rest
of ip) for each “north traffic port” in that NetNodes – port from which
traffic from client to protected PN enters the SDN Switch. For example,
StatsCollectionRep will add for a given PN 12 flow entries in an SDN
Switch with 3 ports through that PN’s inbound traffic enters the OFS.
And, if another NetNode (SDN Switch) was specified to have that PN’s
inbound traffic entering it through 2 ports, then StatsCollectionRep
will program for this PN 8 flow entries in that second NetNode.
* Remove a peacetime counter.
* Read latest counter values for a specified counter. StatsCollectionRep
returns a vector of latest bytes and packets counted for each
protocol-port in each direction (currently only “north to south” is
supported), along with the time it received the reading from ODC.

*DvsnRep* - The module adheres to DvsnRep APIs. Its main tasks are:

* Return diversion properties from a given NetNode to a given AMS. In
this version an empty properties is returned if such diversion is
topologically feasible (AMS is directly attached to the SDN Switch over
which the specified NetNode is modeled. Otherwise no properties is
returned. (This leaves room for remote diversions in future versions,
and topological costs to each distant AMS – e.g., latency, bandwidth
reservation, cost, etc.).
* Divert (attacked) traffic from a specified NetNode through an AMS. As
such the new flow entries take precedence over the peacetime ones.
DvsnRep programs flow entries to divert inbound attacked traffic (or all
traffic if so specified for the PN) from every “north traffic port” into
AMS “north” port. If “symmetric diversion” (for both inbound and
returning, outbound traffic) has been specified for that PN, DvsnRep
programs another set of flow entries to divert attacked (or all) traffic
from every “south traffic port” into AMS “south” port. In an sdn-hybrid
switch deployment DvsnRep adds a flow entry for inbound traffic that
returns from AMS south port – with action send to normal, and similarly
it adds a flow entry for outbound returning traffic from AMS north port
– with action of also send to normal. In sdn-native switch the action is
to send to the right output port, but things are more complicated here
determining that right port. We use north port MAC learning to determine
from the source/destination MAC in the packet the right output port.
This scheme of flow entries works well for tcp, udp and icmp attacks.
For “other ip” attacks the flow entries programming is more complex, and
is suppressed here for clarity. The set of flow entries programmed to
divert (but still count) traffic comprises the “attack traffic floor”.
There may be many attack traffic floors, all of which take precedence
over the peacetime stats collection floor (by programming higher
priority flow entries). Additional attacks (except “other ip” attack
which is special case, and is suppressed here) are created with higher
priority traffic floors over previously set attack traffic floors.
Attacks may fully or partially “eclipse” earlier attacks (e.g., tcp port
80 over tcp or vice versa) or be disjoint (e.g., tcp and udp). Stats
collection is taken from all traffic floors – peacetime and attacks. SDN
based detector aggregates all statistics into overall rates, thus
determining if the attack is still on. (Notice that eclipsed peacetime
counted traffic may show zero rates, and that counting is complemented
by the higher priority floor counters.
* End diversion. DvsnRep removes the relevant attack traffic floor
(removing all its flow entries from the NetNode). Note that this affects
neither traffic floors “above” the removed floor nor the traffic floors
“below”. In addition, the SDN based detector receives the same
aggregated rates from counters of remaining floors, so its operation is
not affected either.

*ODLCommon* – This module contains all common elements needed to program
flow entries in ODC. This allows for coherent programming of configured
ODCs (in this version at most one) by StatsCollectionRep and DvsnRep.
For instance ODLCommon instantiates connectivity with the ODCs,
maintains a list of programmed flow entries and cookies assigned each.
It also maintains references to DFAppRoot and FrameworkMain. When an
sdn-native NetNode is added ODLCommon programs 2 flow entries per each
protected link (pair of input-to-output ports) to transfer traffic
between the two ports (traffic entering north port is routed to south
port and vice versa). ODLCommon adds 2 more flow entries for each port
connecting an AMS – to block returning ARP traffic (so as to avoid ARP
floods if AMSs are not configured to block them). This “common traffic
floor” flow entries are set with lowest priority. Their counters are
accounted for neither stats collections nor traffic diversion. When a
NetNode is removed ODLCommon removes this common traffic floor flow
entries.

*FlowEntryMgr* – This module provides an API to perform actions on flow
entries in an SDN switch managed by an ODC, and retrieve information
about all nodes managed by an ODC. Flow entries actions include adding a
specified flow entry in a specified NetNode (SDN Switch/Router),
removing a flow entry, toggling a flow entry, getting details of a flow
entry, and readings statistics gathered by the flow entry. FlowEntryMgr
uses the Connector modules to communicate with ODC.

*Connector* – This module provides the basic APIs to communicate with
ODC, wrapping REST communications. After initializing connection details
with a specified ODC the Connector allows getting or deleting something
from ODC, as well as posting or putting something to ODC.

*ODL REST Pojos* – This set of java classes are part of the ODC REST
API, specifying the Java classes of the parameters and the results of
interaction with ODC.

[[basic-control-flows]]
= Basic control flows

Control flows are logically ordered according to module runtime
dependencies, so if module A depends on module B then module B should be
initialized before module A, and terminate after it. Defense4All App
modules depend on most Framework modules, except RestServer.

*Startup* – The startup consists of first instantiating all modules,
then initializing them in order. Module instantiation includes setting
up all externally configured runtime parameters. Initialization starts
with all Framework modules, except the RestServer, which is activated
last (after the application is fully ready). The Framework modules are
initialized in the following order: FrameworkMain, RepoFactory,
FlightRecorder, FrameworkMgmtPoint. Then the Framework initializes the
Defense4All Application. Defense4All Application modules are initialized
in the following order: MitigationDriverLocal, global Defense4All
Application Repos, ODLStatsCollectionRep, ODLDvsnRep, AmsRep,
StatsCollector, DetectorMgr and RateBasedDetector, AttackDecisionPoint,
MitigationMgr, DFMgmtPoint. When FrameworkMgmtPoint initializes it
retrieves from global repos all user configurations in previous
lifecycles, and re-applies them in all the relevant modules - as if the
user has just configured them, however with one difference – any
dynamically added state to configured elements (e.g., PNs) is preserved.
Similarly, Defense4All Application MgmtPoint re-applies all user
configurations done in previous lifecycles. This design leaves all
modules completely stateless, allowing flexibility, consistency and
robustness in applying user configurations (whether to latest or any
previous state snapshot).

*Termination* – In this flow first the RestServer is stopped, then the
Defense4All Application modules, and finally the Framework modules. The
order of Defense4All Application modules tear-down is: DFMgmtPoint,
MitigationMgr and MitigationDriverLocal, AttackDecisionPoint,
DetectionMgr and RateBasedDetector, StatsCollector, AmsRep,
OdlStatsCollectoinRep, OdlDvsnRep. The order of Framework modules
tear-down is: FrameworkMgmtPoint, FlightRecorder, RepoFactory and all
Repos. Each module is expected to store all durable state in relevant
Repos. Then RepoFactory, flushes all Repos cached state to stable
storage and/or replicates to other Repo replicas, of there are any.

*Reset* – In this flow all modules are reset according to reset level
(currently soft, dynamic or factory). The order of modules’ reset is
identical to that of termination. Reset includes clearing cached
information as well as fully or selectively information from Repos –
according to the type of reset.

[[configurations-and-setup-flows]]
= Configurations and setup flows

*OFC (OpenFlowController = ODC)* – When DFMgmtPoint receives from
DFRestService a request to add OFC, it first records the added OFC in
OFCs Repo, then notifies ODLStatsCollectionRep and ODLDvsnRep, which in
turn notifies ODL to initiate connection to added OFC (ODC). ODL
instantiates a REST client for communication with ODC. NetNode -
Multiple NetNodes can be added. Each NetNode models a switch or similar
network device, along with its traffic ports, protected links and
connections to AMSs. When DFMgmtPoint receives from DFRestService a
request to add a NetNode, it first records the added NetNode in NetNodes
Repo, and then notifies ODLStatsCollectionRep and ODLDvsnRep, followed
by MitigationMgr. ODLStatsCollectionRep and ODLDvsnRep in turn notify
ODL, and the latter installs low priority flow entries to pass traffic
between protected links’ port pairs. MitigationMgr notifies
MitigationDriverLocal, which updates its NetNode-AMS connectivity groups
for consistent assignment of AMSs to diversion from given NetNodes.

*AMS* – Multiple AMSs can be added. When DFMgmtPoint receives from
DFRestService a request to add an AMS, it first records the added AMS in
AMSs Repo, then notifies AMSRep. AMSRep can optionally pre-configure
protection capabilities in the added AMS, and start monitoring its
health.

*PN* - Multiple PNs can be added. When DFMgmtPoint receives from
DFRestService a request to add a PN, it first records the added PN in
PNs Repo, then notifies MitigationMgr, and finally it notifies the
DetectionMgr. MitigationMgr notifies MitigationDriverLocal, which in
turn notifies AMSRep. AMSRep can preconfigure the AMS for this PN, as
well its EventMgr to accept events related to this PN’s traffic.
DetectionMgr notifies RateBasedDetector, which in turn notifies
StatsCollector. StatsCollector queries ODLStatsCollectionRep about
possible placements of stats collection counters for this PN.
ODLStatsCollectionRep returns all NetNodes configured for this PN (and
if none configured it returns all NetNodes currently known to
Defense4All). StatsCollector “chooses” (this only presently available)
counter locations option. For each of the NetNodes it then asks
ODLStatsCollectionRep to create a counter for the subject PN in each of
the selected NetNodes. The counter is essentially a set of flow entries
set for the protocols of interest (tcp, udp, icmp, and rest of ip) on
each north traffic port. The counter is given a priority and this
constitutes the peacetime traffic floor (to monitor traffic by
periodically reading all counter flow entry traffic count values).
Because PN may be re-introduced at restart or a change in network
topology may require re-calculation of counter locations, it is possible
that some/all counters may already be in place. Only new counters are
added. No-longer needed counters are removed. ODLStatsCollectionRep
configures the flow entries according to NetNode type: for hybrid
NetNodes the flow entry action is “send to normal” (i.e., proceed to
legacy routing), while for native NetNodes the action is matching output
port (in each protected link). OdlStatsCollectionRep invokes ODL to
create each specified flow entry. The latter invokes FlowEntryMgr and
Connector to send the request to ODC.

[[attack-detection-flow]]
= Attack Detection flow

Periodically StatsCollector requests ODL StatsCollectionRep to query the
ODC for the latest statistics for each set counter for each configured
PN. ODLStatsCollectionRep invokes FlowEntryMgr to obtain statistics in
each flow entry in a counter. The latter invokes the Connector to obtain
the desired statistics from the ODC.

ODLStatsCollectionRep aggregates the obtained results in a vector of
stats (latest bytes and packets readings per each protocol) and returns
that vector. StatsCollector feeds each counter stats vector to
DetectionMgr, who forwards the stats vector to RateBasedDetector. The
latter maintains stats information for every counter as well as
aggregated counter stats for every PN. Stats information includes time
of previous reading, and for every protocol - latest rates and
exponential averages.

The RateBasedDetector checks for significant and prolonged latest rates
deviations from the averages, and if such deviations are found in the PN
aggregated level it notifies AttackDecisionPoint about attack detection.
As long as deviations continue the RateBasedDetector will continue
notifying AttackDecisionPoint about the detections. It sets an
expiration time for every detection notification, and repeatable
notifications essentially prolong the detection expiration.

AttackDecisionPoint honors all detections. If it has already declared an
attack on that protocol-port, then the AttackDecisionPoint associated
the additional detection with that existing attack. Otherwise it creates
a new attack, and notifies MitigationMgr to mitigate that attack (next
section). Periodically, AttackDecisionPoint checks the status of all
detections of each live attack. If all detections are expired,
AttackDecisionPoint declares attack end and notifies MitigationMgr to
stop mitigating the attack.

[[attack-mitigation-flow]]
= Attack Mitigation flow

MitigationMgr, upon receiving mitigate notification from
AttackDecisionPoint, attempts to find a plugged-in MitigationDriver to
handle the mitigation. Currently, it requests its only plugged-in
MitigationDriverLocal.

MitigationDriverLocal checks if there are known, live and available AMSs
to which attacked (or all) traffic can be diverted from NetNodes through
which attacked traffic flows. It selects one of the suitable AMSs and
configures it prior to diverting attack traffic to the selected AMS. For
example MitigationDriverLocal retrieves from Repo the relevant protocol
averages and configures them in AMS through the AMSRep.

MitigationDriverLocal then requests ODLDvsnRep to divert attacked PN
protocol-port (or all PN) traffic from each of the NetNodes through PN
traffic flows – to the selected AMS.

ODLDvsnRep creates a new highest priority traffic-floor (that contains
flow entries with priority higher than any flow entry in previously set
traffic floors). The traffic floor contains all flow entries to divert
and count traffic from every ingress/northbound traffic port into the
AMS, and back from AMS to the relevant output (southbound) ports.
Optionally diversion can be “symmetric” (in both directions), in which
case flow entries are added to divert traffic from southbound ports into
the AMS, and back from AMS to northbound ports. Note that StatsCollector
treats this added traffic floor as any other, and passes obtained
statistics from this floor to DetectionMgr/RateBasedDetector. Because
traffic floors are aggregated (in the same NetNode as well as across
NetNodes) for a given PN the combined rates remain the same as prior to
diversion. Just like ODLStatsCollectionRep, ODLDvsnRep also utilizes
lower level modules to install the flow entries in desired NetNodes.

Finally, MitigationDriverLocal notifies AMSRep to optionally start
monitoring this attack and notify AttackDecisionPoint if the attack
continues or new attacks develop. AMSRep can do that through
AMSBasedDetector module.

If MitigationDriverLocal finds no suitable AMSs or fails to configure
any of its mitigation steps it aborts the mitigation attempt,
asynchronously notifying MitigationMgr about it. The mitigation then
remains in status “no-resources”.

When MitigationMgr receives a notification to stop mitigating an attack,
it forwards this notification to the relevant (and currently the only)
MitigationDriver – MitigationDriverLocal. The latter reverses the
actions in mitigation start: It notifies AMSRep to stop monitoring for
this attack, it cancels diversion for attacked traffic, and finally
notifies AMSRep to optionally remove pre-mitigation configurations.

[[problems-and-troubleshooting]]
= Problems and troubleshooting

Please refer to Defense4All log for information specific to the problem
encountered. Defense4All log is */var/log/defense4all/server.log*

'''Defense4All fails to start '''– If the RestServer fails to start
check to see if there is a conflict in port number. Defense4All uses
port 8086. If RepoFactory fails to initialize check if Cassandra service
is running (sudo service Cassandra stop/start/restart). If the
Defense4All Application fails to initialize the problem may lie with
system resources (threads, memory…). Try to restart the machine. Another
problem may be with corrupted Cassandra DF DB (keyspace). In such a case
try to perform restore or reset [see guidelines].

'''Defense4All fails to terminate '''– It is possible that its
RestServer crashed. The only way to stop Defense4All in such a case is
by killing its JVM Linux process from command line (kill -9
<Defense4All-JVM-process-number>).

'''Defense4All fails to reset '''- Some of the problems in starting
Defense4All may apply to reset as well. Specifically Cassandra service
should be up for reset. If reset fails manual Cassandra cleanup may be
required: from command line type “cassandra-cli” then “drop keyspace
DF;” then “quit;”. Also, flows created by Defense4All in ODC should be
removed. Unfortunately in this version Defense4All flows are not
identified as such by their name (rather their name is the cookie number
used to retrieve statistics from the ODC). To identify a flow entry set
by Defense4All one has to inspect the protocols set and priorities.
Protocols: 6,17,1,0. Priorities: 10,11,12,13, 31, 32, 33, 34, 50X, 70X,
90X… depending on active attacks.

'''Defense4All fails to add OFC '''- Check the failure reason (REST or
CLI). Other than incorrect parameters the problems may be that the added
ODC is not alive or has addressability (address+port) or security
(user+password) then specified in the API. Also Cassandra service should
be up. Note that in this release “remove OFC” is not supported. To
remove ODC reset needs to be done.

'''Defense4All fails to add NetNode '''– Check the failure reason (REST
or API). Other than incorrect parameters Cassandra service may be down.
Next, the problems may be that flow entries that Defense4All creates at
NetNode addition may conflict with priorities of other existing flow
entries in the Switch/Router modeled as NetNode. If so, Defense4All may
be conflicting with another SDN application. One recovery option is to
use a second switch modeled as NetNode in a bump in the wire
configuration before or after the originally planned switch/router, and
defined that second switch as NetNode for Defense4All to use for stats
collection and/or traffic redirection to AMSs.

'''Defense4All fails to remove NetNode '''– Check the failure reason
(REST or API). Other than incorrect parameters Cassandra service may be
down. Next, the problem may be that the DF_GLOBAL_NETNODES table (column
family) is corrupted, so it should be removed entirely (“Cassandra-cli”
then “use DF;” then “truncate column family DF_GLOBAL_NETNODES;”).
Another problem maybe that the ODC is not up, so Defense4All cannot
remove flow entries that it has set on that NetNode.

'''Defense4All fails to add AMS '''– Check the failure reason (REST or
API). Other than incorrect parameters Cassandra service may be down.
Next, the problems may be that the AMS is not alive or is not connected.

'''Defense4All fails to remove AMS '''– Check the failure reason (REST
or API). Other than incorrect parameters Cassandra service may be down.
Next, the problems may be that the AMS is not alive or is not connected.

'''Defense4All fails to add PN *– Check the failure reason (REST or
API). Other than incorrect parameters Cassandra service may be down.
Next, the problems may be that ODC or one of the relevant NetNodes or
AMSs may not be alive and connected. Next, the problems may be that flow
entries that Defense4All creates at PN addition may conflict with
priorities of other existing flow entries in the Switch/Router modeled
as NetNode through which traffic of this PN flows. If so, Defense4All
may be conflicting with another SDN application. One recovery option is
to use a second switch modeled as NetNode in a bump in the wire
configuration before or after the originally planned switch/router, and
defined that second switch as NetNode for Defense4All to use for stats
collection and/or traffic redirection to AMSs.* '''

'''Defense4All fails to remove PN '''– Check the failure reason (REST or
API). Other than incorrect parameters Cassandra service may be down.
Next, the problem may be that the DF_GLOBAL_PNS table (column family) is
corrupted, so it should be removed entirely (“Cassandra-cli” then “use
DF;” then “truncate column family DF_GLOBAL_PNS;”). Another problem
maybe that the ODC or one of the NetNodes is not up, so Defense4All
cannot remove flow entries that it has set. Finally an AMS may not be up
to remove configurations set at PN addition.

'''Defense4All fails to retrieve/dump/cleanup flight recorder log
records '''– Check the failure reason (REST or API). Other than
incorrect parameters Cassandra service may be down. Next, the problem
may be that the FWORK_FLIGHT_RECORDER_EVENTS or
FWORK_FLIGHT_RECORDER_SLICES tables (column families) are corrupted, so
both of them should be removed entirely (“Cassandra-cli” then “use DF;”
then “truncate column family FWORK_FLIGHT_RECORDER_EVENTS;” and
“truncate column family FWORK_FLIGHT_RECORDER_SLICES;”). Caution –
“truncating” these column families will lead to loss of all flight
records currently stored in Cassandra.

'''Defense4All fails to retrieve attack(s)/mitigation(s) '''– Check the
failure reason (REST or API). Other than incorrect parameters Cassandra
service may be down. Next, the problem may be that the
DF_GLOBAL_ATTACKS/ DF_GLOBAL_MITIGATIONS table (column family) is
corrupted, so should be removed entirely (“Cassandra-cli” then “use DF;”
then “truncate column family DF_GLOBAL_ATTACKS or
DF_GLOBAL_MITIGATIONS;”). Caution – “truncating” these column families
will lead to loss of all records of current attacks and mitigations. In
such a case traffic redirection flow entries may need to be manually
removed from all relevant netnodes (look for priorities 5X, 7X, 9X and
so on with action “redirect”).

'''Defense4All operational errors and failures '''- Depending on the
nature of the error/failure liveness of external entities (Cassandra,
ODC, NetNodes, AMSs) may need to be checked/restarted. If it is an
internal defense4All error the following recovery steps should be
attempted in this order: Defense4All restart, restart of Defense4All
hosting machine, Defense4All reset and possibly restore to an earlier
state (along with manual cleanup of flow entries and AMS configurations
set by Defense4All).

*Attack not detected* – Check Defense4All logs to see if there were any
errors recorded in stats collection and detection mechanisms. Check if
ODC and relevant NetNodes are alive. Check latest rates compared to
averages. Averages may be skewed, but resetting Defense4All during the
attack will not help, as Defense4All will obtain skewed averages. Wait
until the attack is over, then reset Defense4All, and re-add the
attacked PN with different attack thresholds.

*Mitigation is in status NO_RESOURCES* – Means that Defense4All
mitigationDriverLocal failed to drive this mitigation because of either
internal Defense4All error or lack of AMS resources. If there are indeed
no AMS resources – no recovery is needed. Otherwise check the liveness
of relevant AMS. Also check if Cassandra is running, if ODC is up, and
if relevant NetNodes are alive. In case of Defense4All internal error
(according to Defense4All logs) there may be corruption in the
DF_GLOBAL_ATTACKS/ DF_GLOBAL_MITIGATIONS table (column family), so both
should be removed entirely (“Cassandra-cli” then “use DF;” then
“truncate column family DF_GLOBAL_ATTACKS or DF_GLOBAL_MITIGATIONS;”).
Then, future detections will recreate the relevant attack, and
mitigation records. If this does not help, restart Defense4All.

'''Mitigation not terminated '''– Cleanup external elements from this
mitigation: manually remove relevant attack flow entries from all
relevant netnodes (look for priorities 5X, 7X, 9X and so on with action
“redirect”). Then restart Defense4All, and finally reset it.

[[continuity]]
= Continuity

Service Continuity is deemed by authors as a more complete term than
High Availability. It is defined here as the “_ability to deliver
required *level of service*, at tolerable *cost*, in the presence of
*disrupting events*_”. Where:

* *Disrupting events* can be load change, logical error,
failure/disaster, administrative actions (e.g., upgrade), external
attacks, etc.
* *Level of service* can include response time, throughput,
survivability of data/operations, security/privacy, etc. Required level
of service may differ for every service function, for every type of
event, at different event handling phases.
* '''Cost '''can include people (number, expertise), equipment
(hardware, software), facilities (space, power).

*Clustering and fault-tolerance* - Clusters help addressing both
Scalability and High Availability. If one of cluster members fails
another cluster members can quickly assume its responsibilities. This
overcomes member failures, member’s hosting machine failures, and member
network connectivity failures. Defense4All clustering is left to future
releases. In this release Defense4All runs as a Linux restartable
service. This allows overcoming intermittent/sporadic Defense4All
failures. Failure of Defense4All hosting machine means longer time and
modest additional human effort to revive the machine and its hosted
Defense4All. If the machine cannot be brought up Defense4All can be
started on another machine in the network. To ensure that Defense4All
will resume its operation (rather that restart from scratch) the user
needs to pre-load Defense4All (latest or earlier) state snapshot on that
machine. A none-clustered environment affects the time and the human
effort to recover from machine failures. The time factor is less
critical, as Defense4All runs out of path anyway, so its longer
unavailability for the most part means longer time to detect and
mitigate new attacks.

*State persistence* – Defense4All persists state in Cassandra DB running
in the same machine. In this release only one Cassandra instance cluster
is configured. As long as local stable storage does not crash, Linux
restart of the Defense4All service allows Defense4All to quickly pick up
its latest state from Cassandra and resume its latest operation. The
same will happen at failure and restart of the machine hosting
Defense4All. Taking Defense4All state backup, and restoring on another
machine allows resuming Defense4All operation on that machine.
Multi-node Cassandra clusters (in future versions) will increase state
persistence while reducing recovery time and effort.

'''Restart process '''– When Defense4All (re)starts it first checks for
saved configuration data, and simply re-plays the configuration steps
against all its relevant modules, driving any relevant external
programming/configuration actions (e.g., against ODC, AMS devices) – for
example re-adding a PN. The only difference between this configuration
replay and original configuration is that any dynamically obtained data
is preserved – for example all PN statistics. This allows to easily
reaching internal consistency, especially in cases where Defense4All or
its hosting machine crashed. When configuration action derivatives are
replayed against external entities, for example adding missing PN stats
counters, and removing no longer necessary ones – consistency with
external entities is also reached. Defense4All modules are tolerable to
already existing full or partial configuration of external entities
(e.g., counters in switches, traffic redirections, or AMS
configuration). Finally, Defense4All opens for business (launching its
web server), and allows the user or other component to complete
Defense4All missing configurations according to possible changes while
Defense4All was down. This allows reaching end-to-end consistency.

'''Reset '''– Defense4All allows the user to reset its dynamically
obtained data and configuration information (factory reset). This allows
overcoming many logical errors and misconfigurations. Note that mere
Defense4All restart or failover would not overcome such problems. This
mechanism, is therefore complementary to the restart-failover mechanism,
and should typically be applied as last resort.

*Failure isolation and Health Tracker* – In Defense4All failure
isolation takes place in the form of failure immediate recovery or
compensation (as much as possible), and failure recording in a special
module called Health Tracker. Except of handful of substantial failures
(like failure to start the Framework) no failure in any module
immediately causes Defense4All to stop. Instead, each module records
each failure in its scope providing severity specification, and
indication of failure permanence. If the combined severity (permanent or
temporary) of all failures exceeds a globally set threshold, the
HealthTracker triggers Defense4All shutdown (and revival by Linux). In
the future permanent or repeating temporary faults will cause
HealthTracker to trigger Defense4All soft and dynamic reset (of
dynamically obtained data) or suggest the administrator a factory reset
(that also includes configuration information).

*State backup and restore* – The administrator can snapshot Defense4All
state, save the backup in a different location, and restore in the
original or new Defense4All location. As stated above, this allows
overcoming certain logical bugs and mis-configurations, as well as
permanent failure of machine hosting Defense4All. The steps for
snapshotting Defense4All state:

1.  Quiesce Defense4All – shutdown (causing flush of all current state
to stable storage). Avoid performing any configurations changes when it
is brought back up. (thus avoiding new state changes).
2.  Take the Cassandra snapshot for Defense4All DB - “DF”: Refer to
http://www.datastax.com/docs/1.0/operations/backup_restore[http://www.datastax.com/docs/1.0/operations/backup_restore]
for Cassandra backup-restore guidelines.
3.  Copy the snapshot files to desired storage archive.

The steps for restoring a Defense4All backup in a target machine:

1.  Restore the desired saved snapshot in the target machine (same as
backup or different). Refer to
http://www.datastax.com/docs/1.0/operations/backup_restore[http://www.datastax.com/docs/1.0/operations/backup_restore]
for Cassandra backup-restore guidelines.
2.  Bring up Cassandra on that machine.
3.  Bring up Defense4All on that machine.

[[maintenance-and-upgrades]]
= Maintenance and upgrades

A key question in upgrade is whether the format of the data changes.
Future versions of Defense4All will have to tackle the changed data
format in one of two ways – 1) either automatically upgrade the state
format in repository as part of the upgrade process or 2) require
Defense4All to reset, then remove all existing repository tables, prior
to creating the ones in the new format. Another key question in upgrade
is compatibility with external entities – OFC, NetNodes, AMSs.
StatsCollectionRep, DvsnRep and AmsRep in the upgraded version must be
able to work with their external entities whether from scratch or from
previously set configuration and data obtained at runtime.

The upgrade Defense4All process involves:

* Backing up its state
* Optionally factory-resetting it
* Stopping it
* Upgrading any external entities
* Upgrading it
* (Re)Starting it

To downgrade Defense4All:

* Factory-reset it
* Stop it
* Downgrade any external entities
* Downgrade it
* Restore its backed up state prior to upgrade
* Start it

Because in this version Defense4All is not clustered, cluster rolling
upgrades do not apply here.

[[new-terms-and-concepts]]
= New terms and concepts

* *PN* – Protected Network, is a user defined protected network element
with a given protection specification. The network is specified by any
combination of two parts – 1) network address range (and optionally only
protocol-L4 port), and 2) network links through which the specified
traffic flows. Either of the two parts (but not both) may be
unspecified. In the first release only the second part is implemented.
Protection specification indicates a range of attributes related to
detection and mitigation of attacks against the subject PN.
* *NetNode* – models a switch or similar network device, along with its
traffic ports, protected links and connections to AMSs. NetNode
specifies interesting network location through which traffic of one or
more PNs normally flows (if not redirected), and/or to which AMSs are
connected. At peace-time Defense4All sets counters for PN in every
network through that PN’s traffic flows. At attack Defense4All selects
one or more AMSs connected to introduced NetNodes, and redirects
attacked/all traffic to the AMSs connected to those NetNodes.
* *Traffic port* – port through which inbound or outbound traffic enters
the NetNode
* *Protected link* – a set of entry-exit ports in a switch. Defense4All
avoids learning all routing tables by requiring an sdn-native switch set
as “bump-in-the-wire” in the network topology. Defense4All programs
non-attacked traffic entering one of the entry-exit pair-ports to exit
the other.
* *Traffic floor* – a set of flow entries that Defense4All programs on a
given NetNode. Different PNs have their own traffic floors – both for
peacetime attack detection, and for traffic redirection at attack
mitigation. Attack traffic floors contain flow entries with priorities
higher than all previously set attack mitigation traffic floor for that
PN, as well as the peacetime traffic floor (which contains flow entries
with sole purpose of counting PN traffic so as to learn behavior and
anomalies). Attacks may fully or partially “eclipse” earlier attacks
(e.g., tcp port 80 over tcp or vice versa) or be disjoint (e.g., tcp and
udp). Stats collection is taken from all traffic floors – peacetime and
attacks. SDN based detector aggregates all statistics into overall
rates, thus determining if the attack is still on. (Notice that eclipsed
peacetime counted traffic may show zero rates, and that counting is
complemented by the higher priority floor counters.
* *AMS* – Attack mitigation system that detects, mitigates and reports
network cyber-attacks. For example, Radware’s DefensePro is such an AMS
that is capable to detect, mitigate and report a broad range of
cyber-attacks.
* *Detection* – Detector indication of monitored traffic anomaly. The
detection has an expiration time, and can be “renewed”.
* *Attack* – Suspected or detected DDoS or other network cyber-attack on
a PN. The attack may be on any combination of a network link,
destination address, protocol, and L4 port. Defense4All maintains an
attack lifecycle in which it attempts to mitigate the attack according
to specifications per the subject PN.
* *Mitigation* – The activity/activities that is/are going on to
mitigate a given attack. The activities are determined by specification
per the subject PN, as well as available mitigation and network
resources. For example, a user may specify for PN1 that attacked/all
traffic should be redirected to a specific AMS. At the same time the
user can specify for PN2 that only reporting of attack status is needed.
* *OFC/ODC* – SDN Controller that supports OpenFlow network programming
(hence OFC = OpenFlow Controller). OpendDaylight Controller provides
this flavor both for OpenFlow enabled network devices as well as other
network devices with adequate plugins in the ODC.
* *sdn-hybrid and sdn-native* – ODL Reps supports two types of SDN
Switches: sdn-hybrid, which support both SDN and legacy routing, and
sdn-native, which supports SDN only routing. Counting traffic in
sdn-hybrid switch can be simply accomplished by programming a flow entry
with desired traffic selection criteria and the action is “send to
normal”, i.e., continue with legacy routing. Counting traffic in
sdn-native switch requires an explicit routing action (i.e., which
output port to send the traffic to). Defense4All avoids learning all
routing tables by requiring an sdn-native switch which is more or less a
bump-in the wire with respect to traffic routing, i.e., traffic entering
port 1 normally exits port 2 and traffic entering port 3 normally exits
port 4 and vice versa. Such a switch allows for easy programming of flow
entries just to count traffic or to divert traffic to/from attached AMS.
So when Defense4All programs a traffic counting flow entry with
selection criteria that includes port 1, its action will be output to
port 2, and similarly with 3 to 4. In future versions this restriction
should be lifted. See also the_' Protected link_' term.

[[other-information]]
= Other Information

[[security-and-privacy]]
=== Security and privacy

Defense4All REST API presently does not check for credentials. Nor does
it define user roles according to which usage of certain REST APIs is
allowed/restricted.

[[compatibility]]
=== Compatibility

This Defense4All version (1.0.7) is compatible with ODC 1.0. The
reference implementation of AmsRep is over Radware’s DefensePro versions
– hardware version VL, software versions 6.03, 6.07, 6.09.

[[performance-and-scalability-information]]
=== Performance and scalability information

TBD.

[[reference-materials]]
=== Reference materials

[ODC]

Back to Defense4All:User_Guide[Defense4All User Guide Page]
