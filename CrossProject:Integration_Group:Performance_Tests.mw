== Introduction ==
Performance tests are needed not only for benchmarking our controller but also for improving our code by continuosly monitoring the collected data. We also stated that for next release '''Helium''' our focus will be performance and stability and as a result we have decided to create and maintain this wiki.

== Scope ==
The goal of this wiki is to:
* Provide a set of recommendations for doing performance test
* Describe which performance tests are currently running in OpenDaylight
* Publish performance test results

== Performance metrics ==
We agreed we are going to start with following performance metrics:

* Inventory performance:
** Max number of OF switches and ports
** Time/CPU/RAM consumed in learning switches, ports and topology 

* Flow performance:
** Max number of flows in memory
** Max flows/sec we can process
** Time/CPU/RAM consumed in pushing flows

== Controller Baseline ==
In order to run an homogeneous test, we agreed on using this configuration for all performance test:

* HW/SW configuration: 
** Use dedicated VM or bare-metal for controller
** CPU: 4 Cores
** RAM: 6 GB
** OS: Linux Server (no Desktop) is recommended. Report used OS when publishing results

* Controller configuration:
** Use controller Base edition. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
** ODL Log level set to ERROR, edit  opendaylight/configuration/logback.xml
** Optional: To enable new datastore edit file opendaylight/config/initial/01-md-sal.xml and follow instructions in the file comments (comment everything within the DATA-BROKER comments and uncomment everything within the NEW-DATA-BROKER comments)
** Enable OF plugin with -of13 option: '''run.sh -of13'''
** Set Java memory: Min=1G, Max=4G, in run.sh we have to add following JVM options:
<pre>
run.sh -Xms1g -Xmx4g    # Minimum heap size 1G and maximum heap size 4G. Increase to higher values if we encounter 
                        # OutOfMemoryError crashes when MACs and Switches count are increased in CBench cmdline args
</pre>

== Performance tools ==

* '''Mininet'''
Available at [http://mininet.org mininet.org] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]].
Mininet simulates a topology of openflow switches connected each other and also hosts attached to them. This tool can be used for inventory performance by increasing the number of switches and ports in the topology. It also allows custom configurations so that we can create topologies close to real scenarios (i.e. data centers)

* '''Cbench'''
Available at [http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New) Openflowhub] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]]. Cbench emulates a bunch of openflow switches which connect to a controller, send packet-in messages, and watch for flow-mods to get pushed down. This tool can be used for flow performance

* '''Enhanced Cbench'''
We can use Cbench with Performance test scripts created by Michal Rehak to generate a 2D/3D plot with Gnuplot. Check instructions in [[CrossProject:Integration_Group:Performance_Test_Enhanced_Cbench|Enhanced CBench Test]]

* '''Jconsole'''
With Jconsole it is possible to monitor CPU and RAM for a Java process. Instructions on how to start the Jconsole are available [[Media:Jconsole.pdf|here]].

* '''Yourkit'''
This tool can be used to monitor CPU and RAM per Java bundle, problem is that it is not free.

* '''Jenkins Monitoring plugin'''
Jenkins can also monitor CPU and RAM on slave nodes (like controller VM) through [https://wiki.jenkins-ci.org/display/JENKINS/Monitoring Monitoring plugin]

=== CBench Setup ===

The [https://github.com/andi-bigswitch/oflops OFLOPS/CBench READMEs] are quite short on details. [http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New) This page] is a more useful resource but lacks explanation and is Debian-centric (I think most ODL devs are using Fedora).

According to the OFLOPS README, ''net-snmp-devel'' and ''libpcap-devel'' are required dependencies. Starting from a clean Fedora 20 install, you'll also need ''autoconf'', ''automake'', ''libtool'' and ''libconfig-devel'' to get CBench built:

<pre>
[~]$ sudo yum install net-snmp-devel libpcap-deve autoconf automake libtool libconfig-devel
</pre>

CBench lives in the OFLOPS repo, so you'll need to clone that repo to your local system:

<pre>
[~]$ git clone https://github.com/andi-bigswitch/oflops.git
</pre>

You'll also need the OpenFlow source code, as CBench must be pointed at it during the build:

<pre>
[~]$ git clone git://gitosis.stanford.edu/openflow.git
</pre>

Use the ''boot.sh'' script to build a ''configure'' file:

<pre>
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File doesn't exist
[~/oflops]$ ./boot.sh
<snip>
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File exists
</pre>

You can now build OFLOPS:

<pre>
[~/oflops]$ ./configure --with-openflow-src-dir=/absolute/path/to/openflow/source
[~/oflops]$ make
[~/oflops]$ sudo make install
[~/oflops]$ where cbench
/usr/local/bin/cbench
</pre>

== Test Cases ==

=== Mininet Test ===
Still in design, the idea will be to bring up small to large topologies of 16-32-64-128-256-... switches and verify the controller can properly learn topology: switches, ports and links. Also measure CPU and RAM consumed on each iteration. We can also configure mininet to bring a custom topology close to a data center.

Test Steps:
* 1. Download latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
* 2. Set controller Log level to ERROR, edit opendaylight/configuration/logback.xml 
* 3. Start controller with recommended options: '''run.sh -of13 -Xms1g -Xmx4g'''
* 4. Start mininet with tree topology of 15 switches: '''sudo mn --controller=remote,ip=<controllerIP> --topo tree,4''' or custom topology
* 5. Check RESTCONF inventory with: 
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/restconf/operational/opendaylight-inventory:nodes(|grep "openflow:")
</pre>
* 6. Check AD-SAL topology through GUI or NB API:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/topology/default (|grep "openflow:")</pre>
Next steps 7&8 are designed to test AD-SAL ARP Handler and Simple Forwarding apps
* 7. Do a ping test in mininet: '''mininet> pingall'''
* 8. Check hosts are learned and flows are created:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/hosttracker/default/hosts/active
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/statistics/default/flow
</pre>
* 9. Monitor CPU and RAM
* 10. Repeat the test increasing the number of switches, like -tree,5 (31 switches)

=== CBench Test ===
Still in design, the idea will be to bring up small to large topologies of 16-32-64-128-256-... switches and benchmark how many flows/sec the controller can handle. Also measure CPU and RAM consumed on each iteration. This test needs the reactive forwarding bundle loaded into OSGi framework.
 
Test Steps:
* 1. Download latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
* 2. Download the OF plugin reactive forwarding bundle from same link and follow link instructions to install it in opendayligh/plugins
* 3. Delete two AD-SAL bundles simple forwarding and arp handler that interfere with MD-SAL Cbench measurements:
<pre>
rm plugins/org.opendaylight.controller.samples.simpleforwarding-0.4.2-SNAPSHOT.jar
rm plugins/org.opendaylight.controller.arphandler-0.5.2-SNAPSHOT.jar
</pre>
* 4. Set controller Log level to ERROR, edit opendaylight/configuration/logback.xml 
* 5. Start controller with recommended options: '''run.sh -of13 -Xms1g -Xmx4g'''
* 6. Initiate the drop test in the controller. There are two types of drop tests: RPC (programming flows directly through an RPC to the OF Plugin) and data store (programming flows by writing them into the MD-SAL config space, from where they are picked up by the FRM and programmed into the plugin). For the data store performance, the latter is of interest.
** To turn on the RPC drop test, type from the controller’s OSGI console: > '''dropAllPacketsRpc on'''
** To turn on the data store drop test, type from the controller’s OSGI console: > '''dropAllPackets on'''
* 7. Start Cbench test with 16 switches: '''cbench -c <controllerIP> -p 6633 -m 1000 -l 10 -s 16 -M 100000'''. The first run will probably fail. You need to repeat this a few times. See below for what you should see:
<pre>
mininet@mininet-vm:~$ cbench -c 192.168.162.1 -p 6633 -m 1000 -l 10 -s 16 -M 100000 
cbench: controller benchmarking tool
   running in mode 'latency'
   connecting to controller at 192.168.162.1:6633 
   faking 16 switches offset 1 :: 10 tests each; 1000 ms per test
   with 100000 unique source MACs per switch
   learning destination mac addresses before the test
   starting test with 0 ms delay after features_reply
   ignoring first 1 "warmup" and last 0 "cooldown" loops
   connection delay of 0ms per 1 switch(es)
   debugging info is off
11:29:29.813 16  switches: flows/sec:  718  717  717  717  718  718  718  717  714  692  631  568  368  362  361  361   total = 9.789443 per ms 
11:29:30.915 16  switches: flows/sec:  746  746  746  746  746  746  747  745  736  708  654  582  390  380  375  375   total = 10.155651 per ms 
11:29:32.015 16  switches: flows/sec:  774  774  774  774  774  774  774  773  766  736  665  589  398  391  389  387   total = 10.509467 per ms 
11:29:33.117 16  switches: flows/sec:  726  726  726  726  726  726  726  726  722  692  635  556  379  373  366  364   total = 9.894337 per ms 
11:29:34.219 16  switches: flows/sec:  772  772  772  772  772  773  773  773  765  734  666  608  398  391  387  387   total = 10.502649 per ms 
11:29:35.321 16  switches: flows/sec:  749  749  749  747  747  749  749  749  745  717  652  578  389  385  380  378   total = 10.202389 per ms 
11:29:36.422 16  switches: flows/sec:  660  659  659  659  657  656  655  650  646  614  547  488  347  341  335  333   total = 8.903142 per ms 
11:29:37.524 16  switches: flows/sec:  636  635  636  635  635  634  635  634  634  604  546  477  328  323  321  318   total = 8.621379 per ms 
11:29:38.626 16  switches: flows/sec:  665  665  665  665  665  665  665  665  659  629  575  491  344  339  335  333   total = 9.019201 per ms 
11:29:39.727 16  switches: flows/sec:  637  637  637  637  636  635  636  632  629  596  546  478  331  327  319  320   total = 8.624522 per ms 
RESULT: 16 switches 9 tests min/max/avg/stdev = 8621.38/10509.47/9603.64/755.15 responses/s
 </pre>
* 8. Monitor CPU and RAM
* 9. Repeat the test increasing the number of switches (-s 32), and number of Mac/switch (-M 200000)

=== Known Issues ===
* Cbench Throughput test does not work well, CPU goes very high and cannot process incoming packet-in messages in an stable way. It looks like CPU is being used by Netty to flush some buffers, OF plugin devs are working on fixing this.
* The data store drop test '''dropAllPackets on''' does not work well either, flows get hung after a while. FRM seems to be the bottleneck, OF plugin devs are looking at this was well.

[[Category:Integration Group]]
