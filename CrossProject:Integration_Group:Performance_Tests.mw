== Introduction ==
Performance tests are needed not only for benchmarking our controller but also for improving our code by continuosly monitoring the collected data. We also stated that for next release '''Helium''' our focus will be performance and stability and as a result we have decided to create and maintain this wiki.

== Scope ==
The goal of this wiki is to:
* Provide a set of recommendations for doing performance test
* Describe which performance tests are currently running in OpenDaylight
* Publish performance test results

== Performance metrics ==
We agreed we are going to start with following performance metrics:

* Inventory performance:
** Max number of OF switches and ports
** Time/CPU/RAM consumed in learning switches, ports and topology 

* Flow performance:
** Max number of flows in memory
** Max flows/sec we can process
** Time/CPU/RAM consumed in pushing flows

== Controller Baseline ==
In order to run an homogeneous test, we agreed on using this configuration for all performance test:

* HW/SW configuration: 
** Use dedicated VM or bare-metal for controller
** CPU: 4 Cores
** RAM: 6 GB
** OS: Linux Server (no Desktop) is recommended. Report used OS when publishing results

* Controller configuration:
** Use controller Base edition. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
** ODL Log level set to ERROR, edit  opendaylight/configuration/logback.xml
** Optional: To enable new datastore edit file opendaylight/config/initial/01-md-sal.xml and follow instructions in the file comments (comment everything within the DATA-BROKER comments and uncomment everything within the NEW-DATA-BROKER comments)
** Enable OF plugin with -of13 option: '''run.sh -of13'''
** Set Java memory: Min=4G, Max=4G, in run.sh we have to add following JVM options:
<pre>
run.sh -Xmx4G -XX:+UseG1GC -XX:MaxPermSize=512m -Xms4G    

# Minimum heap size 4G and maximum heap size 4G. When min and max are the same, there will be no dynamic heap allocation. 
  When increasing to higher values, increase both at same time for better performance 
# Use G1GC garbage collection strategy. 
  Getting Started with the G1GC Garbage Collector: 
    http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html 
  Garbage First Garbage Collector (G1 GC) - Migration to, Expectations and Advanced Tuning: 
    http://www.slideshare.net/MonicaBeckwith/garbage-first-garbage-collector-g1-gc-migration-to-expectations-and-advanced-tuning



# OutOfMemoryError crashes when MACs and Switches count are increased in CBench cmdline args

</pre>

== Performance tools ==

* '''Mininet'''
Available at [http://mininet.org mininet.org] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]].
Mininet simulates a topology of openflow switches connected each other and also hosts attached to them. This tool can be used for inventory performance by increasing the number of switches and ports in the topology. It also allows custom configurations so that we can create topologies close to real scenarios (i.e. data centers)

* '''Cbench'''
Available at [http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New) Openflowhub] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]]. Cbench emulates a bunch of openflow switches which connect to a controller, send packet-in messages, and watch for flow-mods to get pushed down. This tool can be used for flow performance

* '''Enhanced Cbench'''
We can use Cbench with Performance test scripts created by Michal Rehak to generate a 2D/3D plot with Gnuplot. Check instructions in [[CrossProject:Integration_Group:Performance_Test_Enhanced_Cbench|Enhanced CBench Test]]

* '''Jconsole'''
With Jconsole it is possible to monitor CPU and RAM for a Java process. Instructions on how to start the Jconsole are available [[Media:Jconsole.pdf|here]].

* '''Yourkit'''
This tool can be used to monitor CPU and RAM per Java bundle, problem is that it is not free.

* '''Jenkins Monitoring plugin'''
Jenkins can also monitor CPU and RAM on slave nodes (like controller VM) through [https://wiki.jenkins-ci.org/display/JENKINS/Monitoring Monitoring plugin]

=== CBench Setup ===

The [https://github.com/andi-bigswitch/oflops OFLOPS/CBench READMEs] are quite short on details. [http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New) This page] is a fairly useful resource, but the steps below are more succinct and give additional explanation.

According to the OFLOPS README, ''net-snmp-devel'' and ''libpcap-devel'' are required dependencies. Starting from a clean Fedora 20 install, you'll also need ''autoconf'', ''automake'', ''libtool'' and ''libconfig-devel'' to get CBench built. Git is required for the install process:

<pre>
[~]$ sudo yum install net-snmp-devel libpcap-devel autoconf make automake libtool libconfig-devel git
</pre>

CBench lives in the OFLOPS repo, so you'll need to clone that repo to your local system. The ''--recursive'' flag pulls down the project's submodule in the same step (submodule seems to be optional):

<pre>
[~]$ git clone --recursive https://github.com/andi-bigswitch/oflops.git
</pre>

Go ahead and build the submodule (seems to be optional):

<pre>
[~/oflops/netfpga-packet-generator-c-library]$ ./autogen.sh
[~/oflops/netfpga-packet-generator-c-library]$ ./configure 
[~/oflops/netfpga-packet-generator-c-library]$ make
</pre>

You'll need the OpenFlow source code, as CBench must be pointed at it during the build (this is 1.0, 1.3 currently breaks the CBench build):

<pre>
[~]$ git clone git://gitosis.stanford.edu/openflow.git
</pre>

Use the ''oflops/boot.sh'' script to build an ''oflops/configure'' file:

<pre>
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File doesn't exist
[~/oflops]$ ./boot.sh
<snip>
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File exists
</pre>

You can now build OFLOPS:

<pre>
[~/oflops]$ ./configure --with-openflow-src-dir=/absolute/path/to/openflow/source
[~/oflops]$ make
[~/oflops]$ sudo make install
[~/oflops]$ whereis cbench
/usr/local/bin/cbench
</pre>

== Test Cases ==

=== Mininet Test ===
Still in design, the idea will be to bring up small to large topologies of 16-32-64-128-256-... switches and verify the controller can properly learn topology: switches, ports and links. Also measure CPU and RAM consumed on each iteration. We can also configure mininet to bring a custom topology close to a data center.

Test Steps:
* 1. Download latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
* 2. Set controller Log level to ERROR, edit opendaylight/configuration/logback.xml 
* 3. Start controller with recommended options: '''run.sh -of13 -Xms1g -Xmx4g'''
* 4. Start mininet with tree topology of 15 switches: '''sudo mn --controller=remote,ip=<controllerIP> --topo tree,4''' or custom topology
* 5. Check RESTCONF inventory with: 
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/restconf/operational/opendaylight-inventory:nodes(|grep "openflow:")
</pre>
* 6. Check AD-SAL topology through GUI or NB API:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/topology/default (|grep "openflow:")</pre>
Next steps 7&8 are designed to test AD-SAL ARP Handler and Simple Forwarding apps
* 7. Do a ping test in mininet: '''mininet> pingall'''
* 8. Check hosts are learned and flows are created:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/hosttracker/default/hosts/active
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/statistics/default/flow
</pre>
* 9. Monitor CPU and RAM
* 10. Repeat the test increasing the number of switches, like -tree,5 (31 switches)

=== CBench Test ===
This test is still being designed, but the idea is to bring up varying typologies of switches (16, 32, 64...) and benchmark how many flows/sec the controller can handle, as well as the CPU and RAM usage of each iteration.

Test Steps:

* 1. OpenDaylight depends on Java (and which, to set $JAVA_HOME). The install process depends on unzip and wget.

<pre>
[~]$ sudo yum install java-1.7.0-openjdk which unzip wget
</pre>

* 2. Download the latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts]] for details.

<pre>
[~]$ wget 'https://jenkins.opendaylight.org/integration/job/integration-master-project-centralized-integration/lastSuccessfulBuild/artifact/distributions/base/target/distributions-base-0.1.2-SNAPSHOT-osgipackage.zip'
[~]$ unzip distributions-base-0.1.2-SNAPSHOT-osgipackage.zip
</pre>

* 3. Download the OpenFlow plugin reactive forwarding bundle and install it by moving it to the ''opendaylight/plugins'' directory.

<pre>
[~/opendaylight/plugins]$ wget 'https://jenkins.opendaylight.org/openflowplugin/job/openflowplugin-merge/lastSuccessfulBuild/org.opendaylight.openflowplugin$drop-test/artifact/org.opendaylight.openflowplugin/drop-test/0.0.3-SNAPSHOT/drop-test-0.0.3-SNAPSHOT.jar'
</pre>

* 4. The AD-SAL simple forwarding and ARP handler bundles apparently interfere with MD-SAL CBench measurements. Delete them:
<pre>
[~/opendaylight/plugins]$ rm org.opendaylight.controller.samples.simpleforwarding-0.4.2-SNAPSHOT.jar
[~/opendaylight/plugins]$ rm org.opendaylight.controller.arphandler-0.5.2-SNAPSHOT.jar
</pre>

* 5. Edit the ''opendaylight/configuration/logback.xml'' file to set the controller log level to ERROR:

<pre>
  <!-- Controller log level -->
  <logger name="org.opendaylight.controller" level="ERROR"/>
</pre>

* 6. Start the controller. It's recommended that you use the following options:

<pre>
[~/opendaylight]$ ./run.sh -of13 -Xms1g -Xmx4g
</pre>

* 7. Initiate the drop test in the controller. There are two types of drop tests: RPC (programming flows directly through an RPC to the OF Plugin) and data store (programming flows by writing them into the MD-SAL config space, from where they are picked up by the FRM and programmed into the plugin). For the data store performance, the latter is of interest.

Turn on the RPC drop test. From the controller’s OSGi console, which your shell drops into after starting the controller:

<pre>
osgi> dropAllPacketsRpc on
DropAllFlows transitions to on
</pre>

Turn on the data store drop test. From the controller’s OSGi console, which your shell drops into after starting the controller:

<pre>
osgi> dropAllPackets on
DropAllFlows is already on
</pre>

* 8. If you haven't already, install CBench as described in [[CrossProject:Integration_Group:Performance_Test#CBench_Setup]]. 

* 9. You can now start a CBench test (from a different shell, not the OSGi console). Replace ''localhost'' with the IP of the machine running the controller, if it's not your local machine. Note that the first run seems to give non-representative results, but future runs are consistent.

<pre>
[~]$ cbench -c localhost -p 6633 -m 1000 -l 10 -s 16 -M 100000
cbench: controller benchmarking tool
   running in mode 'latency'
   connecting to controller at localhost:6633 
   faking 16 switches :: 10 tests each; 1000 ms per test
   with 100000 unique source MACs per switch
   learning destination mac addresses before the test
   starting test with 0 ms delay after features_reply
   ignoring first 1 "warmup" and last 0 "cooldown" loops
   connection delay of 0ms per 1 switch(es)
   debugging info is off
17:45:11.159 16  switches: fmods/sec:  259  367  138  226  220  294  475  150  126  250  132  113  224  135  158  126   total = 3.392956 per ms 
17:45:12.259 16  switches: fmods/sec:  393  421  349  420  446  360  427  425  410  413  342  445  472  347  482  405   total = 6.556993 per ms 
17:45:13.360 16  switches: fmods/sec:  647  549  666  538  566  701  612  599  555  600  648  464  569  656  549  619   total = 9.537990 per ms 
17:45:14.460 16  switches: fmods/sec:  774  787  834  756  797  834  542  757  760  791  812  798  829  823  535  699   total = 12.127988 per ms 
17:45:15.560 16  switches: fmods/sec:  776  793  773  743  738  706  816  736  800  804  772  804  762  721  780  744   total = 12.267975 per ms 
17:45:16.661 16  switches: fmods/sec:  864  935  725  905  868  716  931  779  855  954  715  907  841  706  927  780   total = 13.407987 per ms 
17:45:17.761 16  switches: fmods/sec:  771  805  811  931  890  891  782  825  785  781  787  900  847  906  771  840   total = 13.322973 per ms 
17:45:18.861 16  switches: fmods/sec:  808  794  778  873  796  846  816  855  854  795  795  883  770  857  839  846   total = 13.204868 per ms 
17:45:19.961 16  switches: fmods/sec:  939  876  871  805  790  789  815  825  952  875  872  798  817  828  809  805   total = 13.465987 per ms 
17:45:21.062 16  switches: fmods/sec:  829  917  944  888  843  889  783  783  819  892  918  878  821  900  787  782   total = 13.672795 per ms 
RESULT: 16 switches 9 tests min/max/avg/stdev = 6556.99/13672.79/11951.73/2257.99 responses/s
</pre>

* 10. Monitor CPU and RAM usage.

* 11. Repeat the test, increasing the number of switches (-s 32) and MACs/switch (-M 200000).

=== Known Issues ===
* Cbench Throughput test does not work well, CPU goes very high and cannot process incoming packet-in messages in an stable way. It looks like CPU is being used by Netty to flush some buffers, OF plugin devs are working on fixing this.
* The data store drop test '''dropAllPackets on''' does not work well either, flows get hung after a while. FRM seems to be the bottleneck, OF plugin devs are looking at this was well.

[[Category:Integration Group]]
