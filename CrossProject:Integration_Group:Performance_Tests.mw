== Introduction ==
Performance tests are needed not only for benchmarking our controller but also for improving our code by continuosly monitoring the collected data. We also stated that for next release '''Helium''' our focus will be performance and stability and as a result we have decided to create and maintain this wiki.

== Scope ==
The goal of this wiki is to:
* Provide a set of recommendations for doing performance test
* Describe which performance tests are currently running in OpenDaylight
* Publish performance test results

== Performance metrics ==
We agreed we are going to start with following performance metrics:

* Inventory performance:
** Max number of OF switches and ports
** Time/CPU/RAM consumed in learning switches, ports and topology 

* Flow performance:
** Max number of flows in memory
** Max flows/sec we can process
** Time/CPU/RAM consumed in pushing flows

== Controller requirements ==
In order to run an homogeneous test, we agreed on using this configuration for all performance test:

* HW configuration: 
** Use dedicated VM or bare-metal for controller
** CPU: 4 Cores
** RAM: 4 GB

* SW configuration: 
** Use controller Base edition. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
** ODL Log level set to ERROR, edit  opendaylight/configuration/logback.xml
** Set Java memory: Min=2G, Max=4G, in run.sh we have to add following JVM options:
<pre>
run.sh -Xms2g -Xmx4g    # Minimum heap size 2G and maximum heap size 4G. Increase to higher values if we encounter 
                        # OutOfMemoryError crashes when MACs and Switches count are increased in CBench cmdline args
</pre>

== Performance tools ==

* '''Mininet'''
Available at [http://mininet.org mininet.org] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]].
Mininet simulates a topology of openflow switches connected each other and also hosts attached to them. This tool can be used for inventory performance by increasing the number of switches and ports in the topology. It also allows custom configurations so that we can create topologies close to real scenarios (i.e. data centers)

* '''Cbench'''
Available at [http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New) Openflowhub] and also installed in the [[CrossProject:Integration_Group:Test_VMs|Test VMs]]. Cbench emulates a bunch of openflow switches which connect to a controller, send packet-in messages, and watch for flow-mods to get pushed down. This tool can be used for flow performance

* '''Enhanced Cbench'''
We can use Cbench with Performance test scripts created by Michal Rehak to generate a 2D/3D plot with Gnuplot. Check instructions in [[CrossProject:Integration_Group:Performance_Test_Enhanced_Cbench|Enhanced CBench Test]]

* '''Jconsole'''
With Jconsole it is possible to monitor CPU and RAM for a Java process. Instructions on how to start the Jconsole are available [[Media:Jconsole.pdf|here]].

* '''Yourkit'''
This tool can be used to monitor CPU and RAM per Java bundle, problem is that it is not free.

* '''Jenkins Monitoring plugin'''
Jenkins can also monitor CPU and RAM on slave nodes (like controller VM) through [https://wiki.jenkins-ci.org/display/JENKINS/Monitoring Monitoring plugin]

== Test Cases ==

=== Mininet Test ===
Still in design, the idea will be to bring up small to large topologies of 16-32-64-128-256-... switches and verify the controller can properly learn topology: switches, ports and links. Also measure CPU and RAM consumed on each iteration. We can also configure mininet to bring a custom topology close to a data center.

Test Steps:
* 1. Download latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
* 2. Set controller Log level to ERROR, edit opendaylight/configuration/logback.xml 
* 3. Start controller with recommended options: run.sh -of13 -Xms2g -Xmx4g
* 4. Start mininet with tree topology of 15 switches: '''sudo mn --controller=remote,ip=<controllerIP> --topo tree,4''' or custom topology
* 5. Check RESTCONF inventory with: 
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/restconf/operational/opendaylight-inventory:nodes(|grep "openflow:")
</pre>
* 6. Check AD-SAL topology through GUI or NB API:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/topology/default (|grep "openflow:")</pre>
Next steps 7&8 are designed to test AD-SAL ARP Handler and Simple Forwarding apps
* 7. Do a ping test in mininet: '''mininet> pingall'''
* 8. Check hosts are learned and flows are created:
<pre>
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/hosttracker/default/hosts/active
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/statistics/default/flow
</pre>
* 9. Monitor CPU and RAM
* 10. Repeat the test increasing the number of switches, like -tree,5 (31 switches)

=== CBench Test ===
Still in design, the idea will be to bring up small to large topologies of 16-32-64-128-256-... switches and benchmark how many flows/sec the controller can handle. Also measure CPU and RAM consumed on each iteration. This test needs the reactive forwarding bundle loaded into OSGi framework.
 
Test Steps:
* 1. Download latest controller base distribution. See [[CrossProject:Integration_Group:Controller_Artifacts|Controller Artifacts for testing]]
* 2. Download the OF plugin reactive forwarding bundle from same link and follow link instructions to install it in opendayligh/plugins
* 3. Delete two AD-SAL bundles simple forwarding and arp handler that interfere with MD-SAL Cbench measurements:
<pre>
rm plugins/org.opendaylight.controller.samples.simpleforwarding-0.4.2-SNAPSHOT.jar
rm plugins/org.opendaylight.controller.arphandler-0.5.2-SNAPSHOT.jar
</pre>
* 4. Set controller Log level to ERROR, edit opendaylight/configuration/logback.xml 
* 5. Start controller with recommended options: run.sh -of13 -Xms2g -Xmx4g
* 6. Initiate the drop test in the controller. There are two types of drop tests: RPC (programming flows directly through an RPC to the OF Plugin) and data store (programming flows by writing them into the MD-SAL config space, from where they are picked up by the FRM and programmed into the plugin). For the data store performance, the latter is of interest.
** To turn on the RPC drop test, type from the controller’s OSGI console: > dropAllPacketsRpc on
** To turn on the data store drop test, type from the controller’s OSGI console: > dropAllPackets on
* 7. Start Cbench test with 16 switches: '''cbench -c <controllerIP> -p 6633 -m 1000 -l 10 -s 16 -M 1000'''. The first run will probably fail. You need to repeat this a few times. See below for what you should see:
<pre>
mininet@mininet-vm:~$ cbench -c 192.168.162.1 -p 6633 -m 1000 -l 10 -s 16 -M 1000
cbench: controller benchmarking tool
   running in mode 'latency'
   connecting to controller at 192.168.162.1:6633 
   faking 16 switches offset 1 :: 10 tests each; 1000 ms per test
   with 1000 unique source MACs per switch
   learning destination mac addresses before the test
   starting test with 0 ms delay after features_reply
   ignoring first 1 "warmup" and last 0 "cooldown" loops
   connection delay of 0ms per 1 switch(es)
   debugging info is off
19:13:16.141 16  switches: flows/sec:  0  9  9  9  18  9  18  9  18  18  18  9  9  9  9  9   total = 0.180000 per ms 
19:13:17.242 16  switches: flows/sec:  26  26  25  26  25  25  25  25  24  24  25  23  24  23  24  22   total = 0.392000 per ms 
19:13:18.343 16  switches: flows/sec:  40  41  41  40  39  38  38  38  38  38  37  37  35  35  35  34   total = 0.603999 per ms 
19:13:19.445 16  switches: flows/sec:  55  54  52  55  52  52  52  51  51  50  50  49  50  48  48  47   total = 0.815998 per ms 
19:13:20.548 16  switches: flows/sec:  50  50  49  50  50  49  49  49  49  49  49  48  48  48  48  48   total = 0.782999 per ms 
19:13:21.649 16  switches: flows/sec:  50  50  50  50  50  49  50  50  49  49  49  49  48  49  48  48   total = 0.787999 per ms 
19:13:22.750 16  switches: flows/sec:  59  59  60  59  59  59  59  59  59  58  59  59  58  58  58  58   total = 0.939999 per ms 
19:13:23.852 16  switches: flows/sec:  50  50  50  51  50  50  50  51  50  50  50  50  49  49  50  49   total = 0.798999 per ms 
19:13:24.953 16  switches: flows/sec:  56  57  57  56  56  56  55  56  55  55  56  56  55  55  55  55   total = 0.890999 per ms 
19:13:26.054 16  switches: flows/sec:  58  57  58  58  57  57  58  57  58  56  57  57  57  56  56  56   total = 0.912999 per ms 
RESULT: 16 switches 9 tests min/max/avg/stdev = 392.00/940.00/769.55/162.58 responses/s
</pre>
* 8. Monitor CPU and RAM
* 9. Repeat the test increasing the number of switches (-s 32), and number of Mac/switch (-M 2000)

[[Category:Integration Group]]
