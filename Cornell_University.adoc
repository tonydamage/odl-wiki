[[managing-openflow-switches]]
== Managing OpenFlow switches

When Dell offered Cornell University the opportunity to deliver a
complete OpenFlow network, including 31 top of the line OpenFlow
switches, for a big discount for the new Gates Hall building, it was too
good an opportunity to pass up. In mid 2014, work began on installing
the switches and managing them with OpenDaylight, when a proprietary
solution they originally evaluated ended up being too expensive.

Dave Juers, the engineer responsible for managing the OpenFlow network,
is not a network engineer by trade - he started our conversation with a
big disclaimer: "I am a biologist by training, I came into this through
the back door". So when he initially started learning about
OpenDaylight, the learning curve was steep. Their first introduction to
OpenDaylight concepts was a high-level introduction: "we had a training
session in Silicon Valley where we used mininet and Beacon to learn the
basics of managing OpenFlow networks".

They got a lot of support from their hardware partner Dell, who designed
the network, installed 31 48-port Dell S4810 and S4820 switches, and set
up 4 separate OpenFlow instances on them. Since the switches are hybrid
switches, some configuration was required to make the network 100%
OpenFlow. The support was vital, because the people managing the network
were system administrators, not programmers.

[[choosing-opendaylight]]
== Choosing OpenDaylight

The deciding factors for their choice of OpenDaylight were price, and
the fact that it was open source. The philosophical alignment of open
source, community development was important to the team. After
originally testing the controller on a Windows laptop, OpenDaylight is
now deployed in production on Red Hat Enterprise Linux. Another factor
that was important was the ability to program flows through a user
interface - "we didn't want to have to connect through a command line
interface to program the switches", says Dave.

The university has now been running OpenDaylight Hydrogen in production
since mid 2014. The primary customers of the network are the Computer
Science and Information Sciences departments who are resident in Gates
Hall, but the network is currently servicing the needs of 4 departments.
The network provides the infrastructure for multiple production
workloads, including file servers for research projects, virtual
machines for student work. One of the goals of Dave's team was to reduce
redundancy caused by multiple departments maintaining their own IT
staff, and the OpenFlow network has been very useful in working towards
that goal. One of the particularities of the university network is that
many of the resources are on the publicly addressable internet, which
makes running the network particularly challenging.

[[future-plans]]
== Future plans

When I asked Dave what he was looking forward to in the future from
OpenDaylight, his main request was access to fine grained analytics and
data from network devices. The ability to get real-time diagnostics
per-port, and to verify easily that data was flowing as intended through
the underlying fabric, would make his life easier when diagnosing issues
on the network.

The team is currently looking at migrating from Hydrogen to Helium. They
are already using Helium on a research and development network, and they
hope to move the production network over soon to take advantage of
stability and features in the newer release. One of the projects driving
the move to Helium is a plan to deploy Infrastructure as a Service
software on the network, which will happen in the coming months.

What is his favourite thing about the OpenDaylight community? "They're
*everywhere*! I run into OpenDaylight people at conferences, I read
their blogs. Vendors I speak with always have connections with and
support the OpenDaylight community. Being part of the OpenDaylight
community feels like being part of a _movement_."

Category:Case studies[Category:Case studies]
