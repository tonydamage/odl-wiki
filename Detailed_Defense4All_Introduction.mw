===[[Detailed Defense4All Introduction:Introduction|Introduction]] ===
A well-known DoS attack mitigation and threats detection strategy is to divert suspected traffic from its normal network path to dedicated attack mitigation infrastructures, for cleaning and threat detection. These infrastructures are also known as “Security Centers” or “Scrubbing Centers”, mainly built of L3-L7 DoS attack mitigations devices. These “Security Centers” can be deployed in dedicated remote sites within the network in “out of path” manner (i.e. not “inline” with the native traffic flow), so diversion of traffic toward these centers is essential. During the traffic cleaning process, the attack mitigation infrastructure identifies and drops malicious IP packets and forwards legitimate IP packets back to their original targeted network destinations. The “Scrubbing Centers” can be located within the enterprise cooperate network, datacenter, cloud and also as part of carrier’s infrastructure.   
In general protection against DDoS attacks in Out-Off-Path (OOP) systems comprises three main elements: 
# Collection of traffic statistics and learning of statistics behavior of protected objects during peace-time. From this collected statistics, the normal traffic baselines of the protected objects are built.  
# Detection of DoS attack patterns as traffic anomalies deviating from normal baselines.  
# Diversion of suspicious traffic from its normal path to mitigation (scrubbing) centers for traffic cleansing, selective source blockage, etc.  “Clean” traffic out of scrubbing centers is re-injected back to packet’s original destination. 
Defense4All is a security SDN application for detecting and driving mitigation of DoS/DDoS attacks in different SDN topologies. It realizes anti-DoS in Out Of Path mode for OpenDaylight SDN environment. Administrators can configure Defense4All to protect certain networks/servers (henceforth, protected networks or PNs). 

Defense4All exploits SDN capabilities to count specified traffic, and installs traffic counting flows for each protocol of each configured PN in every network location through which traffic of the subject PN flows. Defense4All then monitors traffic of all configured PNs, summarizing readings, rates, and averages from all relevant network locations. In case it detects a deviation from normal learned traffic behavior in some protocol (TCP, UDP, ICMP, or rest of the traffic) of some PN, Defense4All declares an attack against that protocol in the subject PN. 

To mitigate a detected attack Defense4All performs several steps: 1) It selects one or more mitigation devices to mitigate the attack. 2) It configures SDN to divert attacked traffic through the mitigation devices. 3) It continues monitoring attacked traffic both from SDN and (optionally) from mitigation devices. When Defense4All receives no indications about the attack it cancels attacked traffic diversions and returns to peace-time monitoring.

Administrators need to notify Defense4All about relevant SDN controller, switches/routers, and mitigation devices of choice. Defense4All users can retrieve all present and past information about PN learned traffic normal, attacks and mitigations, health and status of mitigation devices and links, SDN configurations, and other operational information. Past information is stored in persistent flight recorder storage.

In this version Defense4All runs as a single instance (non-clustered), but it integrates three main fault tolerance features - 1) it runs as a Linux service that is automatically restarted should it fail, 2) its state is entirely persisted in stable storage, and upon restart Defense4All obtains the latest state, and 3) it carries a health tracker with restart and (several degrees of) reset capabilities to overcome certain logical and aging bugs.

Defense4All is designed for pluggability, allowing integration of different types of mitigation devices [Defense4All contains a reference implementation of a driver to Radware’s mitigation device – DefensePro. To attach some other mitigation device one needs to integrate driver to that device into Defense4All.], different SDN and non-SDN based attack detectors, different mitigation drivers, and different abstraction levels of network controller functionality for collecting traffic statistics and for traffic diversion. Finally, Defense4All itself resides in a framework, and can be replaced by other SDN applications [SDN and mitigation device drivers should be repackaged into the framework rather than Defense4All].

The diagram below describes the possible state of any given PN. Radware DefensePro, abbreviated as "DP" is an example of an incorporated AMS.
[[File:pn_possible_states.jpg|none|900px|PN possible states]]

===[[Detailed Defense4All Introduction:Deployment alternatives|Deployment alternatives]] ===
Defense4All supports “short diversion”, in which the AMS (Attack mitigation system) is connected to the edge router, so one hop traffic redirection is achieved. See also PE1 and DefensePro 2 in Figure 3. “Long diversion”, in which AMS scrubbing centers are located in arbitrary remote locations in the network, may be added in future Defense4All versions. See also PE2 and DefensePro 1 in Figure below.  
Defense4All supports both automatic and manual diversion modes. The manual mode includes user-based confirmation before diversion.
[[File:redirection_alternatives.jpg|none|900px|Defense4All Deployment traffic redirection alternatives]]

===[[Detailed Defense4All Introduction:Defense4All in ODL Environment|Defense4All in ODL Environment]] ===
Defense4All is an SDN application for detecting and mitigating DDoS attacks. The application communicates with OpenDaylight Controller via the ODC north-bound REST API. Through this API Defense4All performs two main tasks:
# Monitoring behavior of protected traffic - the application sets flow entries in selected network locations to read traffic statistics for each of the PNs (aggregating statistics collected for a given PN from multiple locations).
# Diverting attacked traffic to selected AMSs – the application set flow entries in selected network locations to divert traffic to selected AMSs. When an attack is over the application removes these flow entries, thus returning to normal operation and traffic monitoring.
Defense4All can optionally communicate with the defined AMSs – e.g., to dynamically configure them, monitor them or collect and act upon attack statistics from the AMSs. The API to AMS is not standardized, and in any case beyond the scope of the OpenDaylight work. Defense4All contains a reference implementation pluggable driver to communicate with Radware’s DefensePro AMS.
The application presents its north-bound REST and CLI APIs to allow its manager to:
# Control and configure the application (runtime parameters, ODC connectivity, AMSs in domain, PNs, etc.).
# Obtain reporting data – operational or security, current or historical, unified from Defense4All and other sources (like ODC, AMSs).
[[File:D4A_in_odl.jpg|none|600px|Defense4All logical positioning in OpenDaylight environment]]
Defense4All comprises an SDN applications Framework and the Defense4All application itself – packaged as a single entity. Application integration into the Framework is pluggable, so any other SDN application can benefit from the common Framework services. The main advantages of this architecture are:
# Faster application development and changes - the Framework contains common code for multiple applications, complex elements (e.g., clustering or repository services) are implemented once for the benefit of any application.
# Faster, flexible deployment in different environments, form-factors, satisfying different NFRs – the Framework masks from SDN applications factors such as required data survivability, scale and elasticity, availability, security.
# Enhanced robustness - complex Framework code is implemented and tested once, cleaner separation of concerns leads to more stable code, Framework can increase robustness proactively with no additional code in application logic (e.g., periodic application recycle).
# Unified management of common aspects – common look and feel.
===[[Detailed Defense4All Introduction:Defense4All Framework View|Defense4All Framework View]] ===
[[File:framework_view.jpg|none|900px|Defense4All Structure from Framework point of view]]
The Framework contains the following elements:

'''FrameworkMain''' – the Framework root point contains references to all Framework modules and global repositories, as well as the roots of deployed SDN applications (in current version the Framework can accommodate only one). This is also the point to start, stop or reset the Framework (along with its hosted application)
WebServer – Jetty web server running the Jersey RESTful Web Services framework, with Jackson parser for JSON encoded parameters. The REST Web Server runs a servlet for Framework and another servlet for each deployed application (currently only one). All REST and CLI APIs are supported through this REST Web Server.

'''FrameworkRestService''' – A set of classes constituting the Framework Servlet that responds to Framework REST requests (e.g., get latest Flight Recorder records, perform factory reset, etc.). The FrameworkRestService invokes control and configuration methods against the FrameworkMgmtPoint, and for reporting it retrieves information directly from the relevant repositories. For flight recordings it invokes methods against the FlightRecorder.

'''FrameworkMgmtPoint''' – Is the point to drive control and configuration commands (e.g., start, stop, reset, set address of the hosting machine, etc.). FrameworkMgmtPoint invokes in turn methods against other relevant modules in the right order. It forwards lifecycle requests (start, stop, reset) directly to FrameworkMain to drive them in the right order.
'''Defense4All Application''' – Is the AppRoot object that should be implemented/extended by any SDN application – in our case Defense4All. SDN Applications do not have “main”, and their lifecycle (start, stop, reset) is managed by the Framework operating against the Application root object, which then drives all lifecycle operations in the application. This module also contains reference back to the Framework, allowing the application to use Framework services (e.g., create a Repo, log a flight record) and common utilities.
'''Common classes and utilities''' – is a library of convenience classes and utilities, from which any Framework or SDN Application module can benefit. Examples include wrapped threading services (for asynchronous, periodic or background execution), short hash of a string, confirmation by user, etc.

'''Repository services''' – One of the key elements in the Framework philosophy is decoupling compute state from compute logic. All durable state should be stored in a set of repositories that can be then replicated, cached, distributed under the covers –with no awareness of the compute logic (Framework or Application). Repository services comprise the RepoFactory and Repo or its annotations friendly equivalent – the EntityManager. The RepoFactory is responsible to establish connectivity with the underlying Repository plugged in service, instantiate new requested repositories and return references to existing ones. The chosen underlying Repository service is Hector Client over Cassandra NoSQL DB. Repo presents an abstraction of a single DB table. It allows reading the whole table, only table keys (tables are indexed by only the single primary key), records or single cells, as well as writing records or single cells with controlled eagerness. A sub-record (with only a portion of cells) may be written. In such a case the appearing cells override existing ones in the repository. Other cells in the repository remain unchanged. In contrast to relational DB, in which all columns must be specified up-front (in schema design), Repo leverages the underlying Cassandra support to contain rows (records) in the same table with different sets of columns, some of which may not being even defined up-front. Furthermore, cells with new columns can be added or removed on the fly. RepoFactory and Repo (as well as its Entity Manager annotation friendly equivalent) constitute a convenience library targeted to Framework and SDN Applications goals – on top of the Hector client library communicating with Cassandra Repository cluster. Scaling Cassandra cluster, distributing data shards across Cassandra cluster members, configuring read/write eagerness and consistency – are for the most part encapsulated in this layer. 

'''Logging and Flight Recorder services''' – The logging service simply uses Log4J library to log error, warning, trace or informational messages. These logs are mainly for Defense4All developers. Administrators can obtain additional details about failures from errors log. FlightRecorder records all flight records recorded by any Defense4All module, including information received from external network elements, such as ODC, AMSs, etc. It then allows a user/administrator to obtain that information through REST/CLI. Flight records can be filtered by categories (zero or more can be specified) and by time ranges. FlightRecorder stores all flight records in its own Repo (with another repo holding time ranges for efficient time ranges retrieval from the records repo). Because all flight records are stored in Cassandra, the number of flight records Defense4All can keep is limited only by the size of the underlying persistent storage capacity of all Cassandra servers, and so even on a single Cassandra instance months of historical information can be kept.

'''HealthTracker''' – Is the point to hold the aggregated runtime health of Defense4All, and to act in response to severe deteriorations. Any module, upon sensing an unexpected/faulty behavior in it or in any other module can record a “health issue” in the HealthTracker, providing health issue significance. This is instead of directly triggering Defense4All termination. The idea is that numerous health issues in a short period of time with high aggregated significance are likely to indicate a significant wide-spread Defense4All problem, but sporadic/intermittent operational “hiccups” can be neglected – even if Defense4All remains less than 100% operational (the administrator can always reset restart it to fully recover). As such, every non-permanent health issue has a gradually diminished affect over time. If Defense4Al health deteriorates below a predefined threshold HealthTracker triggers responsive actions – depending on the nature of health issues. A restart can heal transient problems, and so the HealthTracker triggers Defense4All termination (running as a Linux service Defense4All will be automatically restarted). To recover from more permanent problems HealthTracker may additional trigger a Defense4All reset. If it does not help then the next time the HealthTracker will attempt a more severe reset. As a last resort the administrator can be suggested to perform factory reset.

'''ClusterMgr''' – Currently not implemented. This module is responsible for managing a Defense4All cluster (separate from Cassandra or ODC clusters, modeled as separate tier clusters). A clustered Defense4All carries improved high availability and scalability. Any module in Defense4All Framework or Application can register with ClusterMgr for clustered operation, specifying whether its functionality should be carried out by a single or by multiple/all active instances (running on different Defense4All cluster members). When cluster membership changes, ClusterMgr notifies each instance in each module about its role in the clustered operation of that module. In case of a single active instance that instance is told so, while all other instances are told they are standby. In case of multiple active instances, each active instance is notified about the number of active instances, and its logical enumeration in that range. All state is stored in a globally accessible and shared repository, so any instance of a module is stateless, and can perform any role after every membership change. For example, following membership change N an instance can be enumerated as 2 out of 7, and thus perform relevant portion of the work. Then at membership change N+1 the same instance can be enumerated 5 out of 6, and perform the work portion allocated for 5 and not for 2. We skip the peer messaging services which the ClusterMgr can provide for a more coordinated cross-instance operation.

The Defense4All Application is highly pluggable - it can accommodate different attack detection mechanisms, different attack mitigation drivers, and drivers (called reps – short for representative) to different versions of ODC and different AMSs. Defense4All Application comprises “core” modules and “pluggable” modules implementing well-defined Defense4All Application APIs. 

===[[Detailed Defense4All Introduction:Defense4All Application View|Defense4All Application View]] ===
[[File:d4a_application_view.jpg|none|900px|Defense4All Defense4All Application Structure]]

The Defense4All Application modules are described below:

'''DFAppRoot''' – This is the root module of the Defense4All Application. As mentioned before, the Defense4All Application does not have “main”, and its lifecycle (start, stop, reset) is managed by the Framework operating against this module, which in turn drives all lifecycle operations in the Defense4All Application. DFAppRoot also contains references to all Defense4All Application modules (core and pluggable), global repositories, and reference back to the Framework, allowing the Defense4All Application modules to use Framework services (e.g., create a Repo, log a flight record) and common utilities.

'''DFRestService''' – A set of classes constituting the Defense4All Application Servlet that responds to Defense4All Application REST requests. The DFRestService invokes control and configuration methods against the DFMgmtPoint, and for reporting it retrieves information directly from the relevant repositories. For flight recordings it invokes methods against the FlightRecorder.

'''DFMgmtPoint''' – The point to drive control and configuration commands (e.g., addams, addpn). DFMgmtPoint invokes in turn methods against other relevant modules in the right order.

'''ODL Reps''' – Is a pluggable module-set for different versions of ODC. Comprises two functions in two sub-modules – stats collection for and traffic diversion of relevant traffic. These two sub-modules adhere to StatsCollectionRep DvsnRep APIs. ODL Reps is detailed in Figure 6 and description that follows it.
SDNStatsCollector – Is responsible to set “counters” for every PN at specified network locations (physical or logical). A counter is a set of OpenFlow flow entries in ODC enabled network switches/routers. The SDNStatsCollector periodically collects statistics from those counters and feeds them to the SDNBasedDetectionMgr (see next). The module uses the SDNStatsCollectionRep to both set the counters and read latest statistics from those counters. A stat report consists of read time, counter specification, PN label, and a list of trafficData information, where each trafficData element contains latest bytes and packets values for flow entries configured for <protocol,port,direction> in the counter location. Protocol can be {tcp,udp,icmp,other ip}, port is any layer 4 port, and direction can be {inbound, outbound}.

'''SDNBasedDetectionMgr''' – Is a container for pluggable SDN based detectors. It feeds stat reports, received from SDNStatsCollector, to plugged-in SDN based detectors. It also feeds to all SDN based detectors notifications from AttackDecisionPoint (see ahead) about ended attacks (so as to allow reset of detection mechanisms). 

'''RateBasedDetector sub-module''' – This detector learns for each PN its normal traffic behavior over time, and notifies AttackDecisionPoint (see next) when it detects traffic anomalies. For each protocol {tcp, udp, icmp, other ip} of each PN the RateBasedDetector maintains latest rates and exponential moving averages (baselines) of bytes and packets, as well as last reading time. The detector maintains those values both for each counter as well as aggregation of all counters for each PN. The organization at two levels of calculations (counter and PN aggregate) allows for better scalability (e.g., working with clustered ODC, where each instance is responsible to obtain statistics from a portion of network switches, and bypassing the ODC single instance image API).  Such organization also enables a more precise stats collection (avoiding the difficulty to collect all stats during a very small time interval). Stats are processed at the counter level, and periodically aggregated at the PN level. Continuous detections of traffic anomalies cause the RateBasedDetector to notify AttackDecisionPoint about attack detection. Then absence of anomalies for some period of time causes the detector to stop notifying AttackDecisionPoint about attack detection. The detector specifies a detection duration – time within which the detection is valid. After that time the detection expires, but can be “prolonged” with another notification about the same attack.

'''AttackDecisionPoint''' – This module is responsible to maintain attack lifecycles. It can receive attack detections from multiple detectors. Defense4All supports the RateBasedDetector, external detectors (in future versions), and AMS based detector reference implementation (over Radware’s DefensePro). In current version AttackDecisionPoint fully honors each detection (max detector confidence, max detection confidence). It declares a new attack for every detection of a new attacked traffic (PN, protocol, port), and add more detections for existing (already declared attacks). The module checks periodically the statuses of all attacks. As long as there is at least one unexpired detection (each detection has an expiration time) attack is kept declared. If all detections are expired for a given attack AttackDecisionPoint declared attack end. The module notifies the MitigationMgr (see next) to start mitigating any new declared attack. It notifies the MitigationMgr to stop mitigating ended attacks, and also notifies the detectionMgr to reset stats calculations for traffic on which an attack has just ended.
 
'''MitigationMgr''' - Is a container for pluggable mitigation drivers. The MitigationMgr maintains the lifecycle of all mitigations, resulted from mitigation notifications from AttackDecisionPoint. It holds a pre-ordered list of the MitigationDriver sub-modules, and attempts to satisfy each mitigation in that order. If MitigationDriveri indicates to MitigationMgr that it does not mitigate a mitigation (because of per PN preferences, unavailability of AMS resources, network problems, etc.) MitigationMgr will attempt mitigation by MitigationDriveri+1. If none of the plugged-in MitigationDrivers handle mitigation it remains in status ‘not-mitigated’. 

'''MitigationDriverLocal''' – This mitigation driver is responsible to drive attack mitigations using AMSs in its sphere of management. When requested to mitigate an attack this mitigator performs the following sequence of steps:
# It consults with the plugged in DvsnRep (see ahead) about topologically feasible options of diversion to each of the managed AMSs from each of the relevant network locations. In this version diversion is always done from the location where the stats counters are installed. 
# The MitigationDriverLocal then selects an AMS out of all feasible options (in the first release the selection is trivial – first in list.
# It optionally configures all the AMSs (each diversion source may have a different AMS associated with it) prior to instructing to divert traffic to each. This is done through the plugged in AMSRep. 
# The MitigationDriverLocal instructs the DvsnRep to divert traffic from each source NetNode (in this version NetNode is modeled over an SDN Switch) to the AMS associated with that NetNode. Diversion can be either for inbound traffic only or both for inbound and outbound traffic.
# The mitigation driver notifies the AMSBasedDetector to optionally start monitoring attack status in all the AMSs, and feed attack detections to AttackDecisionPoint.
# In future versions the MitigationDriverLocal should monitor health of all AMSs and relevant portions of network topologies, re-selecting AMSs should some fail, or should network topologies changes require that.
When mitigation should be ended the MitigationDriverLocal notifies AMSBasedDetector to stop monitoring attack status for the ended attack, notifies DvsnRep to stop traffic diversions to all AMSs – for this mitigation, and finally notifies the AMSRep to optionally clean all mitigation related configuration set in each relevant AMS.

'''AMSBasedDetector''' – This optional module (can be packaged as part of the AMSRep) is responsible for monitoring/querying attack mitigation by AMSs. Registering as a detector this module can then notify AttackDecisionPoint about attack continuations and endings. It monitors only specified AMSs and only for specified (attacked) traffic.

'''AMSRep''' - Is a pluggable module for different AMSs. The module adheres to AMSRep APIs. It can support configuration of all introduced AMSs (permanently or before/after attack mitigations). It can also receive/query security information (attack statuses), as well as operational information (health, load). AMSRep module is entirely optional – AMSs can be configured and monitored externally. In many cases attacks can continue be monitored solely via SDN counters. Defense4All contains a reference implementation AMSRep that communicates with Radware’s DefensePro AMSs.

===[[Detailed Defense4All Introduction:Defense4All ODL Reps View|Defense4All ODL Reps View]] ===

[[File:d4a_odl_reps_view.jpg|none|900px|Defense4All Defense4All ODL Reps Structure]]

The figure depicts the Defense4All Application ODL Reps module-set structure. Different versions of OFC may be represented by different versions of ODL Reps module-set. As mentioned before, ODLReps comprises two functions – stats collection for and traffic diversion of relevant traffic. Both or either of the functions may be utilized in a given deployment. As such they have a common point to communicate with the ODC and hold all general information – the ODC (see below).

ODL Reps supports two types of SDN Switches: sdn-hybrid, which support both SDN and legacy routing, and sdn-native, which supports SDN only routing. Counting traffic in sdn-hybrid switch can be simply accomplished by programming a flow entry with desired traffic selection criteria and the action is “send to normal”, i.e., continue with legacy routing. Counting traffic in sdn-native switch requires an explicit routing action (i.e., which output port to send the traffic to). Defense4All avoids learning all routing tables by requiring an sdn-native switch which is more or less a bump-in the wire with respect to traffic routing, i.e., traffic entering port 1 normally exits port 2 and traffic entering port 3 normally exits port 4 and vice versa. Such a switch allows for easy programming of flow entries just to count traffic or to divert traffic to/from attached AMS. So when Defense4All programs a traffic counting flow entry with selection criteria that includes port 1, its action will be output to port 2, and similarly with 3 to 4. In future versions this restriction should be lifted.

The sub-modules are described below:

'''StatsCollectionRep''' - The module adheres to StatsCollectionRep APIs. Its main tasks are:
* Offer counter placement NetNodes in the network. The NetNodes offered are all NetNodes defined for a PN. This essentially maps to all SDN Switches through which traffic of the given PN flows.
* Add a peacetime counter in selected NetNodes to collect statistics for a given PN. StatsCollectionRep creates a single counter for a PN in each NetNodes. (Overall, a NetNodes can have multiple counters for different PNs; and a PN may have multiple counters - in NetNodes as specified for the given PN). StatsCollectionRep translates installation of a counter in NetNodes to programming 4 flow entries (for tcp, udp, icmp, and rest of ip) for each “north traffic port” in that NetNodes – port from which traffic from client to protected PN enters the SDN Switch. For example, StatsCollectionRep will add for a given PN 12 flow entries in an SDN Switch with 3 ports through that PN’s inbound traffic enters the OFS. And, if another NetNode (SDN Switch) was specified to have that PN’s inbound traffic entering it through 2 ports, then StatsCollectionRep will program for this PN 8 flow entries in that second NetNode.
* Remove a peacetime counter.
* Read latest counter values for a specified counter. StatsCollectionRep returns a vector of latest bytes and packets counted for each protocol-port in each direction (currently only “north to south” is supported), along with the time it received the reading from ODC.

'''DvsnRep''' - The module adheres to DvsnRep APIs. Its main tasks are:
* Return diversion properties from a given NetNode to a given AMS. In this version an empty properties is returned if such diversion is topologically feasible (AMS is directly attached to the SDN Switch over which the specified NetNode is modeled. Otherwise no properties is returned. (This leaves room for remote diversions in future versions, and topological costs to each distant AMS – e.g., latency, bandwidth reservation, cost, etc.).
* Divert (attacked) traffic from a specified NetNode through an AMS.  As such the new flow entries take precedence over the peacetime ones. DvsnRep programs flow entries to divert inbound attacked traffic (or all traffic if so specified for the PN) from every “north traffic port” into AMS “north” port. If “symmetric diversion” (for both inbound and returning, outbound traffic) has been specified for that PN, DvsnRep programs another set of flow entries to divert attacked (or all) traffic from every “south traffic port” into AMS “south” port. In an sdn-hybrid switch deployment DvsnRep adds a flow entry for inbound traffic that returns from AMS south port – with action send to normal, and similarly it adds a flow entry for outbound returning traffic from AMS north port – with action of also send to normal. In sdn-native switch the action is to send to the right output port, but things are more complicated here determining that right port. We use north port MAC learning to determine from the source/destination MAC in the packet the right output port. This scheme of flow entries works well for tcp, udp and icmp attacks. For “other ip” attacks the flow entries programming is more complex, and is suppressed here for clarity. The set of flow entries programmed to divert (but still count) traffic comprises the “attack traffic floor”. There may be many attack traffic floors, all of which take precedence over the peacetime stats collection floor (by programming higher priority flow entries). Additional attacks (except “other ip” attack which is special case, and is suppressed here) are created with higher priority traffic floors over previously set attack traffic floors. Attacks may fully or partially “eclipse” earlier attacks (e.g., tcp port 80 over tcp or vice versa) or be disjoint (e.g., tcp and udp). Stats collection is taken from all traffic floors – peacetime and attacks. SDN based detector aggregates all statistics into overall rates, thus determining if the attack is still on. (Notice that eclipsed peacetime counted traffic may show zero rates, and that counting is complemented by the higher priority floor counters.
* End diversion. DvsnRep removes the relevant attack traffic floor (removing all its flow entries from the NetNode). Note that this affects neither traffic floors “above” the removed floor nor the traffic floors “below”. In addition, the SDN based detector receives the same aggregated rates from counters of remaining floors, so its operation is not affected either.

'''ODLCommon''' – This module contains all common elements needed to program flow entries in ODC. This allows for coherent programming of configured ODCs (in this version at most one) by StatsCollectionRep and DvsnRep. For instance ODLCommon instantiates connectivity with the ODCs, maintains a list of programmed flow entries and cookies assigned each. It also maintains references to DFAppRoot and FrameworkMain. When an sdn-native NetNode is added ODLCommon programs 2 flow entries per each protected link (pair of input-to-output ports) to transfer traffic between the two ports (traffic entering north port is routed to south port and vice versa). ODLCommon adds 2 more flow entries for each port connecting an AMS – to block returning ARP traffic (so as to avoid ARP floods if AMSs are not configured to block them). This “common traffic floor” flow entries are set with lowest priority. Their counters are accounted for neither stats collections nor traffic diversion. When a NetNode is removed ODLCommon removes this common traffic floor flow entries.

'''FlowEntryMgr''' – This module provides an API to perform actions on flow entries in an SDN switch managed by an ODC, and retrieve information about all nodes managed by an ODC. Flow entries actions include adding a specified flow entry in a specified NetNode (SDN Switch/Router), removing a flow entry, toggling a flow entry, getting details of a flow entry, and readings statistics gathered by the flow entry. FlowEntryMgr uses the Connector modules to communicate with ODC.
Connector – This module provides the basic APIs to communicate with ODC, wrapping REST communications. After initializing connection details with a specified ODC the Connector allows getting or deleting something from ODC, as well as posting or putting something to ODC. 
ODL REST Pojos – This set of java classes are part of the ODC REST API, specifying the Java classes of the parameters and the results of interaction with ODC.

a
