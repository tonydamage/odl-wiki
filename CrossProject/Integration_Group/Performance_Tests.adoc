[[introduction]]
== Introduction

Performance tests are needed not only for benchmarking our controller
but also for improving our code by continuosly monitoring the collected
data. We also stated that for next release *Helium* our focus will be
performance and stability and as a result we have decided to create and
maintain this wiki.

[[scope]]
== Scope

The goal of this wiki is to:

* Provide a set of recommendations for doing performance test
* Describe which performance tests are currently running in OpenDaylight
* Publish performance test results

[[performance-metrics]]
== Performance metrics

We agreed we are going to start with following performance metrics:

* Inventory Scalability:
** Standalone Box
*** Max number of OF switches and ports
*** Max number of links
*** Max number of network hosts
*** Max number of REST calls per second
*** Time/CPU/RAM consumed in learning switches, ports and topology
** Clustering Scalability
*** Max number of Controller Nodes in cluster.(What is the maximum
value?)
*** Max number of OF switches and ports
*** Max number of Links
*** Max number of hosts
*** Max number of REST calls per second
*** Time/CPU/RAM consumed in learning switches, ports and topology

* Flow performance:
** Max flows/sec we can process for OF1.0
** Max flows/sec we can process for OF1.3
** Packet latency in OF1.0
** Packet latency in OF1.3
** Flow performance in clustered environment
** Layer 2 switch performance (As a factor of number of switches in the
path)
** Time/CPU/RAM consumed in above cases

* REST Performance
** REST Call latency
** SOUTHBOUND REST Call Latency (Eg: Time to install a flow on a switch)

* Datastore Performance
** Max data writes per second
** Max data reads per second
** Data read/write performance in a clustered environment
*** Read performance on a Leader.
*** Write performance on a Leader.
*** Read performance on a Follower.
*** Write performance on a Follower.

[[controller-baseline]]
== Controller Baseline

In order to run an homogeneous test, we agreed on using this
configuration for all performance test:

* HW/SW configuration:
** Use dedicated VM or bare-metal for controller
** CPU: 4 Cores
** RAM: 6 GB
** OS: Linux Server (no Desktop) is recommended. Report used OS when
publishing results.

* Controller configuration:
** Use ODL Helium (Karaf). See
CrossProject:Integration_Group:Controller_Artifacts[Controller Artifacts
for testing]
** Set the ODL log level to ERROR by editing
etc/org.ops4j.pax.logging.cfg, replacing `rootLogger=INFO` with
`rootLogger=ERROR`
** Set the minimum and maximum heap size to 4G. When min and max are the
same, there will be no dynamic heap allocation. If increasing to higher
values, increase both at same time for better performance.
** Use G1GC garbage collection strategy.
***
http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html[Getting
Started with the G1GC Garbage Collector]
***
http://www.slideshare.net/MonicaBeckwith/garbage-first-garbage-collector-g1-gc-migration-to-expectations-and-advanced-tuning[Garbage
First Garbage Collector (G1 GC) - Migration to, Expectations and
Advanced Tuning]

[[performance-tools-overview]]
== Performance Tools Overview

[[actively-used-or-under-active-development]]
=== Actively Used or Under Active Development

[[wcbench]]
==== WCBench

https://github.com/dfarrell07/wcbench[WCBench] consumes CBench as a
library, then builds a robust test automation, stats collection and
stats analysis/graphing system around it. Using WCBench, instead of
CBench directly, is strongly recommended. Please see the
https://github.com/dfarrell07/wcbench/blob/master/README.md[WCBench
README] for detailed documentation.

[[restconf-performance-scale-tools]]
==== RESTCONF Performance & Scale Tools

Set of tools to measure the controller performance and scale through its
NorthBound API. The tools can be used interactively (from command line)
or programmatically (called from scripts). Detailed description can be
found here.

[[mininet]]
==== Mininet

Available at http://mininet.org[mininet.org] and also installed in the
CrossProject:Integration_Group:Test_VMs[Test VMs]. Mininet simulates a
topology of OpenFlow switches connected each other and also hosts
attached to them. This tool can be used for inventory performance by
increasing the number of switches and ports in the topology. It also
allows custom configurations so that we can create topologies close to
real scenarios (i.e. data centers)

[[additional-tools]]
=== Additional Tools

[[cbench]]
==== CBench

Available at
http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New)[Openflowhub]
and also installed in the CrossProject:Integration_Group:Test_VMs[Test
VMs]. CBench emulates a bunch of OpenFlow switches which connect to a
controller, send packet-in messages, and watch for flow-mods to get
pushed down. The recommended way to use CBench is through
https://github.com/dfarrell07/wcbench[WCBench].

[[enhanced-cbench]]
==== Enhanced Cbench

We can use Cbench with Performance test scripts created by Michal Rehak
to generate a 2D/3D plot with Gnuplot. Check instructions in
CrossProject:Integration_Group:Performance_Test_Enhanced_Cbench[Enhanced
CBench Test]

[[jconsole]]
==== Jconsole

With Jconsole it is possible to monitor CPU and RAM for a Java process.
Instructions on how to start the Jconsole are available
Media:Jconsole.pdf[here].

[[yourkit]]
==== Yourkit

This tool can be used to monitor CPU and RAM per Java bundle, problem is
that it is not free.

[[performance-tools-details]]
== Performance Tools Details

[[wcbench-1]]
=== WCBench

The authoritative source for WCBench documentation is the
https://github.com/dfarrell07/wcbench/blob/master/README.md[README] in
its https://github.com/dfarrell07/wcbench[GitHub repo].

[[installing-cbench-and-opendaylight]]
==== Installing CBench and OpenDaylight

---------------------------------------------------
[fedora@dfarrell-wcbench wcbench]$ ./wcbench.sh -ci
CBench is not installed
Installing CBench dependencies
Cloning CBench repo
Cloning openflow source code
Building oflops/configure file
Building CBench
CBench is installed
Successfully installed CBench
Installing OpenDaylight dependencies
Downloading last successful ODL build
Unzipping last successful ODL build
Downloading openflowplugin
Removing simpleforwarding plugin
Removing arphandler plugin
---------------------------------------------------

[[starting-opendaylight]]
==== Starting OpenDaylight

-------------------------------------------------------------------
[fedora@dfarrell-wcbench wcbench]$ ./wcbench.sh -o
Starting OpenDaylight
Giving ODL 90 seconds to get up and running
80 seconds remaining
70 seconds remaining
60 seconds remaining
50 seconds remaining
40 seconds remaining
30 seconds remaining
20 seconds remaining
10 seconds remaining
0 seconds remaining
Installing telnet, as it's required for issuing ODL config.
Issuing `dropAllPacketsRpc on` command via telnet to localhost:2400
Trying ::1...
Connected to localhost.
Escape character is '^]'.
osgi> dropAllPacketsRpc on
DropAllFlows transitions to on
osgi> Connection closed by foreign host.
-------------------------------------------------------------------

[[running-a-test]]
==== Running a Test

--------------------------------------------------------------------
[fedora@dfarrell-wcbench wcbench]$ ./wcbench.sh -t 2 -r
Set MS_PER_TEST to 120000, TESTS_PER_SWITCH to 1, CBENCH_WARMUP to 0
Collecting pre-test stats
Running CBench against ODL on localhost:6633
Collecting post-test stats
Collecting time-irrelevant stats
Average responses/second: 22384.52
/home/fedora/results.csv not found or empty, building fresh one
--------------------------------------------------------------------

[[generating-stats]]
==== Generating Stats

------------------------------------------------------------------------------
[~/wcbench]$ ./stats.py -S
{'fifteen_load': {'max': 0,
                  'mean': 0.62,
                  'min': 0,
                  'relstddev': 0.0,
                  'stddev': 0.0},
 'five_load': {'max': 0,
               'mean': 0.96,
               'min': 0,
               'relstddev': 0.0,
               'stddev': 0.0},
 'flows': {'max': 22384,
           'mean': 22384.52,
           'min': 22384,
           'relstddev': 0.0,
           'stddev': 0.0},
 'iowait': {'max': 0, 'mean': 0.0, 'min': 0, 'relstddev': 0.0, 'stddev': 0.0},
 'one_load': {'max': 0,
              'mean': 0.85,
              'min': 0,
              'relstddev': 0.0,
              'stddev': 0.0},
 'runtime': {'max': 120,
             'mean': 120.0,
             'min': 120,
             'relstddev': 0.0,
             'stddev': 0.0},
 'sample_size': 1,
 'steal_time': {'max': 0,
                'mean': 0.0,
                'min': 0,
                'relstddev': 0.0,
                'stddev': 0.0},
 'used_ram': {'max': 3657,
              'mean': 3657.0,
              'min': 3657,
              'relstddev': 0.0,
              'stddev': 0.0}}
------------------------------------------------------------------------------

[[generating-graphs]]
==== Generating Graphs

-----------------------
./stats.py -g flows ram
-----------------------

image:WCBench_graphed_results.png[ 800 px,title=" 800 px"]

[[restconf-performance-scale-tools-1]]
=== RESTCONF Performance & Scale Tools

This is collection of Python scripts that can be used to measure
controller's performance and scale. The performance tests in this suite
use ODL's RESTCONF API to add/delete flows into/from ODL's configuration
data store. Scripts in this test suite can be used for interactive
performance testing or re-used in other scripts. The test suite allows
users to experiment with different parameters that may impact
controller's performance.

The test suite contains the following scripts:

* *inventory_crawler.py*: Retrieves all openflow nodes from either the
config data store or the operational data store and prints out a summary
of flow and table statistics for each node in the network. Depending on
the print level specified in a command line option, the summary can
shows the overall number of flows in the network, the number of flows in
each node, or detailed data for each flow. The script is useful with
larger number of flows, where megabytes of data are returned by the
controller and the REST utilities such as curl becomes difficult to
interpret.

* *flow_config_blaster.py*: Adds and deletes ("blasts") flows into ODL's
config space. Command line options control the number of "blaster"
threads, the number of blast cycles, the number of flows blasted in each
cycle by each thread, etc. flow_config_blaster.py contains the
_FlowConfigBlaster_ class that is reusable in other tests, such as in
flow_add_delete_test.py

* *flow_config_blaster_fle.py*: "FlowConfigBlaster Floodlight Edition" -
the same as flow_config_blaster, but for the Floodlight controller.

* *config_cleanup.py*: cleans up the config data store by deleting the
entire inventory.

* *flow_add_delete_test.py*: adds/deletes ("blasts") flows into ODL's
config space. Similar to the flow_config_blaster (in fact,
FlowConfigBlaster is used in this test), but has more advanced handling
of the add/delete phases. The test executes in three steps:
** The specified number of flows is added in the 'add cycle' (uses
flow_config_blaster to blast flows)
** The network is polled for flow statistics from the network (using
inventory_crawler) to make sure that all flows have been properly
programmed into the network *and stats can properly read them
** The flows are deleted in the flow cycle (either in 'bulk' using the
config_cleanup script or one by one using the flow_config_blaster)

* *pretty_print.py*: a simple utility to format JSON output from the
controller into a human-readable form (the controller outputs everything
in a single line, so if you use curl and are getting anything beyond
trivial, reading the output is quite difficult).

* multi_blaster.sh: a shell utility that can start multiple
FlowConfigBlasters simultaneously. Even though FlowConfigBlaster is
multithreaded, because of the global interpreter lock it takes multiple
simultaneously running FlowConfigBlasters to fully utilize the
controller's flow processing capacity.

[[how-to-download]]
==== How to Download

The utilities are located in the integration project Git.

--------------------------------------------------------------------------------
git clone https://git.opendaylight.org/gerrit/p/integration.git
cd integration/test/tools/odl-mdsal-clustering-tests/clustering-performance-test
--------------------------------------------------------------------------------

[[prerequisites]]
==== Prerequisites

* Python 2.7
* Packages not commonly installed:
** http://docs.python-requests.org/en/latest/[requests]
** https://pypi.python.org/pypi/netaddr[netaddr]

To install the above libraries, type for example:

-------------------------
sudo pip install requests
sudo pip install netaddr
-------------------------

[[inventory-crawler]]
==== Inventory Crawler

To see the command line options, type:

----------------------------
> ./inventory_crawler --help
----------------------------

You'll see the following output:

------------------------------------------------------------------------------
usage: inventory_crawler.py [-h] [--odlhost ODLHOST] [--odlport ODLPORT]
                            [--plevel PLEVEL]
                            [--datastore {operational,config}] [--no-auth]
                            [--auth] [--debug]

optional arguments:
  -h, --help            show this help message and exit
  --lhost ODLHOST     host where odl controller is running (default is
                        127.0.0.1)
  --port ODLPORT     port on which odl's RESTCONF is listening (default is
                       8181)
  --plevel PLEVEL       Print Level: 0 - Summary (stats only); 1 - Node names;
                        2 - Node details;3 - Flow details
  --datastore {operational,config}
                        Which data store to crawl; default operational
  --no-auth             Do not use authenticated access to REST (default)
  --auth                Use authenticated access to REST (username: 'admin',
                        password: 'admin').
  --debug               List nodes that have not provided proper statistics
                        data
------------------------------------------------------------------------------

*Examples:*

* To show a summary of all flows present in all openflow switches in the
network, type:

------------------------------------------
> ./inventory_crawler.py --plevel=1 --auth
------------------------------------------

NOTE: REST authentication is turned by default in ODL Helium. To use
authenticated REST, you have to specify the '--auth' switch in the
command line.

* To show a summary of all flows programmed into the config data store,
type:

-------------------------------------------------------------
> ./inventory_crawler.py --plevel=1 --datastore=config --auth
-------------------------------------------------------------

[[flow-config-blaster]]
==== Flow Config Blaster

To see the command line options, type:

-----------------------------------
  > ./flow_config_blaster.py --help
-----------------------------------

You'll see the following output:

------------------------------------------------------------------------------
usage: flow_config_blaster.py [-h] [--host HOST] [--port PORT]
                              [--cycles CYCLES] [--threads THREADS]
                              [--flows FLOWS] [--nodes NODES] [--delay DELAY]
                              [--delete] [--no-delete] [--auth]
                              [--startflow STARTFLOW] [--file FILE]

Flow programming performance test: First adds and then deletes flows into the
config tree, as specified by optional parameters.

optional arguments:
  -h, --help            show this help message and exit
  --host HOST        Host where odl controller is running (default is
                        127.0.0.1)
  --port PORT        Port on which odl's RESTCONF is listening (default is
                        8181)
  --cycles CYCLES  Number of flow add/delete cycles; default 1. Both Flow
                        Adds and Flow Deletes are performed in cycles.
                        <THREADS> worker threads are started in each cycle and
                        the cycle ends when all threads finish. Another cycle
                        is started when the previous cycle finished.
  --threads THREADS     Number of request worker threads to start in each
                        cycle; default=1. Each thread will add/delete <FLOWS>
                        flows.
  --flows FLOWS         Number of flows that will be added/deleted by each
                        worker thread in each cycle; default 10
  --nodes NODES         Number of nodes if mininet is not connected;
                        default=16. If mininet is connected, flows will be
                        evenly distributed (programmed) into connected nodes.
  --delay DELAY         Time (in seconds) to wait between the add and delete
                        cycles; default=0
  --delete              Delete all added flows one by one, benchmark delete
                        performance.
  --no-delete           Do not perform the delete cycle.
  --auth                Use the ODL default username/password 'admin'/'admin'
                        to authenticate access to REST; default: no
                        authentication
  --startflow STARTFLOW
                        The starting Flow ID; default=0
  --file FILE           File from which to read the JSON flow template;
                        default: no file, use a built in template.
------------------------------------------------------------------------------

NOTE: The 'startflow' command line parameter is used with multiple
flow_config_blasters blasting flows at the same ODL instance. With
Python's GIL any given blaster can not use more than one CPU even when
multiple blaster threads are specified. Therefore, multiple blaster
processes must be used to test ODL's performance limits. The 'startflow'
parameter gives each blaster process its own flow id space so that each
injects unique flows into ODL's config data store.

NOTE: You don't have to be connected to mininet (or another openflow
network, for that matter) to use this script. If ODL is connected to an
openflow network, flow_config_blaster will evenly distribute flows
across the network.If ODL is not connected to a network, flows are only
stored in the config data store (i.e. nodes that may connect at some
point in the future are in effect "preconfigured"). The not-connected
mode can be used to test the performance of the data store and the REST
subsystems. The 'nodes' parameter determines the number of nodes that
are "pre-programmed" with flows in the non-connected mode.

*Examples:*

* To put 5000 flows into ODL running on the same node as the script
type:

-------------------------------------------------------------
   > ./flow_config_blaster.py --flows=5000 --auth --no-delete
-------------------------------------------------------------

NOTE: The flows will not be deleted, since the 'no-delete- option was
used. Use config_cleanup.py to deletete the flows.

* To use 5 threads to put 5000 flows into ODL running on the same node
as the script type:

-------------------------------------------------------------------------
   > ./flow_config_blaster.py --threads=5 --flows=1000 --auth --no-delete
-------------------------------------------------------------------------

NOTE: each thread will put 1000 flows, and all 5 threads will work
simultaneously.

* To first put and then delete 5000 flows into ODL running on the same
node as the script, type:

------------------------------------------------
  > ./flow_config_blaster.py --flows=5000 --auth
------------------------------------------------

* To use 5 threads to first put and then delete 5000 flows into ODL
running on the same node as the script type:

------------------------------------------------------------
  > ./flow_config_blaster.py --threads=5 --flows=1000 --auth
 
------------------------------------------------------------

NOTE: 5 threads are used to both add and delete flows

* To use 5 threads to first put and then delete 5000 flows into ODL in
10 add/delete cycles, type:

-----------------------------------------------------------------------
  > ./flow_config_blaster.py --threads=5 --flows=100 --cycles=10 --auth
-----------------------------------------------------------------------

`NOTE: 5 threads are used to both add and delete flows. `

`NOTE: Both Add and Delete are performed in the same number of cycles, (10 in this example). 5 worker threads are started in each cycle and the cycle ends when all threads finish.  Cycles are useful to determine performance degradation with increasing` +
`number of flows in the datastore and in the network. `

* To put and then delete 1000 flows with nicira match and action
extensions, type:

----------------------------------------------------------------------------
  >./flow_config_blaster.py --flows=1000 --auth --file=./nicira-ext-all.json
----------------------------------------------------------------------------

NOTE: json for flow adds will be taken from the file
'nicira-ext-all.json'

[[configuration-cleanup]]
==== Configuration Cleanup

----------------------------------------
* To see the command line options, type:
  > ./config_cleanup.py --help
----------------------------------------

--------------------------------------------------------------------------
usage: config_cleanup.py [-h] [--odlhost ODLHOST] [--odlport ODLPORT]
                         [--no-auth] [--auth]

Cleans up the config space

optional arguments:
  -h, --help         show this help message and exit
  --odlhost ODLHOST  host where odl controller is running (default is
                     127.0.0.1)
  --odlport ODLPORT  port on which odl's RESTCONF is listening (default is
                     8181)
  --no-auth          Do not use authenticated access to REST (default)
  --auth             Use authenticated access to REST (username: 'admin',
                     password: 'admin').
 
--------------------------------------------------------------------------

[[flow-adddelete-test]]
==== Flow Add/Delete Test

To see the command line options, type:

------------------------------------------------------------------------------
  >./flow_add_delete_test.py --help

usage: flow_add_delete_test.py [-h] [--host HOST] [--port PORT]
                               [--flows FLOWS] [--cycles CYCLES]
                               [--threads THREADS] [--nodes NODES]
                               [--delay DELAY] [--timeout TIMEOUT] [--delete]
                               [--bulk-delete] [--auth]
                               [--startflow STARTFLOW]

Flow programming performance test: First adds and then deletes flows into the
config tree, as specified by optional parameters.

optional arguments:
  -h, --help            show this help message and exit
  --host HOST           Host where odl controller is running (default is
                        127.0.0.1)
  --port PORT           Port on which odl's RESTCONF is listening (default is
                        8181)
  --cycles CYCLES       Number of flow add/delete cycles; default 1. Both Flow
                        Adds and Flow Deletes are performed in cycles.
                        <THREADS> worker threads are started in each cycle and
                        the cycle ends when all threads finish. Another cycle
                        is started when the previous cycle finished.
  --threads THREADS     Number of request worker threads to start in each
                        cycle; default=1. Each thread will add/delete <FLOWS>
                        flows.
  --flows FLOWS         Number of flows that will be added/deleted by each
                        worker thread in each cycle; default 10
  --nodes NODES         Number of nodes if mininet is not connected;
                        default=16. If mininet is connected, flows will be
                        evenly distributed (programmed) into connected nodes.
  --delay DELAY         Time (seconds) to between inventory polls when waiting
                        for stats to catch up; default=1
  --timeout TIMEOUT     The maximum time (seconds) to wait between the add and
                        delete cycles; default=100
  --delete              Delete all added flows one by one, benchmark delete
                        performance.
  --bulk-delete         Delete all flows in bulk; default=False
  --auth                Use authenticated access to REST (username: 'admin',
                        password: 'admin'); default=False
  --startflow STARTFLOW
                        The starting Flow ID; default=0
  --file FILE           File from which to read the JSON flow template;
                        default: no file, use a built in template.
------------------------------------------------------------------------------

*Examples:*

* To put 5000 flows into ODL, then wait for stats to catch up and then
delete the flows in bulk (using config_cleanup), type:

----------------------------------------------------------------------------
   > ./flow_add_delete_test.py --flows=5000 --auth --no-delete --bulk-delete
----------------------------------------------------------------------------

[[mininet-1]]
=== Mininet

Still in design, the idea will be to bring up small to large topologies
of 16-32-64-128-256-... switches and verify the controller can properly
learn topology: switches, ports and links. Also measure CPU and RAM
consumed on each iteration. We can also configure mininet to bring a
custom topology close to a data center.

Test Steps:

* 1. Download latest controller base distribution. See
CrossProject:Integration_Group:Controller_Artifacts[Controller Artifacts
for testing]
* 2. Set controller Log level to ERROR, edit
opendaylight/configuration/logback.xml
* 3. Start controller with recommended options: *run.sh -of13 -Xms1g
-Xmx4g*
* 4. Start mininet with tree topology of 15 switches: *sudo mn
--controller=remote,ip= --topo tree,4* or custom topology
* 5. Check RESTCONF inventory with:

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/restconf/operational/opendaylight-inventory:nodes(|grep "openflow:")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 6. Check AD-SAL topology through GUI or NB API:

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/topology/default (|grep "openflow:")
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Next steps 7&8 are designed to test AD-SAL ARP Handler and Simple
Forwarding apps

* 7. Do a ping test in mininet: *mininet> pingall*
* 8. Check hosts are learned and flows are created:

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/hosttracker/default/hosts/active
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/statistics/default/flow
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 9. Monitor CPU and RAM
* 10. Repeat the test increasing the number of switches, like -tree,5
(31 switches)

[[cbench-1]]
=== CBench

This test is still being designed, but the idea is to bring up varying
typologies of switches (16, 32, 64...) and benchmark how many flows/sec
the controller can handle, as well as the CPU and RAM usage of each
iteration.

Test Steps:

* 1. OpenDaylight depends on Java (and which, to set $JAVA_HOME). The
install process depends on unzip and wget.

---------------------------------------------------------
[~]$ sudo yum install java-1.7.0-openjdk which unzip wget
---------------------------------------------------------

* 2. Download the latest controller base distribution. See
CrossProject:Integration_Group:Controller_Artifacts for details.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[~]$ wget 'https://jenkins.opendaylight.org/integration/job/integration-master-project-centralized-integration/lastSuccessfulBuild/artifact/distributions/base/target/distributions-base-0.1.2-SNAPSHOT-osgipackage.zip'
[~]$ unzip distributions-base-0.1.2-SNAPSHOT-osgipackage.zip
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 3. Download the OpenFlow plugin reactive forwarding bundle and install
it by moving it to the _opendaylight/plugins_ directory.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[~/opendaylight/plugins]$ wget 'https://jenkins.opendaylight.org/openflowplugin/job/openflowplugin-merge/lastSuccessfulBuild/org.opendaylight.openflowplugin$drop-test/artifact/org.opendaylight.openflowplugin/drop-test/0.0.3-SNAPSHOT/drop-test-0.0.3-SNAPSHOT.jar'
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 4. The AD-SAL simple forwarding and ARP handler bundles apparently
interfere with MD-SAL CBench measurements. Delete them:

----------------------------------------------------------------------------------------------------
[~/opendaylight/plugins]$ rm org.opendaylight.controller.samples.simpleforwarding-0.4.2-SNAPSHOT.jar
[~/opendaylight/plugins]$ rm org.opendaylight.controller.arphandler-0.5.2-SNAPSHOT.jar
----------------------------------------------------------------------------------------------------

* 5. Edit the _opendaylight/configuration/logback.xml_ file to set the
controller log level to ERROR:

------------------------------------------------------------
  <!-- Controller log level -->
  <logger name="org.opendaylight.controller" level="ERROR"/>
------------------------------------------------------------

* 6. Start the controller. It's recommended that you use the following
options:

----------------------------------------------
[~/opendaylight]$ ./run.sh -of13 -Xms1g -Xmx4g
----------------------------------------------

* 7. Initiate the drop test in the controller. There are two types of
drop tests: RPC (programming flows directly through an RPC to the OF
Plugin) and data store (programming flows by writing them into the
MD-SAL config space, from where they are picked up by the FRM and
programmed into the plugin). For the data store performance, the latter
is of interest.

Turn on the RPC drop test. From the controller’s OSGi console, which
your shell drops into after starting the controller:

------------------------------
osgi> dropAllPacketsRpc on
DropAllFlows transitions to on
------------------------------

Turn on the data store drop test. From the controller’s OSGi console,
which your shell drops into after starting the controller:

--------------------------
osgi> dropAllPackets on
DropAllFlows is already on
--------------------------

* 8. If you haven't already, install CBench as described in
CrossProject:Integration_Group:Performance_Test#CBench_Setup.

* 9. You can now start a CBench test (from a different shell, not the
OSGi console). Replace _localhost_ with the IP of the machine running
the controller, if it's not your local machine. Note that the first run
seems to give non-representative results, but future runs are
consistent.

-------------------------------------------------------------------------------------------------------------------------------------------------
[~]$ cbench -c localhost -p 6633 -m 1000 -l 10 -s 16 -M 100000
cbench: controller benchmarking tool
   running in mode 'latency'
   connecting to controller at localhost:6633 
   faking 16 switches :: 10 tests each; 1000 ms per test
   with 100000 unique source MACs per switch
   learning destination mac addresses before the test
   starting test with 0 ms delay after features_reply
   ignoring first 1 "warmup" and last 0 "cooldown" loops
   connection delay of 0ms per 1 switch(es)
   debugging info is off
17:45:11.159 16  switches: fmods/sec:  259  367  138  226  220  294  475  150  126  250  132  113  224  135  158  126   total = 3.392956 per ms 
17:45:12.259 16  switches: fmods/sec:  393  421  349  420  446  360  427  425  410  413  342  445  472  347  482  405   total = 6.556993 per ms 
17:45:13.360 16  switches: fmods/sec:  647  549  666  538  566  701  612  599  555  600  648  464  569  656  549  619   total = 9.537990 per ms 
17:45:14.460 16  switches: fmods/sec:  774  787  834  756  797  834  542  757  760  791  812  798  829  823  535  699   total = 12.127988 per ms 
17:45:15.560 16  switches: fmods/sec:  776  793  773  743  738  706  816  736  800  804  772  804  762  721  780  744   total = 12.267975 per ms 
17:45:16.661 16  switches: fmods/sec:  864  935  725  905  868  716  931  779  855  954  715  907  841  706  927  780   total = 13.407987 per ms 
17:45:17.761 16  switches: fmods/sec:  771  805  811  931  890  891  782  825  785  781  787  900  847  906  771  840   total = 13.322973 per ms 
17:45:18.861 16  switches: fmods/sec:  808  794  778  873  796  846  816  855  854  795  795  883  770  857  839  846   total = 13.204868 per ms 
17:45:19.961 16  switches: fmods/sec:  939  876  871  805  790  789  815  825  952  875  872  798  817  828  809  805   total = 13.465987 per ms 
17:45:21.062 16  switches: fmods/sec:  829  917  944  888  843  889  783  783  819  892  918  878  821  900  787  782   total = 13.672795 per ms 
RESULT: 16 switches 9 tests min/max/avg/stdev = 6556.99/13672.79/11951.73/2257.99 responses/s
-------------------------------------------------------------------------------------------------------------------------------------------------

* 10. Monitor CPU and RAM usage.

* 11. Repeat the test, increasing the number of switches (-s 32) and
MACs/switch (-M 200000).

[[known-issues]]
==== Known Issues

* Cbench Throughput test does not work well, CPU goes very high and
cannot process incoming packet-in messages in an stable way. It looks
like CPU is being used by Netty to flush some buffers, OF plugin devs
are working on fixing this.
* The data store drop test *dropAllPackets on* does not work well
either, flows get hung after a while. FRM seems to be the bottleneck, OF
plugin devs are looking at this was well.
* Can't support more than 128 switches
https://bugs.opendaylight.org/show_bug.cgi?id=1581[Bug 1581]

[[results]]
== Results

[[test-bed-setup]]
=== Test Bed Setup

Device under Test (DUT)

* CPU: Dual Intel(R) Xeon(R) CPU E5-2660 (32 Cores)
* Memory: 8*8GB DDR3 - 1600 Mhz
* Harddrive:
* OS: CentOS 7.0
* Java: OpenJDK java version "1.7.0_65"
* Default ODL memory settings

North/Southbound simulator (SIM)

* CPU: Dual Intel(R) Xeon(R) CPU E5-2660 (32 Cores)
* Memory: 4*8GB DDR3 - 1333 Mhz
* Harddrive:
* OS: CentOS 7.0

[[helium-cbench-results]]
=== Helium CBench Results

.RPC Flow per second - RPC drop-test
CrossProject:Integration_Group:Performance_Test#note1[[1]]
[cols=",,,,,,,,,,,",]
|=======================================================================
||Switches || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 16 || 32 || 64

||Throughput || 29396.66 || 76041.18 || 93280.33 || 104213.88 ||
100489.83 || 101304.47 || 106000.88 || 104909.61 || 89412.92 || 52890.66
|| 29789.51

||latency || 2305 || 5545.5 || 8639 || 11998.99 || 14810.49 || 16793.49
|| 19108.99 || 21534.49 || 43400.98 || 62299.94 || 69135.55
|=======================================================================

image:Rpc-flow.png[ 800 px,title=" 800 px"]

.Flow per second - Datastore drop-test
[cols=",,,,,,,,,,,",]
|=======================================================================
||Switches || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 16 || 32 || 64

||Throughput || 42.48 || 19 || 14 || 11.5 || 10 || 9 || 8 || 7.5 || 6.5
|| 3.5 || 2

||latency || 7 || 7 || 6.5 || 6.5 || 6 || 6 || 5 || 4.5 || 3.5 || 3.5 ||
4
|=======================================================================

[1] limiting to 16 core show the same results.

Category:Integration Group[Category:Integration Group]
