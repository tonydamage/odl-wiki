[[introduction]]
== Introduction

Performance tests are needed not only for benchmarking our controller
but also for improving our code by continuosly monitoring the collected
data. We also stated that for next release *Helium* our focus will be
performance and stability and as a result we have decided to create and
maintain this wiki.

[[scope]]
== Scope

The goal of this wiki is to:

* Provide a set of recommendations for doing performance test
* Describe which performance tests are currently running in OpenDaylight
* Publish performance test results

[[performance-metrics]]
== Performance metrics

We agreed we are going to start with following performance metrics:

* Inventory Scalability:
** Max number of OF switches and ports
** Time/CPU/RAM consumed in learning switches, ports and topology

* Flow performance:
** Max number of flows in memory
** Max flows/sec we can process
** Time/CPU/RAM consumed in pushing flows

[[controller-baseline]]
== Controller Baseline

In order to run an homogeneous test, we agreed on using this
configuration for all performance test:

* HW/SW configuration:
** Use dedicated VM or bare-metal for controller
** CPU: 4 Cores
** RAM: 6 GB
** OS: Linux Server (no Desktop) is recommended. Report used OS when
publishing results

* Controller configuration:
** Use controller Base edition. See
CrossProject:Integration_Group:Controller_Artifacts[Controller Artifacts
for testing]
** ODL Log level set to ERROR, edit
opendaylight/configuration/logback.xml
** Optional: To enable new datastore edit file
opendaylight/config/initial/01-md-sal.xml and follow instructions in the
file comments (comment everything within the DATA-BROKER comments and
uncomment everything within the NEW-DATA-BROKER comments)
** Enable OF plugin with -of13 option: *run.sh -of13*
** Set Java memory: Min=4G, Max=4G, in run.sh we have to add following
JVM options:

--------------------------------------------------------------------------------------------------------------------------------
run.sh -Xmx4G -XX:+UseG1GC -XX:MaxPermSize=512m -Xms4G    

# Minimum heap size 4G and maximum heap size 4G. When min and max are the same, there will be no dynamic heap allocation. 
  When increasing to higher values, increase both at same time for better performance 
# Use G1GC garbage collection strategy. 
  Getting Started with the G1GC Garbage Collector: 
    http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html 
  Garbage First Garbage Collector (G1 GC) - Migration to, Expectations and Advanced Tuning: 
    http://www.slideshare.net/MonicaBeckwith/garbage-first-garbage-collector-g1-gc-migration-to-expectations-and-advanced-tuning



# OutOfMemoryError crashes when MACs and Switches count are increased in CBench cmdline args
--------------------------------------------------------------------------------------------------------------------------------

[[performance-tools]]
== Performance tools

* *Mininet*

Available at http://mininet.org[mininet.org] and also installed in the
CrossProject:Integration_Group:Test_VMs[Test VMs]. Mininet simulates a
topology of openflow switches connected each other and also hosts
attached to them. This tool can be used for inventory performance by
increasing the number of switches and ports in the topology. It also
allows custom configurations so that we can create topologies close to
real scenarios (i.e. data centers)

* *Cbench*

Available at
http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New)[Openflowhub]
and also installed in the CrossProject:Integration_Group:Test_VMs[Test
VMs]. Cbench emulates a bunch of openflow switches which connect to a
controller, send packet-in messages, and watch for flow-mods to get
pushed down. This tool can be used for flow performance

* *Enhanced Cbench*

We can use Cbench with Performance test scripts created by Michal Rehak
to generate a 2D/3D plot with Gnuplot. Check instructions in
CrossProject:Integration_Group:Performance_Test_Enhanced_Cbench[Enhanced
CBench Test]

* *Jconsole*

With Jconsole it is possible to monitor CPU and RAM for a Java process.
Instructions on how to start the Jconsole are available
Media:Jconsole.pdf[here].

* *Yourkit*

This tool can be used to monitor CPU and RAM per Java bundle, problem is
that it is not free.

* *Jenkins Monitoring plugin*

Jenkins can also monitor CPU and RAM on slave nodes (like controller VM)
through
https://wiki.jenkins-ci.org/display/JENKINS/Monitoring[Monitoring
plugin]

[[cbench-setup]]
=== CBench Setup

The https://github.com/andi-bigswitch/oflops[OFLOPS/CBench READMEs] are
quite short on details.
http://www.openflowhub.org/display/floodlightcontroller/Cbench+(New)[This
page] is a fairly useful resource, but the steps below are more succinct
and give additional explanation.

According to the OFLOPS README, _net-snmp-devel_ and _libpcap-devel_ are
required dependencies. Starting from a clean Fedora 20 install, you'll
also need _autoconf_, _automake_, _libtool_ and _libconfig-devel_ to get
CBench built. Git is required for the install process:

-----------------------------------------------------------------------------------------------------
[~]$ sudo yum install net-snmp-devel libpcap-devel autoconf make automake libtool libconfig-devel git
-----------------------------------------------------------------------------------------------------

CBench lives in the OFLOPS repo, so you'll need to clone that repo to
your local system. The _--recursive_ flag pulls down the project's
submodule in the same step (submodule seems to be optional):

-----------------------------------------------------------------------
[~]$ git clone --recursive https://github.com/andi-bigswitch/oflops.git
-----------------------------------------------------------------------

Go ahead and build the submodule (seems to be optional):

-----------------------------------------------------------
[~/oflops/netfpga-packet-generator-c-library]$ ./autogen.sh
[~/oflops/netfpga-packet-generator-c-library]$ ./configure 
[~/oflops/netfpga-packet-generator-c-library]$ make
-----------------------------------------------------------

You'll need the OpenFlow source code, as CBench must be pointed at it
during the build (this is 1.0, 1.3 currently breaks the CBench build):

------------------------------------------------------
[~]$ git clone git://gitosis.stanford.edu/openflow.git
------------------------------------------------------

Use the _oflops/boot.sh_ script to build an _oflops/configure_ file:

--------------------------------------------------------------------------------
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File doesn't exist
[~/oflops]$ ./boot.sh
<snip>
[~/oflops]$ test -e configure && echo "File exists" || echo "File doesn't exist"
File exists
--------------------------------------------------------------------------------

You can now build OFLOPS:

---------------------------------------------------------------------------------
[~/oflops]$ ./configure --with-openflow-src-dir=/absolute/path/to/openflow/source
[~/oflops]$ make
[~/oflops]$ sudo make install
[~/oflops]$ whereis cbench
/usr/local/bin/cbench
---------------------------------------------------------------------------------

[[test-cases]]
== Test Cases

[[mininet-test]]
=== Mininet Test

Still in design, the idea will be to bring up small to large topologies
of 16-32-64-128-256-... switches and verify the controller can properly
learn topology: switches, ports and links. Also measure CPU and RAM
consumed on each iteration. We can also configure mininet to bring a
custom topology close to a data center.

Test Steps:

* 1. Download latest controller base distribution. See
CrossProject:Integration_Group:Controller_Artifacts[Controller Artifacts
for testing]
* 2. Set controller Log level to ERROR, edit
opendaylight/configuration/logback.xml
* 3. Start controller with recommended options: *run.sh -of13 -Xms1g
-Xmx4g*
* 4. Start mininet with tree topology of 15 switches: *sudo mn
--controller=remote,ip= --topo tree,4* or custom topology
* 5. Check RESTCONF inventory with:

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/restconf/operational/opendaylight-inventory:nodes(|grep "openflow:")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 6. Check AD-SAL topology through GUI or NB API:

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/topology/default (|grep "openflow:")
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Next steps 7&8 are designed to test AD-SAL ARP Handler and Simple
Forwarding apps

* 7. Do a ping test in mininet: *mininet> pingall*
* 8. Check hosts are learned and flows are created:

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/hosttracker/default/hosts/active
curl --user "admin":"admin" -H "Accept: application/json" -H "Content-type: application/json" -X GET http://<controllerIP>:8080/controller/nb/v2/statistics/default/flow
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 9. Monitor CPU and RAM
* 10. Repeat the test increasing the number of switches, like -tree,5
(31 switches)

[[cbench-test]]
=== CBench Test

This test is still being designed, but the idea is to bring up varying
typologies of switches (16, 32, 64...) and benchmark how many flows/sec
the controller can handle, as well as the CPU and RAM usage of each
iteration.

Test Steps:

* 1. OpenDaylight depends on Java (and which, to set $JAVA_HOME). The
install process depends on unzip and wget.

---------------------------------------------------------
[~]$ sudo yum install java-1.7.0-openjdk which unzip wget
---------------------------------------------------------

* 2. Download the latest controller base distribution. See
CrossProject:Integration_Group:Controller_Artifacts for details.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[~]$ wget 'https://jenkins.opendaylight.org/integration/job/integration-master-project-centralized-integration/lastSuccessfulBuild/artifact/distributions/base/target/distributions-base-0.1.2-SNAPSHOT-osgipackage.zip'
[~]$ unzip distributions-base-0.1.2-SNAPSHOT-osgipackage.zip
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 3. Download the OpenFlow plugin reactive forwarding bundle and install
it by moving it to the _opendaylight/plugins_ directory.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[~/opendaylight/plugins]$ wget 'https://jenkins.opendaylight.org/openflowplugin/job/openflowplugin-merge/lastSuccessfulBuild/org.opendaylight.openflowplugin$drop-test/artifact/org.opendaylight.openflowplugin/drop-test/0.0.3-SNAPSHOT/drop-test-0.0.3-SNAPSHOT.jar'
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

* 4. The AD-SAL simple forwarding and ARP handler bundles apparently
interfere with MD-SAL CBench measurements. Delete them:

----------------------------------------------------------------------------------------------------
[~/opendaylight/plugins]$ rm org.opendaylight.controller.samples.simpleforwarding-0.4.2-SNAPSHOT.jar
[~/opendaylight/plugins]$ rm org.opendaylight.controller.arphandler-0.5.2-SNAPSHOT.jar
----------------------------------------------------------------------------------------------------

* 5. Edit the _opendaylight/configuration/logback.xml_ file to set the
controller log level to ERROR:

------------------------------------------------------------
  <!-- Controller log level -->
  <logger name="org.opendaylight.controller" level="ERROR"/>
------------------------------------------------------------

* 6. Start the controller. It's recommended that you use the following
options:

----------------------------------------------
[~/opendaylight]$ ./run.sh -of13 -Xms1g -Xmx4g
----------------------------------------------

* 7. Initiate the drop test in the controller. There are two types of
drop tests: RPC (programming flows directly through an RPC to the OF
Plugin) and data store (programming flows by writing them into the
MD-SAL config space, from where they are picked up by the FRM and
programmed into the plugin). For the data store performance, the latter
is of interest.

Turn on the RPC drop test. From the controller’s OSGi console, which
your shell drops into after starting the controller:

------------------------------
osgi> dropAllPacketsRpc on
DropAllFlows transitions to on
------------------------------

Turn on the data store drop test. From the controller’s OSGi console,
which your shell drops into after starting the controller:

--------------------------
osgi> dropAllPackets on
DropAllFlows is already on
--------------------------

* 8. If you haven't already, install CBench as described in
CrossProject:Integration_Group:Performance_Test#CBench_Setup.

* 9. You can now start a CBench test (from a different shell, not the
OSGi console). Replace _localhost_ with the IP of the machine running
the controller, if it's not your local machine. Note that the first run
seems to give non-representative results, but future runs are
consistent.

-------------------------------------------------------------------------------------------------------------------------------------------------
[~]$ cbench -c localhost -p 6633 -m 1000 -l 10 -s 16 -M 100000
cbench: controller benchmarking tool
   running in mode 'latency'
   connecting to controller at localhost:6633 
   faking 16 switches :: 10 tests each; 1000 ms per test
   with 100000 unique source MACs per switch
   learning destination mac addresses before the test
   starting test with 0 ms delay after features_reply
   ignoring first 1 "warmup" and last 0 "cooldown" loops
   connection delay of 0ms per 1 switch(es)
   debugging info is off
17:45:11.159 16  switches: fmods/sec:  259  367  138  226  220  294  475  150  126  250  132  113  224  135  158  126   total = 3.392956 per ms 
17:45:12.259 16  switches: fmods/sec:  393  421  349  420  446  360  427  425  410  413  342  445  472  347  482  405   total = 6.556993 per ms 
17:45:13.360 16  switches: fmods/sec:  647  549  666  538  566  701  612  599  555  600  648  464  569  656  549  619   total = 9.537990 per ms 
17:45:14.460 16  switches: fmods/sec:  774  787  834  756  797  834  542  757  760  791  812  798  829  823  535  699   total = 12.127988 per ms 
17:45:15.560 16  switches: fmods/sec:  776  793  773  743  738  706  816  736  800  804  772  804  762  721  780  744   total = 12.267975 per ms 
17:45:16.661 16  switches: fmods/sec:  864  935  725  905  868  716  931  779  855  954  715  907  841  706  927  780   total = 13.407987 per ms 
17:45:17.761 16  switches: fmods/sec:  771  805  811  931  890  891  782  825  785  781  787  900  847  906  771  840   total = 13.322973 per ms 
17:45:18.861 16  switches: fmods/sec:  808  794  778  873  796  846  816  855  854  795  795  883  770  857  839  846   total = 13.204868 per ms 
17:45:19.961 16  switches: fmods/sec:  939  876  871  805  790  789  815  825  952  875  872  798  817  828  809  805   total = 13.465987 per ms 
17:45:21.062 16  switches: fmods/sec:  829  917  944  888  843  889  783  783  819  892  918  878  821  900  787  782   total = 13.672795 per ms 
RESULT: 16 switches 9 tests min/max/avg/stdev = 6556.99/13672.79/11951.73/2257.99 responses/s
-------------------------------------------------------------------------------------------------------------------------------------------------

* 10. Monitor CPU and RAM usage.

* 11. Repeat the test, increasing the number of switches (-s 32) and
MACs/switch (-M 200000).

[[known-issues]]
=== Known Issues

* Cbench Throughput test does not work well, CPU goes very high and
cannot process incoming packet-in messages in an stable way. It looks
like CPU is being used by Netty to flush some buffers, OF plugin devs
are working on fixing this.
* The data store drop test *dropAllPackets on* does not work well
either, flows get hung after a while. FRM seems to be the bottleneck, OF
plugin devs are looking at this was well.

Category:Integration Group[Category:Integration Group]
