[[opendaylight-clustering]]
== OpenDaylight Clustering

The OpenDaylight controller supports a cluster-based high availability
model. There are several instances of the OpenDaylight controller, which
logically act as one logical controller.

[[prerequisites]]
== Prerequisites

* Read documentation
https://wiki.opendaylight.org/view/OpenDaylight_Controller:Clustering:HowTo[here]
before starting the cluster.
* Start a cluster of Opendaylight controllers with minimum 2
controllers:
** In the first controller machine do   *./run.sh
-Dsupernodes=Controller1_IP:Controller2_IP -start*   with administrator
priviledges
*** This will start Controller1 in a cluster as supernode
** In the second controller machine do   *./run.sh
-Dsupernodes=Controller1_IP:Controller2_IP -start*   with administrator
priviledges
*** This will start Controller2 in a cluster as supernode
* Start Mininet (i.e. *sudo mn --controller=remote,ip=Controller1_IP
--topo tree,2*)
** This generates 3 nodes: s1 (*:01), s2 (*:02), s3 (*:03)
** At this point only the Controller1 get provisioned in OVS bridges
(s1, s2, s3)
** s1 has 2 ports: 1,2 that connects to s2 and s3
** s2 has 3 ports: 1 to host1 (10.0.0.1), 2 to host2 (10.0.0.2) and 3 to
s1
** s3 has 3 ports: 1 to host3 (10.0.0.3), 2 to host4 (10.0.0.4) and 3 to
s1
* Now, puase the Mininet (i.e. _mininet> CTRL + Z_)
* In order to get both controllers provisioned in all OVS bridges, do
the following commands with administrator priviledges:
** _$ ovs-vsctl set-controller s1 tcp:Controller1_IP:6633
tcp:Controller2_IP:6633_
** _$ ovs-vsctl set-controller s2 tcp:Controller1_IP:6633
tcp:Controller2_IP:6633_
** _$ ovs-vsctl set-controller s3 tcp:Controller1_IP:6633
tcp:Controller2_IP:6633_

[[notes]]
== Notes

* All the following test cases are written in Gherkin style
(http://robotframework.googlecode.com/hg/doc/userguide/RobotFrameworkUserGuide.html?r=2.8.3#different-test-case-styles[behavior-driven]),
and when possible the REST requests are provided.
* Note that OpenDaylight controller needs a couple of minutes to get all
of its modules loaded. Therefore, any test that comes after restarting
the controller, has to be done after a couple of minutes delay.
* By adding the proper flows in the the flow tables of the bridges, a
path will be established between hosts, and then the ping will be
successfull in Mininet (i.e. mininet> _' h1 ping h2_').
* For example the following commands will forward every packet arriving
at port 1 to port 2 and vice-versa:
** *sudo dpctl add-flow tcp:Controller_IP:6634
in_port=1,actions=output:2*
** *sudo dpctl add-flow tcp:Controller_IP:6634
in_port=2,actions=output:1*

[[test-content]]
== Test Content

[[cluster-manager]]
=== Cluster Manager

[[two-controllers-running]]
===== Two controllers running

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    When C1 is up and running
    +
    And C2 is up and running
    +
    Then the system is working with C1 and C2

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* See the topology using Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/topology/default) and check
that it shows the topology
* See the topology using Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/topology/default) and check
it that shows the same topology

::
  ;;
    (The following is an alternative test)

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* List the active hosts on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four hosts are listed
* List the active hosts on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four hosts are listed

[[controller1-fails]]
===== Controller1 fails

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    When C1 goes down
    +
    Then C2 takes over
    +
    And the system is working with C2

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. *./run.sh -stop*)
* See the topology using Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/topology/default) and check
that the request fails
* See the topology using Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/topology/default) and check
that it shows the topology

::
  ;;
    (The following is an alternative test)

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* List the active hosts on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that the request fails
* List the active hosts on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four host are still listed and Controller2 takes over

[[controller2-fails]]
===== Controller2 fails

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    When C2 goes down
    +
    Then C1 takes over
    +
    And the system is working with C1

* Similar to the case when Controller1 fails.

[[controller1-recovers-after-failure]]
===== Controller1 recovers after failure

::
  ;;
    Given C1 goses down
    +
    And C2 takes over
    +
    When C1 recovers
    +
    Then the system is working with C1 and C2

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. *./run.sh -stop*)
* See the topology using Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/topology/default) and check
that the request fails
* See the topology using Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/topology/default) and check
that it shows the topology
* Restart Controller1 (i.e. *./run.sh -start*)
* See the topology using Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/topology/default) and check
that it shows the topology
* See the topology using Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/topology/default) and check
it that shows the same topology

::
  ;;
    (The following is an alternative test)

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. *./run.sh -stop*)
* List the active hosts on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that the request fails
* List the active hosts on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four hosts are listed and Controller2 takes over
* Restart Controller1 (i.e. *./run.sh -start*)
* List the active hosts on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four hosts are listed
* List the active hosts on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that all four hosts are listed

[[controller1-and-controller2-fail]]
===== Controller1 and Controller2 fail

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    When C1 goes down
    +
    And C2 goes down
    +
    Then the system does not work any more

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Stop Controller1 (i.e. *./run.sh -stop*)
* See the topology using Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/topology/default) and check
that the request fails
* Stop Controller2 (i.e. *./run.sh -stop*)
* See the topology using Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/topology/default) and check
that the request fails

::
  ;;
    (The following is an alternative test)

* List the active hosts on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that the request fails
* List the active hosts on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/hosttracker/default/hosts/active)
and check that the request fails

'''''

[[forwarding-rules-manager-in-a-cluster]]
=== Forwarding Rules Manager in a Cluster

[[the-installed-flow-can-be-seen-in-a-cluster-of-two-controllers]]
===== The installed flow can be seen in a cluster of two controllers

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    When a flow is installed in a bridge
    +
    Then C1 see the flow
    +
    And C2 see the flow

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Using Controller1 install flow1 on a bridge (i.e. PUT
http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1
+ CrossProject:Integration_Group:REST_Body#Flow1_Body[flow1 body]) and
check the response is success
* See flow1 on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and
check flow is present
* See flow1 on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and
check the same flow is present

[[the-installed-flow-remains-in-the-bridge-after-the-controller-failure]]
===== The installed flow remains in the bridge after the controller
failure

::
  ;;
    Given C1 a controller in cluster of two controllers
    +
    And C2 a controller in cluster of two controllers
    +
    And both controllers get provisioned on all OVS bridges
    +
    And a flow is installed in a bridge
    +
    And C1 see the flow
    +
    And C2 see the flow
    +
    And C1 goes down
    +
    When C1 recovers
    +
    Then C1 see the flow

* Follow the instruction in the *Prerequisites* section to have a
cluster of two controllers, create a network, and then make sure that
both controllers get provisioned on all OVS bridges.
* Using Controller1 install flow1 on a bridge (i.e. PUT
http://Controller1_IP:8080/controller/nb/v2/flowprogrammer/default/node/OF/00:00:00:00:00:00:00:02/staticFlow/flow1
+ CrossProject:Integration_Group:REST_Body#Flow1_Body[flow1 body]) and
check the response is success
* See flow1 on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and
check flow is present
* See flow1 on Controller2 (i.e. GET
http://Controller2_IP:8080/controller/nb/v2/statistics/default/flow) and
check the same flow is present
* Stop Controller1 (i.e. *sudo ./run.sh -stop*)
* See flow1 on Controller1 (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and
check that the request fails
* Restart Controller1 (i.e. *sudo ./run.sh -start*)
* See flow1 on Controller1 after failure (i.e. GET
http://Controller1_IP:8080/controller/nb/v2/statistics/default/flow) and
check the same flow is still present

