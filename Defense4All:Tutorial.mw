=[[Defense4All Tutorial:Introduction|Introduction]] =
A well-known DoS attack mitigation and threats detection strategy is to divert suspected traffic from its normal network path to dedicated attack mitigation infrastructures, for cleaning and threat detection. These infrastructures are also known as “Security Centers” or “Scrubbing Centers”, mainly built of L3-L7 DoS attack mitigations devices. These “Security Centers” can be deployed in dedicated remote sites within the network in “out of path” manner (i.e. not “inline” with the native traffic flow), so diversion of traffic toward these centers is essential. During the traffic cleaning process, the attack mitigation infrastructure identifies and drops malicious IP packets and forwards legitimate IP packets back to their original targeted network destinations. The “Scrubbing Centers” can be located within the enterprise cooperate network, datacenter, cloud and also as part of carrier’s infrastructure.   
In general protection against DDoS attacks in Out-Off-Path (OOP) systems comprises three main elements: 
# Collection of traffic statistics and learning of statistics behavior of protected objects during peace-time. From this collected statistics, the normal traffic baselines of the protected objects are built.  
# Detection of DoS attack patterns as traffic anomalies deviating from normal baselines.  
# Diversion of suspicious traffic from its normal path to mitigation (scrubbing) centers for traffic cleansing, selective source blockage, etc.  “Clean” traffic out of scrubbing centers is re-injected back to packet’s original destination. 
Defense4All is a security SDN application for detecting and driving mitigation of DoS/DDoS attacks in different SDN topologies. It realizes anti-DoS in Out Of Path mode for OpenDaylight SDN environment. Administrators can configure Defense4All to protect certain networks/servers (henceforth, protected networks or PNs). 

Defense4All exploits SDN capabilities to count specified traffic, and installs traffic counting flows for each protocol of each configured PN in every network location through which traffic of the subject PN flows. Defense4All then monitors traffic of all configured PNs, summarizing readings, rates, and averages from all relevant network locations. In case it detects a deviation from normal learned traffic behavior in some protocol (TCP, UDP, ICMP, or rest of the traffic) of some PN, Defense4All declares an attack against that protocol in the subject PN. 

To mitigate a detected attack Defense4All performs several steps: 1) It selects one or more mitigation devices to mitigate the attack. 2) It configures SDN to divert attacked traffic through the mitigation devices. 3) It continues monitoring attacked traffic both from SDN and (optionally) from mitigation devices. When Defense4All receives no indications about the attack it cancels attacked traffic diversions and returns to peace-time monitoring.

Administrators need to notify Defense4All about relevant SDN controller, switches/routers, and mitigation devices of choice. Defense4All users can retrieve all present and past information about PN learned traffic normal, attacks and mitigations, health and status of mitigation devices and links, SDN configurations, and other operational information. Past information is stored in persistent flight recorder storage.

In this version Defense4All runs as a single instance (non-clustered), but it integrates three main fault tolerance features - 1) it runs as a Linux service that is automatically restarted should it fail, 2) its state is entirely persisted in stable storage, and upon restart Defense4All obtains the latest state, and 3) it carries a health tracker with restart and (several degrees of) reset capabilities to overcome certain logical and aging bugs.

Defense4All is designed for pluggability, allowing integration of different types of mitigation devices [Defense4All contains a reference implementation of a driver to Radware’s mitigation device – DefensePro. To attach some other mitigation device one needs to integrate driver to that device into Defense4All.], different SDN and non-SDN based attack detectors, different mitigation drivers, and different abstraction levels of network controller functionality for collecting traffic statistics and for traffic diversion. Finally, Defense4All itself resides in a framework, and can be replaced by other SDN applications [SDN and mitigation device drivers should be repackaged into the framework rather than Defense4All].

The diagram below describes the possible state of any given PN. Radware DefensePro, abbreviated as "DP" is an example of an incorporated AMS.
[[File:pn_possible_states.jpg|none|900px|PN possible states]]

Back to [[Defense4All:User_Guide|Defense4All User Guide Page]]

=[[Tutorial:Deployment alternatives|Deployment alternatives]] =
Defense4All supports “short diversion”, in which the AMS (Attack mitigation system) is connected to the edge router, so one hop traffic redirection is achieved. See also PE1 and DefensePro 2 in Figure 3. “Long diversion”, in which AMS scrubbing centers are located in arbitrary remote locations in the network, may be added in future Defense4All versions. See also PE2 and DefensePro 1 in Figure below.  
Defense4All supports both automatic and manual diversion modes. The manual mode includes user-based confirmation before diversion.
[[File:redirection_alternatives.jpg|none|900px|Defense4All Deployment traffic redirection alternatives]]

=[[Tutorial:Defense4All in ODL Environment|Defense4All in ODL Environment]] =
Defense4All is an SDN application for detecting and mitigating DDoS attacks. The application communicates with OpenDaylight Controller via the ODC north-bound REST API. Through this API Defense4All performs two main tasks:
# Monitoring behavior of protected traffic - the application sets flow entries in selected network locations to read traffic statistics for each of the PNs (aggregating statistics collected for a given PN from multiple locations).
# Diverting attacked traffic to selected AMSs – the application set flow entries in selected network locations to divert traffic to selected AMSs. When an attack is over the application removes these flow entries, thus returning to normal operation and traffic monitoring.
Defense4All can optionally communicate with the defined AMSs – e.g., to dynamically configure them, monitor them or collect and act upon attack statistics from the AMSs. The API to AMS is not standardized, and in any case beyond the scope of the OpenDaylight work. Defense4All contains a reference implementation pluggable driver to communicate with Radware’s DefensePro AMS.
The application presents its north-bound REST and CLI APIs to allow its manager to:
# Control and configure the application (runtime parameters, ODC connectivity, AMSs in domain, PNs, etc.).
# Obtain reporting data – operational or security, current or historical, unified from Defense4All and other sources (like ODC, AMSs).
[[File:D4A_in_odl.jpg|none|600px|Defense4All logical positioning in OpenDaylight environment]]
Defense4All comprises an SDN applications Framework and the Defense4All application itself – packaged as a single entity. Application integration into the Framework is pluggable, so any other SDN application can benefit from the common Framework services. The main advantages of this architecture are:
# Faster application development and changes - the Framework contains common code for multiple applications, complex elements (e.g., clustering or repository services) are implemented once for the benefit of any application.
# Faster, flexible deployment in different environments, form-factors, satisfying different NFRs – the Framework masks from SDN applications factors such as required data survivability, scale and elasticity, availability, security.
# Enhanced robustness - complex Framework code is implemented and tested once, cleaner separation of concerns leads to more stable code, Framework can increase robustness proactively with no additional code in application logic (e.g., periodic application recycle).
# Unified management of common aspects – common look and feel.
=[[Tutorial:Framework View|Framework View]] =
[[File:framework_view.jpg|none|900px|Defense4All Structure from Framework point of view]]
The Framework contains the following elements:

'''FrameworkMain''' – the Framework root point contains references to all Framework modules and global repositories, as well as the roots of deployed SDN applications (in current version the Framework can accommodate only one). This is also the point to start, stop or reset the Framework (along with its hosted application)
WebServer – Jetty web server running the Jersey RESTful Web Services framework, with Jackson parser for JSON encoded parameters. The REST Web Server runs a servlet for Framework and another servlet for each deployed application (currently only one). All REST and CLI APIs are supported through this REST Web Server.

'''FrameworkRestService''' – A set of classes constituting the Framework Servlet that responds to Framework REST requests (e.g., get latest Flight Recorder records, perform factory reset, etc.). The FrameworkRestService invokes control and configuration methods against the FrameworkMgmtPoint, and for reporting it retrieves information directly from the relevant repositories. For flight recordings it invokes methods against the FlightRecorder.

'''FrameworkMgmtPoint''' – Is the point to drive control and configuration commands (e.g., start, stop, reset, set address of the hosting machine, etc.). FrameworkMgmtPoint invokes in turn methods against other relevant modules in the right order. It forwards lifecycle requests (start, stop, reset) directly to FrameworkMain to drive them in the right order.
'''Defense4All Application''' – Is the AppRoot object that should be implemented/extended by any SDN application – in our case Defense4All. SDN Applications do not have “main”, and their lifecycle (start, stop, reset) is managed by the Framework operating against the Application root object, which then drives all lifecycle operations in the application. This module also contains reference back to the Framework, allowing the application to use Framework services (e.g., create a Repo, log a flight record) and common utilities.
'''Common classes and utilities''' – is a library of convenience classes and utilities, from which any Framework or SDN Application module can benefit. Examples include wrapped threading services (for asynchronous, periodic or background execution), short hash of a string, confirmation by user, etc.

'''Repository services''' – One of the key elements in the Framework philosophy is decoupling compute state from compute logic. All durable state should be stored in a set of repositories that can be then replicated, cached, distributed under the covers –with no awareness of the compute logic (Framework or Application). Repository services comprise the RepoFactory and Repo or its annotations friendly equivalent – the EntityManager. The RepoFactory is responsible to establish connectivity with the underlying Repository plugged in service, instantiate new requested repositories and return references to existing ones. The chosen underlying Repository service is Hector Client over Cassandra NoSQL DB. Repo presents an abstraction of a single DB table. It allows reading the whole table, only table keys (tables are indexed by only the single primary key), records or single cells, as well as writing records or single cells with controlled eagerness. A sub-record (with only a portion of cells) may be written. In such a case the appearing cells override existing ones in the repository. Other cells in the repository remain unchanged. In contrast to relational DB, in which all columns must be specified up-front (in schema design), Repo leverages the underlying Cassandra support to contain rows (records) in the same table with different sets of columns, some of which may not being even defined up-front. Furthermore, cells with new columns can be added or removed on the fly. RepoFactory and Repo (as well as its Entity Manager annotation friendly equivalent) constitute a convenience library targeted to Framework and SDN Applications goals – on top of the Hector client library communicating with Cassandra Repository cluster. Scaling Cassandra cluster, distributing data shards across Cassandra cluster members, configuring read/write eagerness and consistency – are for the most part encapsulated in this layer. 

'''Logging and Flight Recorder services''' – The logging service simply uses Log4J library to log error, warning, trace or informational messages. These logs are mainly for Defense4All developers. Administrators can obtain additional details about failures from errors log. FlightRecorder records all flight records recorded by any Defense4All module, including information received from external network elements, such as ODC, AMSs, etc. It then allows a user/administrator to obtain that information through REST/CLI. Flight records can be filtered by categories (zero or more can be specified) and by time ranges. FlightRecorder stores all flight records in its own Repo (with another repo holding time ranges for efficient time ranges retrieval from the records repo). Because all flight records are stored in Cassandra, the number of flight records Defense4All can keep is limited only by the size of the underlying persistent storage capacity of all Cassandra servers, and so even on a single Cassandra instance months of historical information can be kept.

'''HealthTracker''' – Is the point to hold the aggregated runtime health of Defense4All, and to act in response to severe deteriorations. Any module, upon sensing an unexpected/faulty behavior in it or in any other module can record a “health issue” in the HealthTracker, providing health issue significance. This is instead of directly triggering Defense4All termination. The idea is that numerous health issues in a short period of time with high aggregated significance are likely to indicate a significant wide-spread Defense4All problem, but sporadic/intermittent operational “hiccups” can be neglected – even if Defense4All remains less than 100% operational (the administrator can always reset restart it to fully recover). As such, every non-permanent health issue has a gradually diminished affect over time. If Defense4Al health deteriorates below a predefined threshold HealthTracker triggers responsive actions – depending on the nature of health issues. A restart can heal transient problems, and so the HealthTracker triggers Defense4All termination (running as a Linux service Defense4All will be automatically restarted). To recover from more permanent problems HealthTracker may additional trigger a Defense4All reset. If it does not help then the next time the HealthTracker will attempt a more severe reset. As a last resort the administrator can be suggested to perform factory reset.

'''ClusterMgr''' – Currently not implemented. This module is responsible for managing a Defense4All cluster (separate from Cassandra or ODC clusters, modeled as separate tier clusters). A clustered Defense4All carries improved high availability and scalability. Any module in Defense4All Framework or Application can register with ClusterMgr for clustered operation, specifying whether its functionality should be carried out by a single or by multiple/all active instances (running on different Defense4All cluster members). When cluster membership changes, ClusterMgr notifies each instance in each module about its role in the clustered operation of that module. In case of a single active instance that instance is told so, while all other instances are told they are standby. In case of multiple active instances, each active instance is notified about the number of active instances, and its logical enumeration in that range. All state is stored in a globally accessible and shared repository, so any instance of a module is stateless, and can perform any role after every membership change. For example, following membership change N an instance can be enumerated as 2 out of 7, and thus perform relevant portion of the work. Then at membership change N+1 the same instance can be enumerated 5 out of 6, and perform the work portion allocated for 5 and not for 2. We skip the peer messaging services which the ClusterMgr can provide for a more coordinated cross-instance operation.

The Defense4All Application is highly pluggable - it can accommodate different attack detection mechanisms, different attack mitigation drivers, and drivers (called reps – short for representative) to different versions of ODC and different AMSs. Defense4All Application comprises “core” modules and “pluggable” modules implementing well-defined Defense4All Application APIs. 

=[[Tutorial:Application View|Application View]] =
[[File:d4a_application_view.jpg|none|900px|Defense4All Defense4All Application Structure]]

The Defense4All Application modules are described below:

'''DFAppRoot''' – This is the root module of the Defense4All Application. As mentioned before, the Defense4All Application does not have “main”, and its lifecycle (start, stop, reset) is managed by the Framework operating against this module, which in turn drives all lifecycle operations in the Defense4All Application. DFAppRoot also contains references to all Defense4All Application modules (core and pluggable), global repositories, and reference back to the Framework, allowing the Defense4All Application modules to use Framework services (e.g., create a Repo, log a flight record) and common utilities.

'''DFRestService''' – A set of classes constituting the Defense4All Application Servlet that responds to Defense4All Application REST requests. The DFRestService invokes control and configuration methods against the DFMgmtPoint, and for reporting it retrieves information directly from the relevant repositories. For flight recordings it invokes methods against the FlightRecorder.

'''DFMgmtPoint''' – The point to drive control and configuration commands (e.g., addams, addpn). DFMgmtPoint invokes in turn methods against other relevant modules in the right order.

'''ODL Reps''' – Is a pluggable module-set for different versions of ODC. Comprises two functions in two sub-modules – stats collection for and traffic diversion of relevant traffic. These two sub-modules adhere to StatsCollectionRep DvsnRep APIs. ODL Reps is detailed in Figure 6 and description that follows it.
SDNStatsCollector – Is responsible to set “counters” for every PN at specified network locations (physical or logical). A counter is a set of OpenFlow flow entries in ODC enabled network switches/routers. The SDNStatsCollector periodically collects statistics from those counters and feeds them to the SDNBasedDetectionMgr (see next). The module uses the SDNStatsCollectionRep to both set the counters and read latest statistics from those counters. A stat report consists of read time, counter specification, PN label, and a list of trafficData information, where each trafficData element contains latest bytes and packets values for flow entries configured for <protocol,port,direction> in the counter location. Protocol can be {tcp,udp,icmp,other ip}, port is any layer 4 port, and direction can be {inbound, outbound}.

'''SDNBasedDetectionMgr''' – Is a container for pluggable SDN based detectors. It feeds stat reports, received from SDNStatsCollector, to plugged-in SDN based detectors. It also feeds to all SDN based detectors notifications from AttackDecisionPoint (see ahead) about ended attacks (so as to allow reset of detection mechanisms). 

'''RateBasedDetector sub-module''' – This detector learns for each PN its normal traffic behavior over time, and notifies AttackDecisionPoint (see next) when it detects traffic anomalies. For each protocol {tcp, udp, icmp, other ip} of each PN the RateBasedDetector maintains latest rates and exponential moving averages (baselines) of bytes and packets, as well as last reading time. The detector maintains those values both for each counter as well as aggregation of all counters for each PN. The organization at two levels of calculations (counter and PN aggregate) allows for better scalability (e.g., working with clustered ODC, where each instance is responsible to obtain statistics from a portion of network switches, and bypassing the ODC single instance image API).  Such organization also enables a more precise stats collection (avoiding the difficulty to collect all stats during a very small time interval). Stats are processed at the counter level, and periodically aggregated at the PN level. Continuous detections of traffic anomalies cause the RateBasedDetector to notify AttackDecisionPoint about attack detection. Then absence of anomalies for some period of time causes the detector to stop notifying AttackDecisionPoint about attack detection. The detector specifies a detection duration – time within which the detection is valid. After that time the detection expires, but can be “prolonged” with another notification about the same attack.

'''AttackDecisionPoint''' – This module is responsible to maintain attack lifecycles. It can receive attack detections from multiple detectors. Defense4All supports the RateBasedDetector, external detectors (in future versions), and AMS based detector reference implementation (over Radware’s DefensePro). In current version AttackDecisionPoint fully honors each detection (max detector confidence, max detection confidence). It declares a new attack for every detection of a new attacked traffic (PN, protocol, port), and add more detections for existing (already declared attacks). The module checks periodically the statuses of all attacks. As long as there is at least one unexpired detection (each detection has an expiration time) attack is kept declared. If all detections are expired for a given attack AttackDecisionPoint declared attack end. The module notifies the MitigationMgr (see next) to start mitigating any new declared attack. It notifies the MitigationMgr to stop mitigating ended attacks, and also notifies the detectionMgr to reset stats calculations for traffic on which an attack has just ended.
 
'''MitigationMgr''' - Is a container for pluggable mitigation drivers. The MitigationMgr maintains the lifecycle of all mitigations, resulted from mitigation notifications from AttackDecisionPoint. It holds a pre-ordered list of the MitigationDriver sub-modules, and attempts to satisfy each mitigation in that order. If MitigationDriveri indicates to MitigationMgr that it does not mitigate a mitigation (because of per PN preferences, unavailability of AMS resources, network problems, etc.) MitigationMgr will attempt mitigation by MitigationDriveri+1. If none of the plugged-in MitigationDrivers handle mitigation it remains in status ‘not-mitigated’. 

'''MitigationDriverLocal''' – This mitigation driver is responsible to drive attack mitigations using AMSs in its sphere of management. When requested to mitigate an attack this mitigator performs the following sequence of steps:
# It consults with the plugged in DvsnRep (see ahead) about topologically feasible options of diversion to each of the managed AMSs from each of the relevant network locations. In this version diversion is always done from the location where the stats counters are installed. 
# The MitigationDriverLocal then selects an AMS out of all feasible options (in the first release the selection is trivial – first in list.
# It optionally configures all the AMSs (each diversion source may have a different AMS associated with it) prior to instructing to divert traffic to each. This is done through the plugged in AMSRep. 
# The MitigationDriverLocal instructs the DvsnRep to divert traffic from each source NetNode (in this version NetNode is modeled over an SDN Switch) to the AMS associated with that NetNode. Diversion can be either for inbound traffic only or both for inbound and outbound traffic.
# The mitigation driver notifies the AMSBasedDetector to optionally start monitoring attack status in all the AMSs, and feed attack detections to AttackDecisionPoint.
# In future versions the MitigationDriverLocal should monitor health of all AMSs and relevant portions of network topologies, re-selecting AMSs should some fail, or should network topologies changes require that.
When mitigation should be ended the MitigationDriverLocal notifies AMSBasedDetector to stop monitoring attack status for the ended attack, notifies DvsnRep to stop traffic diversions to all AMSs – for this mitigation, and finally notifies the AMSRep to optionally clean all mitigation related configuration set in each relevant AMS.

'''AMSBasedDetector''' – This optional module (can be packaged as part of the AMSRep) is responsible for monitoring/querying attack mitigation by AMSs. Registering as a detector this module can then notify AttackDecisionPoint about attack continuations and endings. It monitors only specified AMSs and only for specified (attacked) traffic.

'''AMSRep''' - Is a pluggable module for different AMSs. The module adheres to AMSRep APIs. It can support configuration of all introduced AMSs (permanently or before/after attack mitigations). It can also receive/query security information (attack statuses), as well as operational information (health, load). AMSRep module is entirely optional – AMSs can be configured and monitored externally. In many cases attacks can continue be monitored solely via SDN counters. Defense4All contains a reference implementation AMSRep that communicates with Radware’s DefensePro AMSs.

=[[Tutorial:ODL Reps View|ODL Reps View]] =

[[File:d4a_odl_reps_view.jpg|none|900px|Defense4All Defense4All ODL Reps Structure]]

The figure depicts the Defense4All Application ODL Reps module-set structure. Different versions of OFC may be represented by different versions of ODL Reps module-set. As mentioned before, ODLReps comprises two functions – stats collection for and traffic diversion of relevant traffic. Both or either of the functions may be utilized in a given deployment. As such they have a common point to communicate with the ODC and hold all general information – the ODC (see below).

ODL Reps supports two types of SDN Switches: sdn-hybrid, which support both SDN and legacy routing, and sdn-native, which supports SDN only routing. Counting traffic in sdn-hybrid switch can be simply accomplished by programming a flow entry with desired traffic selection criteria and the action is “send to normal”, i.e., continue with legacy routing. Counting traffic in sdn-native switch requires an explicit routing action (i.e., which output port to send the traffic to). Defense4All avoids learning all routing tables by requiring an sdn-native switch which is more or less a bump-in the wire with respect to traffic routing, i.e., traffic entering port 1 normally exits port 2 and traffic entering port 3 normally exits port 4 and vice versa. Such a switch allows for easy programming of flow entries just to count traffic or to divert traffic to/from attached AMS. So when Defense4All programs a traffic counting flow entry with selection criteria that includes port 1, its action will be output to port 2, and similarly with 3 to 4. In future versions this restriction should be lifted.

The sub-modules are described below:

'''StatsCollectionRep''' - The module adheres to StatsCollectionRep APIs. Its main tasks are:
* Offer counter placement NetNodes in the network. The NetNodes offered are all NetNodes defined for a PN. This essentially maps to all SDN Switches through which traffic of the given PN flows.
* Add a peacetime counter in selected NetNodes to collect statistics for a given PN. StatsCollectionRep creates a single counter for a PN in each NetNodes. (Overall, a NetNodes can have multiple counters for different PNs; and a PN may have multiple counters - in NetNodes as specified for the given PN). StatsCollectionRep translates installation of a counter in NetNodes to programming 4 flow entries (for tcp, udp, icmp, and rest of ip) for each “north traffic port” in that NetNodes – port from which traffic from client to protected PN enters the SDN Switch. For example, StatsCollectionRep will add for a given PN 12 flow entries in an SDN Switch with 3 ports through that PN’s inbound traffic enters the OFS. And, if another NetNode (SDN Switch) was specified to have that PN’s inbound traffic entering it through 2 ports, then StatsCollectionRep will program for this PN 8 flow entries in that second NetNode.
* Remove a peacetime counter.
* Read latest counter values for a specified counter. StatsCollectionRep returns a vector of latest bytes and packets counted for each protocol-port in each direction (currently only “north to south” is supported), along with the time it received the reading from ODC.

'''DvsnRep''' - The module adheres to DvsnRep APIs. Its main tasks are:
* Return diversion properties from a given NetNode to a given AMS. In this version an empty properties is returned if such diversion is topologically feasible (AMS is directly attached to the SDN Switch over which the specified NetNode is modeled. Otherwise no properties is returned. (This leaves room for remote diversions in future versions, and topological costs to each distant AMS – e.g., latency, bandwidth reservation, cost, etc.).
* Divert (attacked) traffic from a specified NetNode through an AMS.  As such the new flow entries take precedence over the peacetime ones. DvsnRep programs flow entries to divert inbound attacked traffic (or all traffic if so specified for the PN) from every “north traffic port” into AMS “north” port. If “symmetric diversion” (for both inbound and returning, outbound traffic) has been specified for that PN, DvsnRep programs another set of flow entries to divert attacked (or all) traffic from every “south traffic port” into AMS “south” port. In an sdn-hybrid switch deployment DvsnRep adds a flow entry for inbound traffic that returns from AMS south port – with action send to normal, and similarly it adds a flow entry for outbound returning traffic from AMS north port – with action of also send to normal. In sdn-native switch the action is to send to the right output port, but things are more complicated here determining that right port. We use north port MAC learning to determine from the source/destination MAC in the packet the right output port. This scheme of flow entries works well for tcp, udp and icmp attacks. For “other ip” attacks the flow entries programming is more complex, and is suppressed here for clarity. The set of flow entries programmed to divert (but still count) traffic comprises the “attack traffic floor”. There may be many attack traffic floors, all of which take precedence over the peacetime stats collection floor (by programming higher priority flow entries). Additional attacks (except “other ip” attack which is special case, and is suppressed here) are created with higher priority traffic floors over previously set attack traffic floors. Attacks may fully or partially “eclipse” earlier attacks (e.g., tcp port 80 over tcp or vice versa) or be disjoint (e.g., tcp and udp). Stats collection is taken from all traffic floors – peacetime and attacks. SDN based detector aggregates all statistics into overall rates, thus determining if the attack is still on. (Notice that eclipsed peacetime counted traffic may show zero rates, and that counting is complemented by the higher priority floor counters.
* End diversion. DvsnRep removes the relevant attack traffic floor (removing all its flow entries from the NetNode). Note that this affects neither traffic floors “above” the removed floor nor the traffic floors “below”. In addition, the SDN based detector receives the same aggregated rates from counters of remaining floors, so its operation is not affected either.

'''ODLCommon''' – This module contains all common elements needed to program flow entries in ODC. This allows for coherent programming of configured ODCs (in this version at most one) by StatsCollectionRep and DvsnRep. For instance ODLCommon instantiates connectivity with the ODCs, maintains a list of programmed flow entries and cookies assigned each. It also maintains references to DFAppRoot and FrameworkMain. When an sdn-native NetNode is added ODLCommon programs 2 flow entries per each protected link (pair of input-to-output ports) to transfer traffic between the two ports (traffic entering north port is routed to south port and vice versa). ODLCommon adds 2 more flow entries for each port connecting an AMS – to block returning ARP traffic (so as to avoid ARP floods if AMSs are not configured to block them). This “common traffic floor” flow entries are set with lowest priority. Their counters are accounted for neither stats collections nor traffic diversion. When a NetNode is removed ODLCommon removes this common traffic floor flow entries.

'''FlowEntryMgr''' – This module provides an API to perform actions on flow entries in an SDN switch managed by an ODC, and retrieve information about all nodes managed by an ODC. Flow entries actions include adding a specified flow entry in a specified NetNode (SDN Switch/Router), removing a flow entry, toggling a flow entry, getting details of a flow entry, and readings statistics gathered by the flow entry. FlowEntryMgr uses the Connector modules to communicate with ODC.

'''Connector''' – This module provides the basic APIs to communicate with ODC, wrapping REST communications. After initializing connection details with a specified ODC the Connector allows getting or deleting something from ODC, as well as posting or putting something to ODC. 

'''ODL REST Pojos''' – This set of java classes are part of the ODC REST API, specifying the Java classes of the parameters and the results of interaction with ODC.

=[[Tutorial:Basic control flows|Basic control flows]] =
Control flows are logically ordered according to module runtime dependencies, so if module A depends on module B then module B should be initialized before module A, and terminate after it. Defense4All App modules depend on most Framework modules, except RestServer.

'''Startup''' – The startup consists of first instantiating all modules, then initializing them in order. Module instantiation includes setting up all externally configured runtime parameters. Initialization starts with all Framework modules, except the RestServer, which is activated last (after the application is fully ready). The Framework modules are initialized in the following order: FrameworkMain, RepoFactory, FlightRecorder, FrameworkMgmtPoint. Then the Framework initializes the Defense4All Application. Defense4All Application modules are initialized in the following order: MitigationDriverLocal, global Defense4All Application Repos, ODLStatsCollectionRep, ODLDvsnRep, AmsRep, StatsCollector, DetectorMgr and RateBasedDetector, AttackDecisionPoint, MitigationMgr, DFMgmtPoint. When FrameworkMgmtPoint initializes it retrieves from global repos all user configurations in previous lifecycles, and re-applies them in all the relevant modules - as if the user has just configured them, however with one difference – any dynamically added state to configured elements (e.g., PNs) is preserved. Similarly, Defense4All Application MgmtPoint re-applies all user configurations done in previous lifecycles. This design leaves all modules completely stateless, allowing flexibility, consistency and robustness in applying user configurations (whether to latest or any previous state snapshot).

'''Termination''' – In this flow first the RestServer is stopped, then the Defense4All Application modules, and finally the Framework modules. The order of Defense4All Application modules tear-down is: DFMgmtPoint, MitigationMgr and MitigationDriverLocal, AttackDecisionPoint, DetectionMgr and RateBasedDetector, StatsCollector, AmsRep, OdlStatsCollectoinRep, OdlDvsnRep. The order of Framework modules tear-down is: FrameworkMgmtPoint, FlightRecorder, RepoFactory and all Repos. Each module is expected to store all durable state in relevant Repos. Then RepoFactory, flushes all Repos cached state to stable storage and/or replicates to other Repo replicas, of there are any.

'''Reset''' – In this flow all modules are reset according to reset level (currently soft, dynamic or factory). The order of modules’ reset is identical to that of termination. Reset includes clearing cached information as well as fully or selectively information from Repos – according to the type of reset.

=[[Tutorial:Configurations and setup flows|Configurations and setup flows]] =
'''OFC (OpenFlowController = ODC)''' – When DFMgmtPoint receives from DFRestService a request to add OFC, it first records the added OFC in OFCs Repo, then notifies ODLStatsCollectionRep and ODLDvsnRep, which in turn notifies ODL to initiate connection to added OFC (ODC). ODL instantiates a REST client for communication with ODC.
NetNode  - Multiple NetNodes can be added. Each NetNode models a switch or similar network device, along with its traffic ports, protected links and connections to AMSs. When DFMgmtPoint receives from DFRestService a request to add a NetNode, it first records the added NetNode in NetNodes Repo, and then notifies ODLStatsCollectionRep and ODLDvsnRep, followed by MitigationMgr. ODLStatsCollectionRep and ODLDvsnRep in turn notify ODL, and the latter installs low priority flow entries to pass traffic between protected links’ port pairs. MitigationMgr notifies MitigationDriverLocal, which updates its NetNode-AMS connectivity groups for consistent assignment of AMSs to diversion from given NetNodes.

'''AMS''' – Multiple AMSs can be added. When DFMgmtPoint receives from DFRestService a request to add an AMS, it first records the added AMS in AMSs Repo, then notifies AMSRep. AMSRep can optionally pre-configure protection capabilities in the added AMS, and start monitoring its health.

'''PN''' - Multiple PNs can be added. When DFMgmtPoint receives from DFRestService a request to add a PN, it first records the added PN in PNs Repo, then notifies MitigationMgr, and finally it notifies the DetectionMgr. MitigationMgr notifies MitigationDriverLocal, which in turn notifies AMSRep. AMSRep can preconfigure the AMS for this PN, as well its EventMgr to accept events related to this PN’s traffic. DetectionMgr notifies RateBasedDetector, which in turn notifies StatsCollector. StatsCollector queries ODLStatsCollectionRep about possible placements of stats collection counters for this PN. ODLStatsCollectionRep returns all NetNodes configured for this PN (and if none configured it returns all NetNodes currently known to Defense4All). StatsCollector “chooses” (this only presently available) counter locations option. For each of the NetNodes it then asks ODLStatsCollectionRep to create a counter for the subject PN in each of the selected NetNodes. The counter is essentially a set of flow entries set for the protocols of interest (tcp, udp, icmp, and rest of ip) on each north traffic port. The counter is given a priority and this constitutes the peacetime traffic floor (to monitor traffic by periodically reading all counter flow entry traffic count values). Because PN may be re-introduced at restart or a change in network topology may require re-calculation of counter locations, it is possible that some/all counters may already be in place. Only new counters are added. No-longer needed counters are removed. ODLStatsCollectionRep configures the flow entries according to NetNode type: for hybrid NetNodes the flow entry action is “send to normal” (i.e., proceed to legacy routing), while for native NetNodes the action is matching output port (in each protected link). OdlStatsCollectionRep invokes ODL to create each specified flow entry. The latter invokes FlowEntryMgr and Connector to send the request to ODC.

=[[Tutorial:Attack Detection flow|Attack Detection flow]] =
Periodically StatsCollector requests ODL StatsCollectionRep to query the ODC for the latest statistics for each set counter for each configured PN. ODLStatsCollectionRep invokes FlowEntryMgr to obtain statistics in each flow entry in a counter. The latter invokes the Connector to obtain the desired statistics from the ODC. 

ODLStatsCollectionRep aggregates the obtained results in a vector of stats (latest bytes and packets readings per each protocol) and returns that vector. StatsCollector feeds each counter stats vector to DetectionMgr, who forwards the stats vector to RateBasedDetector. The latter maintains stats information for every counter as well as aggregated counter stats for every PN. Stats information includes time of previous reading, and for every protocol - latest rates and exponential averages. 

The RateBasedDetector checks for significant and prolonged latest rates deviations from the averages, and if such deviations are found in the PN aggregated level it notifies AttackDecisionPoint about attack detection. As long as deviations continue the RateBasedDetector will continue notifying AttackDecisionPoint about the detections. It sets an expiration time for every detection notification, and repeatable notifications essentially prolong the detection expiration. 

AttackDecisionPoint honors all detections. If it has already declared an attack on that protocol-port, then the AttackDecisionPoint associated the additional detection with that existing attack. Otherwise it creates a new attack, and notifies MitigationMgr to mitigate that attack (next section). Periodically, AttackDecisionPoint checks the status of all detections of each live attack. If all detections are expired, AttackDecisionPoint declares attack end and notifies MitigationMgr to stop mitigating the attack.

=[[Tutorial:Attack Mitigation flow|Attack Mitigation flow]] =
MitigationMgr, upon receiving mitigate notification from AttackDecisionPoint, attempts to find a plugged-in MitigationDriver to handle the mitigation. Currently, it requests its only plugged-in MitigationDriverLocal. 

MitigationDriverLocal checks if there are known, live and available AMSs to which attacked (or all) traffic can be diverted from NetNodes through which attacked traffic flows. It selects one of the suitable AMSs and configures it prior to diverting attack traffic to the selected AMS. For example MitigationDriverLocal retrieves from Repo the relevant protocol averages and configures them in AMS through the AMSRep. 

MitigationDriverLocal then requests ODLDvsnRep to divert attacked PN protocol-port (or all PN) traffic from each of the NetNodes through PN traffic flows – to the selected AMS. 

ODLDvsnRep creates a new highest priority traffic-floor (that contains flow entries with priority higher than any flow entry in previously set traffic floors). The traffic floor contains all flow entries to divert and count traffic from every ingress/northbound traffic port into the AMS, and back from AMS to the relevant output (southbound) ports. Optionally diversion can be “symmetric” (in both directions), in which case flow entries are added to divert traffic from southbound ports into the AMS, and back from AMS to northbound ports. Note that StatsCollector treats this added traffic floor as any other, and passes obtained statistics from this floor to DetectionMgr/RateBasedDetector. Because traffic floors are aggregated (in the same NetNode as well as across NetNodes) for a given PN the combined rates remain the same as prior to diversion. Just like ODLStatsCollectionRep, ODLDvsnRep also utilizes lower level modules to install the flow entries in desired NetNodes.

Finally, MitigationDriverLocal notifies AMSRep to optionally start monitoring this attack and notify AttackDecisionPoint if the attack continues or new attacks develop. AMSRep can do that through AMSBasedDetector module.

If MitigationDriverLocal finds no suitable AMSs or fails to configure any of its mitigation steps it aborts the mitigation attempt, asynchronously notifying MitigationMgr about it. The mitigation then remains in status “no-resources”.

When MitigationMgr receives a notification to stop mitigating an attack, it forwards this notification to the relevant (and currently the only) MitigationDriver – MitigationDriverLocal. The latter reverses the actions in mitigation start: It notifies AMSRep to stop monitoring for this attack, it cancels diversion for attacked traffic, and finally notifies AMSRep to optionally remove pre-mitigation configurations.


Back to [[Defense4All:User_Guide|Defense4All User Guide Page]]
