<onlyinclude>
This page describes an data plane architecture for building a network virtualization solution using Open vSwitch.  This data plane design is used by two renderers: the [[Group_Policy:Architecture/OpenFlow_Renderer|OpenFlow Renderer]] and the [[Group_Policy:Architecture/OpFlex_Renderer|OpFlex Renderer]].

The design implements an overlay design and is intended to meet the following use cases:
* Routing when required between endpoint groups, including serving as a distributed default gateway
* Optional broadcast within a bridge domain.
* Management of L2 broadcast protocols including ARP and DHCP to avoid broadcasting.
* Layer 2-4 classifiers for policy between endpoint groups, including connection tracking/reflexive ACLs.
* Service insertion/redirection
</onlyinclude>
== Network Architecture ==

=== Network Topology ===
[[File:Overlay design red tunnel.png|framed|Figure 1: An example of a supported network topology, with an underlying IP network and hypervisors with Open vSwitch.  Infrastructure components and elements of the underlay network are shown in grey.  Three endpoint groups exist with different subnets in the same layer 3 context, which are show in red, green, and blue.  A tunneled path (dotted red line) is shown between two red virtual machines on different VM hosts.]]
The network architecture is an overlay network based on VXLAN or similar encapsulation technology, with an underlying IP network that provides connectivity between hypervisors and the controller.  The overlay network is a full-mesh set of tunnels that connect each pair of vSwitches.

The "underlay" IP network has no special requirements though it should be set up with ECMP to the top-of-rack switch for the best performance, but this is not a strict requirement for correct behavior.  Also, the underlay network should be configured with a path MTU that's large enough to accommodate the overlay tunnel headers.  For a typical overlay network with a 1500 byte MTU, a 1600 byte MTU in the underlay network should be sufficient.  If this is not configured correctly, the behavior will be correct but it will result in fragmentation which could have a severe negative effect on performance.

Physical devices such as routers on the IP network are trusted entities in the system since these devices would have the ability to forge encapsulated packets.

=== Control Network ===
The security of the system depends on keeping a logically isolated control network separate from the data network, so that guests cannot reach the control network.  Ideally, the network is kept isolated through an out-of-band control network.  This can be accomplished using a separate NIC, a special VLAN, or other mechanism.  However, the system is also designed to operate in the case where the control traffic and the data traffic are on the same layer 2 network and isolation is still enforced.

In Figure 1, the control network is shown as 172.16/16.  The VM hosts, and controllers all all have addresses on this network, and communicate using OpenFlow and OVSDB on this network.  In the example, the router is shown with an interface configured on this network as well; this works but in practice it is preferable to isolate this network by accessing it through a VPN or jump box if needed.  Note that there is no requirement that the control network be all in one subnet.

The router is also shown with an interface configured on the 10/8 network.  This network will be used for routing traffic destined for internet hosts.  Both the 172.16/16 and 10/8 networks here are isolated from the guest address spaces.

=== Overlay Network ===
Whenever traffic between two guests is in the network, it will be encapsulated using a VXLAN tunnel (though supporting additional encapsulation formats could be configured in the future).  A packet encapsulated as VXLAN contains:
* Outer ethernet header, with source and destination MAC
* Outer IP header, with source and destination IP address
* Outer UDP header
* VXLAN header, with a virtual network identifier (VNI).  The virtual network identifier is a 24-bit field that uniquely identifies an endpoint group in our policy model. 
* Encapsulated original packet, which includes:
** Inner ethernet header, with source and destination MAC
** (Optional) Inner IP header, with source and destination IP address

=== Delivering Packets ===
Endpoints can communicate with each other in a number of different ways, and each is processed slightly differently.  Endpoint groups exist inside a particular layer 2 or layer 3 context which represents a namespace for their network identifiers.  It's only possible for endpoints to address endpoints within the same context, so no communication is possible for endpoints in different layer 3 contexts, and only layer 3 communication is possible for endpoints in different layer 2 contexts.

==== Overlay Tunnels ====
The next key piece of information is the location of the destination endpoint.  For destinations on the same switch, we can simply apply policy (see below), perform any routing action required (see below), then deliver it to the local port.

When the endpoints are located on different switches, we need to use the overlay tunnel.  This is the case shown as a dotted red line in Figure 1.  After policy is applied to the packet, we encapsulated it in a tunnel with the tunnel ID set to a unique ID for the destination endpoint group.  The outer packet is addressed to the IP address of the OVS host that hosts the destination endpoint.  This encapsulated packet is now sent out to the underlay network, which is just a regular IP network that can deliver the packet to the destination switch.

When the encapsulated packet arrives on the other side, the destination vSwitch inspects the metadata of the encapsulation header to see if the policy has been applied already. If the policy has not been applied or if the encapsulation protocol does not support carrying of metadata, the policy must be applied at the destination vSwitch. The packet can now be delivered to the destination endpoint.
==== Bridging and Routing ====
XXX add diagrams here

The system will transparently handle bridging or routing as required.  Bridging occurs between endpoints in the same layer 2 context, while routing will generally be needed for endpoints in different layer 2 contexts.  More specifically, a packet needs to be routed if it is addressed to the gateway MAC address.  We can simply use a fixed MAC address to serve as the gateway everywhere.  Packets addressed to any other MAC address can be bridged.

Bridged packets are easy to handle, since we don't need to do anything special to them to deliver them to the destination.  They can be simply delivered unmodified.

Routing is slightly more complex, though not massively so.  When routing locally on a switch, we simply rewrite the destination MAC address to the MAC of the destination endpoint, and set the source MAC to the gateway MAC, decrement the TTL, and then deliver it to the correct local port. 

When routing to an endpoint on a different switch, we'll actually perform routing in two steps.  On the source switch, we will decrement TTL and rewrite the source MAC address to the MAC of the gateway router (so that both the source and the destination MAC are set to the gateway router's MAC).  It's then delivered to the destination switch using the appropriate tunnel.  On the destination switch, we perform a second routing action by now rewriting the destination MAC as the MAC address of the destination endpoint and decrementing the TTL again.  The reason why do the routing as two hops is that this avoids the need to maintain on every switch the correct MAC address for every endpoint on the network.  Each switch needs the mappings only for endpoints that are directly attached to that switch.

The vSwitch on each host must respond to local ARP requests for the gateway IP address and return a logical MAC address representing the L3 gateway.

==== Communicating with Outside Hosts ====
XXX add diagrams

Everything up until now is quite simple, but it's possible to conceive of situations where endpoints in our network need to communicate over the internet or with other endpoints outside the overlay network.  There are two broad approaches for handling this.  In both cases, we allow such access only via layer 3 communication.

First, we can map physical interfaces on an OVS system into the overlay network.  If a router interface is attached either directly to a physical interface or indirectly via an isolated network, then the router interface can be easily exposed as an endpoint in the network.  Endpoints can then communicate with this router interface (perhaps after some intermediate routing via the distributed routing scheme described above) and from there get to the rest of the world.  Dedicated OVS systems can be this configured as gateway devices into the overlay network which will then be needed for any of this north/south communication.  This has the advantage of being very simple but requires special effort to load balance the traffic effectively.

Second, we can use a DNAT scheme to allow access to endpoints that are reachable via the underlay network.  In this scheme, for every endpoint that is allowed to communicate to these outside hosts, we allocate an IP address from a dedicated set of subnets on the underlay (each network segment in the underlay network will require a separate DNAT range for switches attached to that subnet).  We can perform the DNAT translation on the OVS switch and then simply deliver the traffic to the underlay network to deliver to the internet host or other host, and perform the reverse translation to get back into the overlay network.

== Packet Processing Pipeline ==
Here is a simplified high-level view of what happens to packets in this network when it hits an OVS instance:
# Apply port security rules if enabled on the port to determine if the source identifiers (MAC and IP) are allowed on the port
# Determine the source endpoint group based on the tunnel ID from the from the outer packet or based on source port and source identifiers as configured.  Unknown source identifiers may result in a packet-in if the network is doing learning.  XXX - sEPG can't come from tunnel ID!  Need to feed this through a separate pathway since the policy is applied on ingress
# Catch any special packet types that are handled specially.  This could include ARP, DHCP, or LLDP.  How these are handled may depend on the specific renderer implementation.
# Determine whether the packet will be bridged or routed.  If the destination MAC address is the default gateway MAC, then the packet will be routed, otherwise it will be bridged.
# Determine the destination endpoint group.  For bridged packets, use the destination MAC address.  For routed packets, use the destination IP address.
# Apply the appropriate set of policy rules based on the active subjects for that flow.  We can bypass this step if the tunnel metadata indicates hat the policy has been applied at the source.
# Apply a routing action if needed by modifying the destination MAC and decrementing the TTL.  The routing action to apply depends on if the destination is remote or local.  For local destinations, rewrite the destination MAC to the MAC address for the connected endpoint.  For remote destinations, rewrite the destination MAC to the default gateway MAC.
# Determine the next hop for the flow and deliver it.  If the next hop is a local port, then it is delivered as is.  If the next hop is not local, then the packet is encapsulated and the tunnel ID is set to the network identifier for the destination endpoint group.  If the packet is a layer 2 broadcast packet, then it will need to be written to the correct set of ports, which might be a combination of local and remote ports.

=== Register Usage ===
The processing pipeline needs to store metadata such as the sEPG, dEPG, and broadcast domain. This metadata can be stored in any way supported by the switch. OpenFlow provides a dedicated 64 bit metadata field, Open vSwitch additionally provides multiple 32 bit registers in form of Nicira Extensions. The following examples will use Nicira extensions for simplicity. The choice of register usage is an implementation detail of the renderer.

==== Example Register Allocation ====

;<tt>NXM_NX_REG1</tt>
: Source Endpoint Group (sEPG) ID
;<tt>NXM_NX_REG2</tt>
: L2/L3 Domain - Broadcast / Routing domain
;<tt>NXM_NX_REG3</tt>
: Destination Endpoint Group (dEPG) ID
;<tt>NXM_NX_REG4</tt>
: Port number to send packet to after policy enforcement. This is required because port selection occurs before policy enforcement in the pipeline.

=== Port Security ===
XXX TODO

=== ARP/DHCP Interception ===
XXX TODO

=== Source EPG, Broadcast Domain, and Routing Domain Selection ===

The sEPG is determined based on a separate flow table which maps known OpenFlow port numbers and tunnel identifiers to a locally unique sEPG ID. The sEPG ID is stored in register NXM_NX_REG1 for later use in the pipeline. At the same time, the L2 and L3 domain is determined and stored in register NXM_NX_REG2.

XXX: Flow table example [[User:Tgraf|tgraf]] ([[User talk:Tgraf|talk]]) 09:34, 24 June 2014 (UTC)

A flow hit means that the sEPG is known and the pipeline should proceed to the next stage.

A flow miss means that we have received a packet from an unknown EPG:
# If the packet was received on a local port then this corresponds to the discovery of a new EP for which the Port to EPG mapping has not been populated yet. (XXX: Packet-in? [[User:Tgraf|tgraf]] ([[User talk:Tgraf|talk]]) 09:34, 24 June 2014 (UTC))
# If the packet was received from a tunnel then this corresponds to a packet for which we have not populated the tunnel ID to EGP mapping yet. (XXX: Packet-in? [[User:Tgraf|tgraf]] ([[User talk:Tgraf|talk]]) 09:34, 24 June 2014 (UTC))

=== Select Bridging or Routing ===
XXX TODO

=== Destination EPG Selection ===
XXX TODO

=== Policy Enforcement ===
XXX TODO

=== Forwarding ===
==== Unicast ====
XXX TODO

==== Broadcast ====
XXX TODO

==== QoS ====
XXX TODO

== Service Insertion ==
XXX TODO
