<onlyinclude>
This page describes an data plane architecture for building a network virtualization solution using Open vSwitch.  This data plane design is used by two renderers: the [[Group_Policy:Architecture/OpenFlow_Renderer|OpenFlow Renderer]] and the [[Group_Policy:Architecture/OpFlex_Renderer|OpFlex Renderer]].

The design implements an overlay design and is intended to meet the following use cases:
* Routing when required between endpoint groups, including serving as a distributed default gateway
* Optional broadcast within a bridge domain.
* Management of L2 broadcast protocols including ARP and DHCP to avoid broadcasting.
* Layer 2-4 classifiers for policy between endpoint groups, including connection tracking/reflexive ACLs.
* Service insertion/redirection
</onlyinclude>
== Network Architecture ==

=== Network Topology ===
[[File:Overlay design red tunnel.png|framed|Figure 1: An example of a supported network topology, with an underlying IP network and hypervisors with Open vSwitch.  Infrastructure components and elements of the underlay network are shown in grey.  Three endpoint groups exist with different subnets in the same layer 3 context, which are show in red, green, and blue.  A tunneled path (dotted red line) is shown between two red virtual machines on different VM hosts.]]
The network architecture is an overlay network based on VXLAN or similar encapsulation technology, with an underlying IP network that provides connectivity between hypervisors and the controller.  The overlay network is a full-mesh set of tunnels that connect each pair of vSwitches.

The "underlay" IP network has no special requirements though it should be set up with ECMP to the top-of-rack switch for the best performance, but this is not a strict requirement for correct behavior.  Also, the underlay network should be configured with a path MTU that's large enough to accommodate the overlay tunnel headers.  For a typical overlay network with a 1500 byte MTU, a 1600 byte MTU in the underlay network should be sufficient.  If this is not configured correctly, the behavior will be correct but it will result in fragmentation which could have a severe negative effect on performance.

Physical devices such as routers on the IP network are trusted entities in the system since these devices would have the ability to forge encapsulated packets.

=== Control Network ===
The security of the system depends on keeping a logically isolated control network separate from the data network, so that guests cannot reach the control network.  Ideally, the network is kept isolated through an out-of-band control network.  This can be accomplished using a separate NIC, a special VLAN, or other mechanism.  However, the system is also designed to operate in the case where the control traffic and the data traffic are on the same layer 2 network and isolation is still enforced.

In Figure 1, the control network is shown as 172.16/16.  The VM hosts, and controllers all all have addresses on this network, and communicate using OpenFlow and OVSDB on this network.  In the example, the router is shown with an interface configured on this network as well; this works but in practice it is preferable to isolate this network by accessing it through a VPN or jump box if needed.  Note that there is no requirement that the control network be all in one subnet.

The router is also shown with an interface configured on the 10/8 network.  This network will be used for routing traffic destined for internet hosts.  Both the 172.16/16 and 10/8 networks here are isolated from the guest address spaces.

=== Overlay Network ===
Whenever traffic between two guests is in the network, it will be encapsulated using a VXLAN tunnel (though supporting additional encapsulation formats could be configured in the future).  A packet encapsulated as VXLAN contains:
* Outer ethernet header, with source and destination MAC
* Outer IP header, with source and destination IP address
* Outer UDP header
* VXLAN header, with a virtual network identifier (VNI).  The virtual network identifier is a 24-bit field that uniquely identifies an endpoint group in our policy model. 
* Encapsulated original packet, which includes:
** Inner ethernet header, with source and destination MAC
** (Optional) Inner IP header, with source and destination IP address

=== Delivering Packets ===
[[File:Overlay design blue to red tunnel.png|framed|Figure 2: In this example, we show the path of traffic from the blue guest 192.168.2.3 and the red guest 192.168.1.2.  The traffic is encapsulated in a tunnel marked with the blue endpoint group's VNI while in transit between the two switches.  Because two endpoints are in different subnets, the traffic is routed in two hops: one the source switch and one on the destination switch.]]
[[File:Overlay design red to outside.png|framed|Figure 3: Communicating with outside endpoints requires some special challenges.  In this example, the red endpoint is communicating with an endpoint on the internet through a gateway router.  The traffic goes through a DNAT translation to an IP allocated to the endpoint for this purpose.  The IP communication can then be delivered through the IP underlay network.]]
Endpoints can communicate with each other in a number of different ways, and each is processed slightly differently.  Endpoint groups exist inside a particular layer 2 or layer 3 context which represents a namespace for their network identifiers.  It's only possible for endpoints to address endpoints within the same context, so no communication is possible for endpoints in different layer 3 contexts, and only layer 3 communication is possible for endpoints in different layer 2 contexts.

==== Overlay Tunnels ====
The next key piece of information is the location of the destination endpoint.  For destinations on the same switch, we can simply apply policy (see below), perform any routing action required (see below), then deliver it to the local port.

When the endpoints are located on different switches, we need to use the overlay tunnel.  This is the case shown as a dotted red line in Figure 1.  After policy is applied to the packet, we encapsulated it in a tunnel with the tunnel ID set to a unique ID for the destination endpoint group.  The outer packet is addressed to the IP address of the OVS host that hosts the destination endpoint.  This encapsulated packet is now sent out to the underlay network, which is just a regular IP network that can deliver the packet to the destination switch.

When the encapsulated packet arrives on the other side, the destination vSwitch inspects the metadata of the encapsulation header to see if the policy has been applied already. If the policy has not been applied or if the encapsulation protocol does not support carrying of metadata, the policy must be applied at the destination vSwitch. The packet can now be delivered to the destination endpoint.
==== Bridging and Routing ====
The system will transparently handle bridging or routing as required.  Bridging occurs between endpoints in the same layer 2 context, while routing will generally be needed for endpoints in different layer 2 contexts.  More specifically, a packet needs to be routed if it is addressed to the gateway MAC address.  We can simply use a fixed MAC address to serve as the gateway everywhere.  Packets addressed to any other MAC address can be bridged.

Bridged packets are easy to handle, since we don't need to do anything special to them to deliver them to the destination.  They can be simply delivered unmodified.

Routing is slightly more complex, though not massively so.  When routing locally on a switch, we simply rewrite the destination MAC address to the MAC of the destination endpoint, and set the source MAC to the gateway MAC, decrement the TTL, and then deliver it to the correct local port. 

When routing to an endpoint on a different switch, we'll actually perform routing in two steps.  On the source switch, we will decrement TTL and rewrite the source MAC address to the MAC of the gateway router (so that both the source and the destination MAC are set to the gateway router's MAC).  It's then delivered to the destination switch using the appropriate tunnel.  On the destination switch, we perform a second routing action by now rewriting the destination MAC as the MAC address of the destination endpoint and decrementing the TTL again.  The reason why do the routing as two hops is that this avoids the need to maintain on every switch the correct MAC address for every endpoint on the network.  Each switch needs the mappings only for endpoints that are directly attached to that switch.  A communication pathway requiring this routing is shown in Figure 2.

The vSwitch on each host must respond to local ARP requests for the gateway IP address and return a logical MAC address representing the L3 gateway.

==== Communicating with Outside Hosts ====
Everything up until now is quite simple, but it's possible to conceive of situations where endpoints in our network need to communicate over the internet or with other endpoints outside the overlay network.  There are two broad approaches for handling this.  In both cases, we allow such access only via layer 3 communication.

First, we can map physical interfaces on an OVS system into the overlay network.  If a router interface is attached either directly to a physical interface or indirectly via an isolated network, then the router interface can be easily exposed as an endpoint in the network.  Endpoints can then communicate with this router interface (perhaps after some intermediate routing via the distributed routing scheme described above) and from there get to the rest of the world.  Dedicated OVS systems can be this configured as gateway devices into the overlay network which will then be needed for any of this north/south communication.  This has the advantage of being very conceptually simple but requires special effort to load balance the traffic effectively.

Second, we can use a DNAT scheme to allow access to endpoints that are reachable via the underlay network.  In this scheme, for every endpoint that is allowed to communicate to these outside hosts, we allocate an IP address from a dedicated set of subnets on the underlay (each network segment in the underlay network will require a separate DNAT range for switches attached to that subnet).  We can perform the DNAT translation on the OVS switch and then simply deliver the traffic to the underlay network to deliver to the internet host or other host, and perform the reverse translation to get back into the overlay network.

For the first implementation, we'll stick with the DNAT scheme and consider implementing the gateway-based or other solution.

== Packet Processing Pipeline ==
[[File:Gbp ovs pipeline.png|framed|right|Figure 4: The pipeline for processing packets in the data plane is shown.  The port security feature is shown with a dotted outline to indicate that it is optional.]]
Here is a simplified high-level view of what happens to packets in this network when it hits an OVS instance:
# If data and management network are shared, determine whether packet is targeted for the host system. If so, reinject into host networking stack.
# Apply port security rules if enabled on the port to determine if the source identifiers (MAC and IP) are allowed on the port
#* For packets received from the overlay: Determine the source endpoint group (sEPG) based on the tunnel ID from the outer packet header.
#* For packets received from local ports: Determine sEPG based on source port and source identifiers as configured.
#* As an sEPG can only be associated with a single L2 and L3 context, the context is determined in this step as well.
#* Unknown source identifiers may result in a packet-in if the network is doing learning.
# Handle broadcast and multicast packets while respecting broadcast domains.
# Catch any special packet types that are handled specially.  This could include ARP, DHCP, or LLDP.  How these are handled may depend on the specific renderer implementation.
# Determine whether the packet will be bridged or routed. If the destination MAC address is the default gateway MAC, then the packet will be routed, otherwise it will be bridged.
# Determine the destination endpoint group (dEPG) and outgoing port or next hop while respecting the L2/L3 context.
#* For bridged packets (L2): Determine based on the destination MAC address.
#* For routed packets (L3): Determine based on the destination IP address.
# Apply the appropriate set of policy rules based on the active subjects for that flow.  We can bypass this step if the tunnel metadata indicates hat the policy has been applied at the source.
# Apply a routing action if needed by modifying the destination and source MAC and decrementing the TTL. 
#* For local destination: Rewrite the destination MAC to the MAC address for the connected endpoint, source MAC to the MAC of the default gateway.
#* For remote destinations: Rewrite the destination MAC to the MAC of the next hop, source MAC to the MAC of the default gateway.
# If the next hop is a local port, then it is delivered as-is.  If the next hop is not local, then the packet is encapsulated and the tunnel ID is set to the network identifier for the source endpoint group (sEPG).  If the packet is a layer 2 broadcast packet, then it will need to be written to the correct set of ports, which might be a combination of local and multiple remote tunnel endpoints.

=== Register Usage ===
The processing pipeline needs to store metadata such as the sEPG, dEPG, and broadcast domain. This metadata can be stored in any way supported by the switch. OpenFlow provides a dedicated 64 bit metadata field, Open vSwitch additionally provides multiple 32 bit registers in form of Nicira Extensions. The following examples will use Nicira extensions for simplicity. The choice of register usage is an implementation detail of the renderer.

==== Option 1: Register allocation using Nicira Extensions ====

{| class="wikitable"
|-
! Register !! Value
|-
| <tt>NXM_NX_REG1</tt> || Source Endpoint Group (sEPG) ID
|-
| <tt>NXM_NX_REG2</tt> || L2 context (BD)
|-
| <tt>NXM_NX_REG3</tt> || Destination Endpoint Group (dEPG) ID
|-
| <tt>NXM_NX_REG4</tt> || Port number to send packet to after policy enforcement. This is required because port selection occurs before policy enforcement in the pipeline.
|-
| <tt>NXM_NX_REG5</tt> || L3 context ID (VRF)
|}

==== Option 2: Register allocation using OpenFlow metadata ====

OpenFlow offers a single 64 bit register which can be used to store sEPG, dEPG, and BD throughout the lookup process alternatively. The advantage over using Nicira extensions is better portability and offload capability to hardware.

{| class="wikitable"
|-
! Register !! Value
|-
| metadata[0..15] || Source Endpoint Group (sEPG) ID
|-
| metadata[16..31] || Destination Endpoint Group (dEPG) ID
|-
| metadata[32..39] || L2 context (BD)
|-
| metadata[40..47] || L3 context (VRF)
|-
| metadata[48..63] || Port number to send packet to after policy enforcement. This is required because port selection occurs before policy enforcement in the pipeline.
|}

=== Table/Pipeline Names and Order ===
In order to increase readability, the following table names are used in the following sections. Their order in the pipeline is as follows:

{| class="wikitable"
|-
! Table !! ID !! Description !! Flow Hit !! Flow Miss
|-
| 1 || <tt>PORT_SECURITY</tt> || Optional port security table || Proceed to <tt>SEPG_FILTER</tt> || Drop
|-
| 2 || <tt>SEPG_FILTER</tt> || sEPG selection || Remember sEPG, BD, and VRF. Then proceed to <tt>DEPG_FILTER</tt> || Trigger policy resolution (send to controller)
|-
| 3 || <tt>DPEG_FILTER</tt> || dEPG selection || Remember dEPG and output coordinates, proceed to <tt>POLICY_ENFORCER</tt> || Trigger policy resolution (send to controller)
|-
| 4 || <tt>POLICY_ENFORCER</tt> || Policy enforcement || Forward packet || Drop
|}

OpenFlow >=1.1 capable switches can implement the flow miss policy for each table directly. Pure OpenFlow 1.0 switches will need to have a catch-all flow inserted to enforce the specified policy.

=== Port Security ===
An optional port security table can be inserted at the very beginning of the pipeline. It enforces a list of valid sMAC and sIP addresses for a specific port.

 <nowiki>
priority=30, in_port=TUNNEL_PORT, actions=goto_table:SEPG_FILTER
priority=30, in_port=PORT1, dl_src=MAC1, action=goto_table:SEPG_FILTER
priority=30, in_port=PORT2, dl_src=MAC2, ip, nw_src=IP2, actions=goto_table:SEPG_FILTER
priority=20, in_port=PORT2, dl_src=MAC2, ip, actions=drop
priority=10, in_port=PORT2, dl_src=MAC2, actions=goto_table:SEPG_FILTER
priority=30, in_port=PORT3, actions=goto_table:SEPG_FILTER</nowiki>

The port-security flow-miss policy is set to drop in order for packets received on an unknown port or with an unknown sMAC/sIP to be rejected.

The following mode of enforcement are defined:
# Whitelisted: The port is allowed to use any addresses. All tunnel ports must be whitelisted. The filter is enforced with a single flow matching on in_port and redirects to the next table.
# L2 enforcement: Any packet from the port must use a specific sMAC. The filter is enforced with a single flow matching on the in_port and dl_src and redirects to the next table.
# L3 enforcement: Same as L2 enforcement. Additionally, any IP packet from the port must use a specific sIP. The filter is enforced with three flows with different priority.
## Any IP packet with correct sMAC and sIP is redirected to the next table.
## Any IP packet left over is dropped.
## Any non-IP packet with correct sMAC is redirected to the next table.

=== Source EPG & L2/L3 Domain Selection ===

The sEPG is determined based on a separate flow table which maps known OpenFlow port numbers and tunnel identifiers to a locally unique sEPG ID. The sEPG ID is stored in register NXM_NX_REG1 for later use in the pipeline. At the same time, the L2 and L3 context is determined and stored in register NXM_NX_REG2.

{| class="wikitable"
|-
! Field !! Description
|-
| <tt>table=SEPG_TABLE</tt> || Flow must be in sEPG selection table
|-
| <tt>in_port=$OFPORT</tt> || Flow must match on incoming port
|-
| <tt>tun_id=$VNI</tt> || If in_port is a tunnel, flow must match on tunnel ID
|}

The actions performed are:
# Write sEPG ID corresponding to incoming port or tunnel ID to register
# Write L2/L3 context ID corresponding to incoming port or tunnel ID to registers
# Proceed to dEPG selection

An example flow to map a local port to an sEPG:
 <nowiki>
table=SEPG_FILTER, in_port=$OFPORT
actions=load:$SEPG->NXM_NX_REG1[],
        load:$BD->NXM_NX_REG2[],
        load:$VRF->NXM_NX_REG5[],
        goto_table:$DEPG_FILTER</nowiki>

An example flow to map a tunnel ID to an sEPG:
 <nowiki>
table=SEPG_FILTER, in_port=TUNNEL_PORT, tun_id=$VNI1,
actions=load:$SEPG1->NXM_NX_REG1[],
        load:$BD->NXM_NX_REG2[],
        load:$VRF->NXM_NX_REG5[],
        goto_table:$DEPG_FILTER</nowiki>

A flow hit means that the sEPG is known and the pipeline should proceed to the next stage.

A flow miss means that we have received a packet from an unknown EPG:
# If the packet was received on a local port then this corresponds to the discovery of a new EP for which the Port to EPG mapping has not been populated yet. If the network is learned, generate a packet-in to trigger policy resolution, otherwise drop the packet.
# If the packet was received from a tunnel then this corresponds to a packet for which we have not populated the tunnel ID to EGP mapping yet. If the network is learned, generate a packet-in to trigger policy resolution, otherwise drop the packet.

=== Broadcasting / Multicasting ===
Packets sent to the MAC broadcast addresss (<tt>ff:ff:ff:ff:ff:ff</tt>) must be flooded to all ports belonging to the broadcast domain. This is '''not''' equivalent to the OVS flood action as multiple broadcast domains reside on the same switch. The respective broadcast domains are modeled using OpenFlow group tables as follows:

# Upon addition of a new broadcast domain to the local vSwitch:
#* Create a new OpenFlow group table, using the BD ID as group ID
#*: <code>ovs-ofctl [...] add-group BRIDGE group_id=$BD, type=all</code>
#*:
#* Create a flow in the dEPG selection table matching on broadcast packets and correctly have them flooded to all group members:
#*: <code>priority=10, table=$DEPG_TABLE, reg2=$BD, dl_dst=ff:ff:ff:ff:ff:ff, actions=group:$BD</code>
# Upon addition/removal of a local port
#* Modify group and add/remove output action to port to account for membership change:
#*: <code>osvs-ofctl [...] mod-group $BRIDGE [Old entry,] bucket=output:$PORT</code>
# Upon addition/removal of a non-local port to the BD
#* Modify group and add/remove output + tunnel action to start/stop flooding packets over overlay

XXX: Represent each broadcast/multicast address with a EPG? [[User:Tgraf|tgraf]] ([[User talk:Tgraf|talk]]) 10:45, 24 June 2014 (UTC)

=== Special Packet Types ===
==== ARP Responder ====
In order for the distributed L3 gateway to be reachable, the vSwitch must respond to ARP requests sent to the default gateway address. For this purpose, a flow is added which translates ARP requests into ARP replies and sends them back out the incoming port.

{| class="wikitable"
|-
! Field !! Description
|-
| <tt>priority=20</tt> || Must have higher priority than regular, non-ARP dEPG table flows.
|-
| <tt>table=DEPG_FILTER</tt> || Flow must be in dEPG selection table
|-
| <tt>reg5=2</tt> || Must match a specific L3 context (NXM_NX_REG5)
|-
| <tt>arp, arp_op=1</tt> || Packet must be ARP request
|-
| <tt>arp_tpa=GW_IP</tt> || ARP request must be targeted for IP of gateway
|}

The actions performed are:
# Set dMAC to original sMAC of packet to reverse direction
# Set sMAC to MAC of gateway
# Set ARP operation to (arp-reply)
# Set target hardware address to original source hardware address
# Set source hardware address to MAC of gateway
# Set target protocol address to original source protocol address
# Set source protocol address to IP of gateway
# Transmit packet back out the incoming port

 <nowiki>
priority=20, table=DEPG_FILTER, reg5=$VRF,
arp, arp_op=1, arp_tpa=$GW_ADDRESS,
actions=move:NXM_OF_ETH_SRC[]->NXM_OF_ETH_DST[],
        mod_dl_src:$GW_MAC,
        load:2->NXM_OF_ARP_OP[],
        move:NXM_NX_ARP_SHA[]->NXM_NX_ARP_THA[],
        load:''Hex(''$GW_MAC'')''->NXM_NX_ARP_SHA[],
        move:NXM_OF_ARP_SPA[]->NXM_OF_ARP_TPA[],
        load:''Hex(''$GW_ADDRESS'')''->NXM_OF_ARP_SPA[],
        in_port</nowiki>

==== ARP Optimization ====

[[File:Gbp ovs arp optimization.png|400px|thumb|right]]

As the MAC / IP pairing of endpoints is known in the network. ARP requests can be optimized and translated into unicasts. While it is possible to have a local vSwitch become an ARP responder directly, the unicast translation offers a minimal aliveness check within the scope of the L2 context.

A flow is inserted into the sEPG selection table as follow:
 <nowiki>
priority=10, arp, arp_op=1, dl_dst=ff:ff:ff:ff:ff:ff, actions=controller</nowiki>

As the ARP request is received, the packet is sent to the controller. The controller/agent resolves the MAC address to the IP address and inserts a new DNAT flow to translate subsequent ARP requests for the same transport address directly in the vSwitch:

 <nowiki>
priority=20, arp, arp_op=1, dl_dst=ff:ff:ff:ff:ff:ff,
  actions=mod_dl_dst:$MAC, output:$OFPORT</nowiki>

The ''OFPORT'' is either a local port or the tunnel port. The latter case requires to additional set the tunnel ID as described in previous sections.

'''Note:''' The controller can proactively insert ARP optimization flows for local or even remote endpoints to avoid the one time controller round trip penalty.

The controller/agent then reinjects the original ARP request back into the network via a packet-out OpenFlow message.

==== DHCP Interception ====
XXX TODO

=== Destination EPG Selection (L2) ===
The dEPG selection is performed after the sEPG has been determined. The mapping occurs in its own flow table which contains both L2 and L3 flow entries. This section explains L2 processing, L3 processing is described in the next section.

The purpose of flow entries in this table is to map known destination MAC addresses in a specific L2 context to a dEPG and to prepare the action set for execution after policy enforcement.

{| class="wikitable"
|-
! Field !! Description
|-
| <tt>priority=10</tt> || Must have lower priority than L3 flows
|-
| <tt>table=DEPG_FILTER</tt> || Flow must be in dEPG selection table
|-
| <tt>reg2=2</tt> || Must match on L2 context (NXM_NX_REG2)
|-
| <tt>dl_dst=MAC</tt> || Packet must match on destination MAC of the EP
|}

The actions performed are:
# Write dEPG ID corresponding to dMAC to register to allow matching on it during policy enforcement
# Write expected outgoing port number to register. This can be a local or a tunnel port.
# If outgoing port is a tunnel, also include an action to set the tunnel ID and tunnel destination to map the sEPG to the tunnel ID.
# Proceed to policy enforcement

Example flow for a local endpoint mapping:
 <nowiki>
table=$DEPG_FILTER, reg2=$BD, dl_dst=$MAC,
actions=load:$DEPG->NXM_NX_REG3[],
        load:$OFPORT->NXM_NX_REG4[],
        goto_table:$ENFORCER_TABLE</nowiki>

Example flow for a remote endpoint mapping:
 <nowiki>
table=$DEPG_FILTER, reg2=$BD, dl_dst=$MAC,
actions=load:$DEPG->NXM_NX_REG3[],
        load:$TUNNEL_PORT->NXM_NX_REG4[],
        move:NXM_NX_REG1[]->NXM_NX_TUN_ID[],
        load:$TUNNEL_DST->NXM_NX_TUN_IPV4_DST[],
        goto_table:$ENFORCER_TABLE</nowiki>

A flow hit indicates that both the sEPG and dEPG are known at this point at the packet can proceed to policy enforcement.

A flow miss indicates that the dEPG is not known. If the network is in learning mode, generate a packet-in, otherwise drop the packet.

=== Destination EPG Selection (L3) ===
Much like L2 flows in the dEPG selection table, L3 flows map known destination IP addresses to the corresponding dEPG and outgoing port number.

Additionally, flow hits will result in a routing action performed.

{| class="wikitable"
|-
! Field !! Description
|-
| <tt>priority=15</tt> || Must have higher priority than L2 but lower than ARP flows.
|-
| <tt>table=DEPG_FILTER</tt> || Flow must be in dEPG selection table
|-
| <tt>reg5=2</tt> || Must match on L3 context (NXM_NX_REG5)
|-
| <tt>dl_dst=GW_MAC</tt> || Packet must match MAC of gateway
|-
| <tt>nw_dst=PREFIX</tt> || Packet must match on a IP subnet
|}

The actions performed are:
# Write dEPG ID corresponding to destination subnet to register to allow matching on it during policy enforcement
# Write expected outgoing port number to register. This can be a local or a tunnel port
# If outgoing port is a tunnel, also include an action to set the tunnel ID and tunnel destination to map the sEPG to the tunnel ID.
# Modify destination MAC to the nexthop. The nexthop can be the MAC of the EP or another router.
# Set source MAC to MAC of local default gateway
# Decrement TTL
# Proceed to policy enforcement

Example flow for a local endpoint over L3:

 <nowiki>
table=DEPG_TABLE, reg5=$VRF, dl_dst=$ROUTER_MAC, ip, nw_dst=$PREFIX,
actions=load:$DEPG->NXM_NX_REG3[],
        load:$OFPORT->NXM_NX_REG4[],
        mod_dl_dst:$DST_EP_MAC,
        mod_dl_src:$OWN_ROUTER_MAC,
        dec_ttl,
        goto_table:$POLICY_ENFORCER</nowiki>

Example flow for a remote endpoint over L3:

 <nowiki>
table=DEPG_TABLE, reg5=$VRF, dl_dst=$ROUTER_MAC, ip, nw_dst=$PREFIX,
actions=load:$DEPG->NXM_NX_REG3[],
        load:$TUNNEL_PORT->NXM_NX_REG4[],
        move:NXM_NX_REG1[]->NXM_NX_TUN_ID[],
        load:$TUNNEL_DST->NXM_NX_TUN_IPV4_DST[],
        mod_dl_dst:$NEXTHOP,
        mod_dl_src:$OWN_ROUTER_MAC,
        dec_ttl,
        goto_table:$POLICY_ENFORCER</nowiki>

=== Policy Enforcement ===
Given the sEPG, BD/VRF, and dEPG are known at this point, the policy is enforced in a separate flow table by matching on the sEPG and dEPG as found in the respective registers. Additional filters may be provided as specified by the policy. 

{| class="wikitable"
|-
! Field !! Description
|-
| <tt>table=POLICY_ENFORCER</tt> || Flow must be in policy enforcement table.
|-
| <tt>reg1=$SEPG</tt> || Must match on sEPG of packet
|-
| <tt>reg3=$DEPG</tt> || Must match on dEPG of packet
|}

The policy may require to match on additional fields such as L3 ports, TCP flags, labels, conditions, etc.

The actions performed on flow hit depend on the specified policy and are described in the next section.

Example of a flow in the policy enforcement table:
 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG, tcp_dst=DPORT/MASK,
actions=output:NXM_NX_REG4[]</nowiki>

A flow miss indicates that no policy has been specified or the policy has not been populated. Depending
on whether the policy population is proactive or reactive, the action on flow miss is either drop or
notification of the controller/agent to trigger policy resolution.

=== Policy Actions & Packet Rewrite ===

The policy may specify multiple actions which are to be performed on matching policy classifiers.
The following actions are supported:

==== Accept ====

Forward/route the packet as previously selected in the dEPG selection table. This translates to
executing the queued up action set and forwarding the packet to the port number stored in
<tt>NXM_NX_REG4</tt> which represents the L2 nexthop.

Basic example flow to allow an sEPG talk to a dEPG:
 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG,
actions=output:NXM_NX_REG4[]</nowiki>

==== Drop ====

Disregard any previous forwarding or routing decision and drop the packet:

 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG,
actions=clear_actions, drop</nowiki>

==== Log ====

The logging action is an extension to the drop action. It will send packet to the controller for logging
purposes. The controller will then drop the packet.

 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG,
actions=clear_actions, controller:[...]</nowiki>

==== Set QoS ====

The *Set QoS* action allows to modify the QoS mark of a packet. This includes the DiffServ
field as well as ECN information. Note that this action may only be applied to IP packets.

This action is typically followed by an allow or redirect action.

 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG,
actions=mod_nw_tos:TOS, mod_nw_ecn:ECN, ...</nowiki>

==== Redirect ====

This action can be used to overwrite the standard bridging or routing decision that would be
taken for regular network operation. This can be used for service redirection or NAT purposes.

 <nowiki>
table=$POLICY_ENFORCER reg1=$SEPG, reg3=$DEPG,
actions=mod_nw_tos:TOS, mod_nw_ecn:ECN</nowiki>

==== Mirror ====

This action causes the packet to be cloned and forwarded to an additional port (port mirroring).

TBD

== Service Insertion ==
XXX TODO
