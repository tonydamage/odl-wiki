== Rough requirements for Helium ==

# Distribute data in the datastore into shards such that a subset of shards can be located in any cluster member
# A shard should hold all the data for a given top-level yang module (a top-level yang module corresponding to an application)
# A shard should be backed by a persistent store so that when a cluster member is restarted the shard can be reconstructed from the persisted data
# It should be possible to do a transaction on a single shard
# Distributed transactions need not be supported
# There should be logging/monitoring of transactions and data change notifications that would help an app developer tune data access. For example we know that remote-reads could be expensive so we should log reads that took an extra-ordinary amount of time or which is trying to get too much data. Similarly for data change notifications we should log if a registration results in too many notifications or the data delivered by that notification is too much or if delivery is taking too much time.

== General Design Principles ==

# The Clustered/Sharded Data Store should be a drop in replacement for the InMemory Data Store
# Reuse the InMemory Data Store to represent a shard because it already takes care of maintaining a tree which contains data for all modules and can therefore be scaled down to deal with data for a single module
# Use Akka for doing operations on remote shards. Akka seems to fit very well with the existing design of md-sal as it is already based on the actor model.
# The sharding strategy should be customizable. ODL should ship with a default sharding strategy. The sharding strategy determines the location of a shard possibly based on the instance identifier of a data object and the collection of the cluster members


== Design options ==
=== Option 1 : Invoking a remote shard using Rpc ===

This first option does not assume the existence of Akka. With Akka some of these classes and interfaces will disappear. For example a Shard will itself be an actor and will thus not require a proxy and stub for access. In this Rpc option we're not drilling down too much into "How did the Sharded Data Store figure out where the other cluster members are?" The '''ClusterManager''' is a proxy for a component that provides that information to the Sharded Data Store


[[File:High Level Design.png|General Concepts|x600px]]


[[File:Create sharded data store.png|Creation of the Sharded Data Store|1000x500px]]


[[File:Create new transaction.png|Creating a new transaction|1000x500px]]


[[File:Write to transaction.png|Writing data using a transaction to a remote shard|1000x500px]]

== Open Questions/Random Thoughts ==


=== Moving a Shard/How to avoid moving a shard  ===

A clustered system starts up one member (process) at a time. When the very first member comes up it is not aware of any other existing members in a cluster. So it would probably have to assume that all shards are local. When the second member in a cluster comes up it needs to take ownership of some of the shards. This will involve (a) Informing member 1 of the intent to relocate the shard (2) The copying of all shard data from member 1 to member 2. 

Some questions,

# What happens when the shard is being moved? Do all transaction requests to that shard fail?
# What strategies can be used to avoid downtime
# Should the datastore have an “open for business” flag. That is only after all the nodes in the system are up and running will we start processing transactions?
# What happens if there are components which try adding data to the store before the store is open for business? There are certain components like topology manager which add some high-level nodes at startup. (It’s probably only one component)

=== Determining Shard location ===

It seems logical that to determine on which cluster member a Shard lives we need to know (a) some key based on which we can determine the location of a Shard and (b) A collection of cluster members. For a cluster member we may need to know for one the IP address of the member so that when we have to remotely invoke operations on a shard we can do so. 

Some questions,

# What hashing algorithm should we use to translate the inputs to provide us the location of the shard? Consistent Hashing seems like it would fit the bill - it should work fine if the cluster membership is relatively static and it could probably still be relevant when we move to a more dynamic cluster.

=== Akka Questions ===

* Is it bad to use Typed Actors? Why should we avoid them? 

Some answers here : http://doc.akka.io/docs/akka/snapshot/scala/typed-actors.html

TypedActors can very easily be abused as RPC, and that is an abstraction which is well-known to be leaky. Hence TypedActors are not what we think of first when we talk about making highly scalable concurrent software easier to write correctly. They have their niche, use them sparingly.

=== Why can't we use an existing Distributed Data Store instead of rolling our own? ===

# Most distributed DBs do not support transactions. Not even transactions on a single shard. We do intend to support transactions on a given shard.
# Not sure if the current existing DB's could even perform well - they certainly cannot perform as well as our in-memory data store
# External DBs generally do not do data change notifications
# If we used an external DB that would make deployment a little more complicated - we would have to setup ODL and also the external DB - some people like the current deployment simplicity of ODL
# One of the principles that we want to follow is to discourage data reads and promote data delivery (via change notifications) in this model the advantage of fast reads that a high performing external DB like say Mongo would become irrelevant

=== What happens to the system when a given cluster member is down? ===

In Helium since we're not planning to do replication what happens if a node is down. This means certain shards would be down effectively rendering the system unusable. Wondering if we can really get away with not doing replication in Helium.

=== Notes regarding Sharding design ===

The design of sharding should be done carefully based on the queries applications make and noting down that '''it will be painful(migration involved)''' if we want to change the sharding logic later after release.

== Clustered Datastore Persistence Options ==
AKKA Persistence : http://doc.akka.io/docs/akka/2.3.2/java/persistence.html
Roll our own using some existing in-process DB : https://wiki.opendaylight.org/view/OpenDaylight_Controller:MD-SAL:Architecture:Clustered_Data_Store:Persistence_Options#Shard_Persisting_Strategy


== Proof of Concept ==

=== Goals ===

* Figure out if Akka can be leveraged for clustering
* Validate design concepts
* Make design choices
* Estimate performance characteristics

=== Focus Areas ===

* Data Distribution / Sharding / Aggregation (Scatter Gather)
** Determine location of Shard
** Akka Clustering
** Akka Remoting
** Akka Sharding
* Persistence / Recovery 
** Akka Persistence
* Replication / High Availability
* Querying / Indexing
* Serviceability (Monitoring and Diagnosis)
** Akka atmos
* Data Change Notification (Query like Filters)
* Serialization over the wire
** Google Protocol Buffers
** EXI
** BSON
* Data Validators (nothing to do with DataCommitHandlers)
