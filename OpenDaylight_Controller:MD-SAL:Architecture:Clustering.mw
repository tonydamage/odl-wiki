== Rough requirements for Helium ==

# Distribute data in the datastore into shards such that a subset of shards can be located in any cluster member
# A shard should hold all the data for a given top-level yang module (a top-level yang module corresponding to an application)
# A shard should be backed by a persistent store so that when a cluster member is restarted the shard can be reconstructed from the persisted data
# It should be possible to do a transaction on a single shard
# Distributed transactions need not be supported
# There should be logging/monitoring of transactions and data change notifications that would help an app developer tune data access. For example we know that remote-reads could be expensive so we should log reads that took an extra-ordinary amount of time or which is trying to get too much data. Similarly for data change notifications we should log if a registration results in too many notifications or the data delivered by that notification is too much or if delivery is taking too much time.

== General Design Principles ==

# The Clustered/Sharded Data Store should be a drop in replacement for the InMemory Data Store
# Reuse the InMemory Data Store to represent a shard because it already takes care of maintaining a tree which contains data for all modules and can therefore be scaled down to deal with data for a single module
# Use Akka for doing operations on remote shards. Akka seems to fit very well with the existing design of md-sal as it is already based on the actor model.
# The sharding strategy should be customizable. ODL should ship with a default sharding strategy. The sharding strategy determines the location of a shard possibly based on the instance identifier of a data object and the collection of the cluster members


== Design ==

=== Components ===

[[File:High Level Design.png|General Concepts|x600px]]


{| class="wikitable"
|-
! Component Name !! Description
|-
| ClusteringConfiguration || The ClusteringConfiguration represents information about the cluster. The information it provides would be roughly the following,

# What are the nodes in the cluster?
# Which shards live on each node?
# What data goes in each shard?

|-
| ClusteringService || The ClusteringService would have the following responsibilities

# Read the cluster configuration. Where it reads the cluster configuration from should not matter. Initially we could even read the configuration from the file system. Overtime ofcourse we could have a "primary" node come up with a cluster configuration and distribute it to the other members in the cluster.
# Resolve the node name to actual host name/ip
# Given an InstanceIdentifier provide the address of a shard

|-
| DistributedDataStore || The DistributedDataStore would have the following responsibilities

# Implement the DOMDataStore so that we could replace the InMemoryDataStore with the DistributedDataStore
# Create the local shard actors as per the cluster configuration
# Create the listener wrapper actors when a consumer registers a listener. 
|-
| Shard || A Shard would be a '''processor''' which contains some of the data in the system. Since a Shard is an actor you would communicate with it using messages. The messages passed to a shard would for the most part be similar to the operations on the DOMDataStore interface. 

Since the Shard is a ""Processor"" in akka-persistence is a special actor which when passed a ""Persistent"" message will log it to a journal. This journal along with snapshots would be used as a method to recover the state of the DataStore. The state of the Shard would be maintained in an InMemoryDataStore object. 

The MD-SAL DataStore supports three phase commit. The Shard will therefore also provide the functions of the ThreePhasCommitCohort.

|-
| ShardTransaction || A ShardTransaction would be an actor which wraps an InMemoryDataStoreTransaction. Any operation that needs to be done on a transaction - namely ""read"", ""write"", ""delete"" and ""ready"" would be fronted by the ShardTransaction. 

|-
| TransactionProxy || The TransactionProxy will hold the reference to the remote ShardTransaction actor and when returned to the consumer of the DistributedDataStore could be used to invoke the transaction operations on the remote transaction.

|-
| ListenerWrapper || The ListenerWrapper is an actor that would represent a local data change listener. It would be created as a remote actor on the node where the Data Change registration is done. 

|-
| ListenerProxy || The ListenerProxy represents a remote data change listener. When the local Shard issues a data change notification it is the ListenerProxy's responsibility to send that data change notification over to the remote ListenerWrapper actor.


|}

==== Packaging ====

The following bundles should be created,

# MD-SAL Clustering Service
# MD-SAL Distributed DataStore

==== Configuration ====

Cluster configuration defines the members of the cluster and what lives within it. This configuration can be static or dynamic. To make things simple we could go with a static configuration for Helium. The configuration could be defined in a file or files which could be put in the ODL distribution. When the ODL controller is started up we would pass the configuration file to it.

When the MD-SAL Clustering Service bundle comes up it could look at which specific configuration needs to be loaded, reads it from disk and initializes itself. 

Clustering configuration would be as follows,

<code>
cluster-configuration {
    role-name {
        shards [
            {
                name : <string>
                module-qname : <string>
                key : <string>
                replica : <number>
            }
        ]
    }
}
</code>

Now since we are planning to use akka pay special attention to the ""role-name"". The role-name that one uses should correspond to the role-name specified for this node in the akka-cluster configuration. Right now I can see this as a potential area where mistakes could be made as two separate configuration files need to be kept in sync (need to think of a clean solution for this).

==== Discovery ====

ClusteringService will be responsible for Discovery and all related functions. It will depend on [http://doc.akka.io/docs/akka/snapshot/java/cluster-usage.html akka-clustering] to identify the members of the cluster.

When the ClusteringService comes up it first checks for the state of the cluster. It looks up all the members in the cluster and verifies that all the roles defined in the cluster-configuration are fulfilled by the cluster membership. Once all the members with the required roles are up and running the Clustering Service notifies it's listeners that the controller is now open for business.

==== Sharding ====

The DistributedDataStore is responsible for creating the Shard's. When the DistributedDataStore is created it is passed a reference to the ClusteringService. It registers itself as a listener to the ClusteringService. It then asks the ClusteringService to give it a collection of shards that belong on this member. Using that information it creates a bunch of local Shard actors.

===== Creating a new transaction =====


==== Recovery ====

==== Availability ====

==== Monitoring ====

== Open Questions/Random Thoughts ==

=== Why can't we use an existing Distributed Data Store instead of rolling our own? ===

# Most distributed DBs do not support transactions. Not even transactions on a single shard. We do intend to support transactions on a given shard.
# Not sure if the current existing DB's could even perform well - they certainly cannot perform as well as our in-memory data store
# External DBs generally do not do data change notifications
# If we used an external DB that would make deployment a little more complicated - we would have to setup ODL and also the external DB - some people like the current deployment simplicity of ODL
# One of the principles that we want to follow is to discourage data reads and promote data delivery (via change notifications) in this model the advantage of fast reads that a high performing external DB like say Mongo would become irrelevant

=== Notes regarding Sharding design ===

The design of sharding should be done carefully based on the queries applications make and noting down that '''it will be painful(migration involved)''' if we want to change the sharding logic later after release.

== Clustered Datastore Persistence Options ==
AKKA Persistence : http://doc.akka.io/docs/akka/2.3.2/java/persistence.html
Roll our own using some existing in-process DB : https://wiki.opendaylight.org/view/OpenDaylight_Controller:MD-SAL:Architecture:Clustered_Data_Store:Persistence_Options#Shard_Persisting_Strategy


== Proof of Concept ==

=== Goals ===

* Figure out if Akka can be leveraged for clustering
* Validate design concepts
* Make design choices
* Estimate performance characteristics

=== Focus Areas ===

* Data Distribution / Sharding / Aggregation (Scatter Gather)
** Determine location of Shard
** Akka Clustering
** Akka Remoting
** Akka Sharding
* Persistence / Recovery 
** Akka Persistence
* Replication / High Availability
* Querying / Indexing
* Serviceability (Monitoring and Diagnosis)
** Akka atmos
* Data Change Notification (Query like Filters)
* Serialization over the wire
** Google Protocol Buffers
** EXI
** BSON
* Data Validators (nothing to do with DataCommitHandlers)
