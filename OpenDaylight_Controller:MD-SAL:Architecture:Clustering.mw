== Rough requirements for Helium ==

# Distribute data in the datastore into shards such that a subset of shards can be located in any cluster member
# A shard should hold all the data for a given top-level yang module (a top-level yang module corresponding to an application)
# A shard should be backed by a persistent store so that when a cluster member is restarted the shard can be reconstructed from the persisted data
# It should be possible to do a transaction on a single shard
# Distributed transactions need not be supported
# There should be logging/monitoring of transactions and data change notifications that would help an app developer tune data access. For example we know that remote-reads could be expensive so we should log reads that took an extra-ordinary amount of time or which is trying to get too much data. Similarly for data change notifications we should log if a registration results in too many notifications or the data delivered by that notification is too much or if delivery is taking too much time.

== General Design Principles ==

# The Clustered/Sharded Data Store should be a drop in replacement for the InMemory Data Store
# Reuse the InMemory Data Store to represent a shard because it already takes care of maintaining a tree which contains data for all modules and can therefore be scaled down to deal with data for a single module
# Use Akka for doing operations on remote shards. Akka seems to fit very well with the existing design of md-sal as it is already based on the actor model.
# The sharding strategy should be customizable. ODL should ship with a default sharding strategy. The sharding strategy determines the location of a shard possibly based on the instance identifier of a data object and the collection of the cluster members


== Design ==

=== High Level ===

[[File:High Level Design.png|General Concepts|x600px]]


{| class="wikitable"
|-
! Component Name !! Description
|-
| ClusteringConfiguration || The ClusteringConfiguration represents information about the cluster. The information it provides would be roughly the following,

# What are the nodes in the cluster?
# Which shards live on each node?
# What data goes in each shard?

|-
| ClusteringService || The ClusteringService would have the following responsibilities

# Read the cluster configuration. Where it reads the cluster configuration from should not matter. Initially we could even read the configuration from the file system. Overtime ofcourse we could have a "primary" node come up with a cluster configuration and distribute it to the other members in the cluster.
# Resolve the node name to actual host name/ip
# Given an InstanceIdentifier provide the address of a shard

|-
| DistributedDataStore || The DistributedDataStore would have the following responsibilities

# Implement the DOMDataStore so that we could replace the InMemoryDataStore with the DistributedDataStore
# Create the local shard actors as per the cluster configuration
# Create the listener wrapper actors when a consumer registers a listener. 
|-
| Shard || A Shard would be '''processor''' which contains some of the data in the system. Since a Shard is an actor you would communicate with it using messages. The messages passed to a shard would for the most part be similar to the methods on the DOMDataStore interface. 

A Shard would also be a Processor. A Processor in akka-persistence is a special actor which when passed a ""Persistent"" message will log it to a journal. This journal along with snapshots would be used as a method to recover the state of the DataStore.

The state of the Shard would be maintained in an InMemoryDataStore object. 

|-
| ShardTransaction || A ShardTransaction would be an actor which wraps
|}

==== Cluster Member discovery ====




== Open Questions/Random Thoughts ==


=== Moving a Shard/How to avoid moving a shard  ===

A clustered system starts up one member (process) at a time. When the very first member comes up it is not aware of any other existing members in a cluster. So it would probably have to assume that all shards are local. When the second member in a cluster comes up it needs to take ownership of some of the shards. This will involve (a) Informing member 1 of the intent to relocate the shard (2) The copying of all shard data from member 1 to member 2. 

Some questions,

# What happens when the shard is being moved? Do all transaction requests to that shard fail?
# What strategies can be used to avoid downtime
# Should the datastore have an “open for business” flag. That is only after all the nodes in the system are up and running will we start processing transactions?
# What happens if there are components which try adding data to the store before the store is open for business? There are certain components like topology manager which add some high-level nodes at startup. (It’s probably only one component)

=== Determining Shard location ===

It seems logical that to determine on which cluster member a Shard lives we need to know (a) some key based on which we can determine the location of a Shard and (b) A collection of cluster members. For a cluster member we may need to know for one the IP address of the member so that when we have to remotely invoke operations on a shard we can do so. 

Some questions,

# What hashing algorithm should we use to translate the inputs to provide us the location of the shard? Consistent Hashing seems like it would fit the bill - it should work fine if the cluster membership is relatively static and it could probably still be relevant when we move to a more dynamic cluster.

=== Akka Questions ===

* Is it bad to use Typed Actors? Why should we avoid them? 

Some answers here : http://doc.akka.io/docs/akka/snapshot/scala/typed-actors.html

TypedActors can very easily be abused as RPC, and that is an abstraction which is well-known to be leaky. Hence TypedActors are not what we think of first when we talk about making highly scalable concurrent software easier to write correctly. They have their niche, use them sparingly.

=== Why can't we use an existing Distributed Data Store instead of rolling our own? ===

# Most distributed DBs do not support transactions. Not even transactions on a single shard. We do intend to support transactions on a given shard.
# Not sure if the current existing DB's could even perform well - they certainly cannot perform as well as our in-memory data store
# External DBs generally do not do data change notifications
# If we used an external DB that would make deployment a little more complicated - we would have to setup ODL and also the external DB - some people like the current deployment simplicity of ODL
# One of the principles that we want to follow is to discourage data reads and promote data delivery (via change notifications) in this model the advantage of fast reads that a high performing external DB like say Mongo would become irrelevant

=== What happens to the system when a given cluster member is down? ===

In Helium since we're not planning to do replication what happens if a node is down. This means certain shards would be down effectively rendering the system unusable. Wondering if we can really get away with not doing replication in Helium.

=== Notes regarding Sharding design ===

The design of sharding should be done carefully based on the queries applications make and noting down that '''it will be painful(migration involved)''' if we want to change the sharding logic later after release.

== Clustered Datastore Persistence Options ==
AKKA Persistence : http://doc.akka.io/docs/akka/2.3.2/java/persistence.html
Roll our own using some existing in-process DB : https://wiki.opendaylight.org/view/OpenDaylight_Controller:MD-SAL:Architecture:Clustered_Data_Store:Persistence_Options#Shard_Persisting_Strategy


== Proof of Concept ==

=== Goals ===

* Figure out if Akka can be leveraged for clustering
* Validate design concepts
* Make design choices
* Estimate performance characteristics

=== Focus Areas ===

* Data Distribution / Sharding / Aggregation (Scatter Gather)
** Determine location of Shard
** Akka Clustering
** Akka Remoting
** Akka Sharding
* Persistence / Recovery 
** Akka Persistence
* Replication / High Availability
* Querying / Indexing
* Serviceability (Monitoring and Diagnosis)
** Akka atmos
* Data Change Notification (Query like Filters)
* Serialization over the wire
** Google Protocol Buffers
** EXI
** BSON
* Data Validators (nothing to do with DataCommitHandlers)
